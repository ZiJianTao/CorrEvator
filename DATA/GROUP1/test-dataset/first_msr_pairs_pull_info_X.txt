[{"A_title": "StopWatch: suspend() acts as split() if followed by stop()In my opinion it is a bug that suspend() acts as split() if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause where time counter doesnt continue. So a following stop()-call shouldnt increase the time counter should it?", "A_clean_title": ["stopwatch", "stop", "watch", "suspend", "act", "as", "split", "follow", "by", "stop", "my", "opinion", "it", "bug", "that", "suspend", "act", "as", "split", "follow", "by", "stop", "see", "below", "stopwatch", "stop", "watch", "sw", "new", "stopwatch", "stop", "watch", "sw", "start", "thread", "sleep", "1000", "sw", "suspend", "time", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "thread", "sleep", "2000", "time", "again", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "sw", "resum", "thread", "sleep", "3000", "sw", "suspend", "time", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "thread", "sleep", "4000", "time", "again", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "thread", "sleep", "5000", "sw", "stop", "time", "but", "time", "not", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "suspend", "resum", "like", "paus", "where", "time", "counter", "doesnt", "continu", "so", "follow", "stop", "call", "shouldnt", "increas", "time", "counter", "it"], "B_title": "Add a test to make sure that no one keeps being left in StopWatch after a running state ==. ", "B_clean_title": ["add", "test", "make", "sure", "that", "no", "one", "keep", "be", "left", "stopwatch", "stop", "watch", "after", "run", "state"]},
{"A_title": "StopWatch: suspend() acts as split() if followed by stop()In my opinion it is a bug that suspend() acts as split() if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause where time counter doesnt continue. So a following stop()-call shouldnt increase the time counter should it?", "A_clean_title": ["stopwatch", "stop", "watch", "suspend", "act", "as", "split", "follow", "by", "stop", "my", "opinion", "it", "bug", "that", "suspend", "act", "as", "split", "follow", "by", "stop", "see", "below", "stopwatch", "stop", "watch", "sw", "new", "stopwatch", "stop", "watch", "sw", "start", "thread", "sleep", "1000", "sw", "suspend", "time", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "thread", "sleep", "2000", "time", "again", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "sw", "resum", "thread", "sleep", "3000", "sw", "suspend", "time", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "thread", "sleep", "4000", "time", "again", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "thread", "sleep", "5000", "sw", "stop", "time", "but", "time", "not", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "suspend", "resum", "like", "paus", "where", "time", "counter", "doesnt", "continu", "so", "follow", "stop", "call", "shouldnt", "increas", "time", "counter", "it"], "B_title": "Fixed bug in StopWatch . stopTime. ", "B_clean_title": ["fix", "bug", "stopwatch", "stop", "watch", "stoptim", "stop", "time"]},
{"A_title": "Record type invalid property not reported on function with @this annotationNone", "A_clean_title": ["record", "type", "invalid", "properti", "not", "report", "function", "thi", "annotationnon", "annot", "none"], "B_title": "Fix wrong closing curly brace in object literals. ", "B_clean_title": ["fix", "wrong", "close", "curli", "brace", "object", "liter"]},
{"A_title": "HypergeometricDistribution.sample suffers from integer overflowHi I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesnt work as well as it used to with large integer values – the example code below should return a sample between 0 and 50 but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo    public static void main(String args)      HypergeometricDistribution a = new HypergeometricDistribution(         43130568 42976365 50);     System.out.printf(%d %d%n a.getSupportLowerBound() a.getSupportUpperBound()); // Prints 0 50     System.out.printf(%d%na.sample());                                             // Prints -50       In the debugger I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean() – instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it based on a quick test.", "A_clean_title": ["hypergeometricdistribut", "sampl", "hypergeometr", "distribut", "suffer", "integ", "overflowhi", "overflow", "hi", "have", "applic", "which", "broke", "when", "port", "common", "math", "it", "look", "like", "hypergeometricdistribut", "sampl", "hypergeometr", "distribut", "method", "doesnt", "work", "as", "well", "as", "it", "use", "larg", "integ", "valu", "exampl", "code", "below", "return", "sampl", "between", "50", "but", "usual", "return", "50", "import", "org", "apach", "common", "math3", "distribut", "hypergeometricdistribut", "hypergeometr", "distribut", "public", "class", "foo", "public", "static", "void", "main", "string", "arg", "hypergeometricdistribut", "hypergeometr", "distribut", "new", "hypergeometricdistribut", "hypergeometr", "distribut", "43130568", "42976365", "50", "system", "out", "printf", "getsupportlowerbound", "get", "support", "lower", "bound", "getsupportupperbound", "get", "support", "upper", "bound", "print", "50", "system", "out", "printf", "na", "sampl", "print", "50", "debugg", "trace", "it", "as", "far", "as", "integ", "overflow", "hypergeometricdistribut", "getnumericalmean", "hypergeometr", "distribut", "get", "numer", "mean", "instead", "do", "return", "doubl", "getsamples", "get", "sampl", "size", "getnumberofsuccess", "get", "number", "success", "doubl", "getpopulations", "get", "popul", "size", "it", "could", "return", "getsamples", "get", "sampl", "size", "doubl", "getnumberofsuccess", "get", "number", "success", "doubl", "getpopulations", "get", "popul", "size", "thi", "seem", "fix", "it", "base", "quick", "test"], "B_title": "Fix numeric mean .. ", "B_clean_title": ["fix", "numer", "mean"]},
{"A_title": "HypergeometricDistribution.sample suffers from integer overflowHi I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesnt work as well as it used to with large integer values – the example code below should return a sample between 0 and 50 but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo    public static void main(String args)      HypergeometricDistribution a = new HypergeometricDistribution(         43130568 42976365 50);     System.out.printf(%d %d%n a.getSupportLowerBound() a.getSupportUpperBound()); // Prints 0 50     System.out.printf(%d%na.sample());                                             // Prints -50       In the debugger I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean() – instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it based on a quick test.", "A_clean_title": ["hypergeometricdistribut", "sampl", "hypergeometr", "distribut", "suffer", "integ", "overflowhi", "overflow", "hi", "have", "applic", "which", "broke", "when", "port", "common", "math", "it", "look", "like", "hypergeometricdistribut", "sampl", "hypergeometr", "distribut", "method", "doesnt", "work", "as", "well", "as", "it", "use", "larg", "integ", "valu", "exampl", "code", "below", "return", "sampl", "between", "50", "but", "usual", "return", "50", "import", "org", "apach", "common", "math3", "distribut", "hypergeometricdistribut", "hypergeometr", "distribut", "public", "class", "foo", "public", "static", "void", "main", "string", "arg", "hypergeometricdistribut", "hypergeometr", "distribut", "new", "hypergeometricdistribut", "hypergeometr", "distribut", "43130568", "42976365", "50", "system", "out", "printf", "getsupportlowerbound", "get", "support", "lower", "bound", "getsupportupperbound", "get", "support", "upper", "bound", "print", "50", "system", "out", "printf", "na", "sampl", "print", "50", "debugg", "trace", "it", "as", "far", "as", "integ", "overflow", "hypergeometricdistribut", "getnumericalmean", "hypergeometr", "distribut", "get", "numer", "mean", "instead", "do", "return", "doubl", "getsamples", "get", "sampl", "size", "getnumberofsuccess", "get", "number", "success", "doubl", "getpopulations", "get", "popul", "size", "it", "could", "return", "getsamples", "get", "sampl", "size", "doubl", "getnumberofsuccess", "get", "number", "success", "doubl", "getpopulations", "get", "popul", "size", "thi", "seem", "fix", "it", "base", "quick", "test"], "B_title": "Remove extraneous whitespace. ", "B_clean_title": ["remov", "extran", "whitespac"]},
{"A_title": "too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(53) ...)Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type this step size is not checked against the integration range so if the integration range is extremely short this step size may evaluate the function out of the range (and in fact it tries afterward to go back and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem the step size is checked and truncated if needed.", "A_clean_title": ["too", "larg", "first", "step", "embed", "rung", "kutta", "integr", "dormand", "princ", "53", "adapt", "step", "size", "integr", "comput", "first", "step", "size", "by", "themselv", "it", "not", "provid", "embed", "rung", "kutta", "type", "thi", "step", "size", "not", "check", "against", "integr", "rang", "so", "integr", "rang", "extrem", "short", "thi", "step", "size", "may", "evalu", "function", "out", "rang", "fact", "it", "tri", "afterward", "go", "back", "fail", "stop", "gragg", "bulirsch", "stoer", "integr", "not", "have", "thi", "problem", "step", "size", "check", "truncat", "need"], "B_title": "Fix step size = t - stepStart ;. ", "B_clean_title": ["fix", "step", "size", "stepstart", "step", "start"]},
{"A_title": "Add support for --manage_closure_dependencies and --only_closure_dependencies with compilation level WHITESPACE_ONLYNone", "A_clean_title": ["add", "support", "manag", "closur", "depend", "onli", "closur", "depend", "compil", "level", "whitespac", "onlynon", "onli", "none"], "B_title": "improve comment. ", "B_clean_title": ["improv", "comment"]},
{"A_title": "Add support for --manage_closure_dependencies and --only_closure_dependencies with compilation level WHITESPACE_ONLYNone", "A_clean_title": ["add", "support", "manag", "closur", "depend", "onli", "closur", "depend", "compil", "level", "whitespac", "onlynon", "onli", "none"], "B_title": "don  t skip building all passes when building. ", "B_clean_title": ["don", "skip", "build", "all", "pass", "when", "build"]},
{"A_title": "Add support for --manage_closure_dependencies and --only_closure_dependencies with compilation level WHITESPACE_ONLYNone", "A_clean_title": ["add", "support", "manag", "closur", "depend", "onli", "closur", "depend", "compil", "level", "whitespac", "onlynon", "onli", "none"], "B_title": "Allow closure pass through. ", "B_clean_title": ["allow", "closur", "pass", "through"]},
{"A_title": "FastMath.max(50.0f -50.0f) => -50.0f; should be +50.0fFastMath.max(50.0f -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case testMinMaxFloat() because that has a bug too - it tests doubles not floats.", "A_clean_title": ["fastmath", "max", "fast", "math", "50", "0f", "50", "0f", "50", "0f", "+50", "0ffastmath", "max", "0f", "fast", "math", "50", "0f", "50", "0f", "50", "0f", "+50", "0f", "thi", "becaus", "wrong", "variabl", "return", "bug", "wa", "not", "detect", "by", "test", "case", "testminmaxfloat", "test", "min", "max", "float", "becaus", "that", "ha", "bug", "too", "it", "test", "doubl", "not", "float"], "B_title": "Fix 3481 test. ", "B_clean_title": ["fix", "3481", "test"]},
{"A_title": "FastMath.max(50.0f -50.0f) => -50.0f; should be +50.0fFastMath.max(50.0f -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case testMinMaxFloat() because that has a bug too - it tests doubles not floats.", "A_clean_title": ["fastmath", "max", "fast", "math", "50", "0f", "50", "0f", "50", "0f", "+50", "0ffastmath", "max", "0f", "fast", "math", "50", "0f", "50", "0f", "50", "0f", "+50", "0f", "thi", "becaus", "wrong", "variabl", "return", "bug", "wa", "not", "detect", "by", "test", "case", "testminmaxfloat", "test", "min", "max", "float", "becaus", "that", "ha", "bug", "too", "it", "test", "doubl", "not", "float"], "B_title": "Fix float . max ( a  b ). ", "B_clean_title": ["fix", "float", "max"]},
{"A_title": "FastMath.max(50.0f -50.0f) => -50.0f; should be +50.0fFastMath.max(50.0f -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case testMinMaxFloat() because that has a bug too - it tests doubles not floats.", "A_clean_title": ["fastmath", "max", "fast", "math", "50", "0f", "50", "0f", "50", "0f", "+50", "0ffastmath", "max", "0f", "fast", "math", "50", "0f", "50", "0f", "50", "0f", "+50", "0f", "thi", "becaus", "wrong", "variabl", "return", "bug", "wa", "not", "detect", "by", "test", "case", "testminmaxfloat", "test", "min", "max", "float", "becaus", "that", "ha", "bug", "too", "it", "test", "doubl", "not", "float"], "B_title": "Fix typo in FastMath . exp ( x ). Fix NaN in FastMath . max ( a  b ). ", "B_clean_title": ["fix", "typo", "fastmath", "fast", "math", "exp", "fix", "nan", "na", "fastmath", "fast", "math", "max"]},
{"A_title": "RegulaFalsiSolver failureThe following unit test:  @Test public void testBug()      final UnivariateRealFunction f = new UnivariateRealFunction()              @Override             public double value(double x)                  return Math.exp(x) - Math.pow(Math.PI 3.0);                      ;      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100 f 1 10);    fails with  illegal state: maximal count (100) exceeded: evaluations   Using PegasusSolver the answer is found after 17 evaluations.", "A_clean_title": ["regulafalsisolv", "regula", "falsi", "solver", "failureth", "failur", "follow", "unit", "test", "test", "public", "void", "testbug", "test", "bug", "final", "univariaterealfunct", "univari", "real", "function", "new", "univariaterealfunct", "univari", "real", "function", "overrid", "public", "doubl", "valu", "doubl", "return", "math", "exp", "math", "pow", "math", "pi", "univariaterealsolv", "univari", "real", "solver", "solver", "new", "regulafalsisolv", "regula", "falsi", "solver", "doubl", "root", "solver", "solv", "100", "10", "fail", "illeg", "state", "maxim", "count", "100", "exceed", "evalu", "pegasussolv", "pegasu", "solver", "answer", "found", "after", "17", "evalu"], "B_title": "FastMath . abs ( x ) doesn  t allow false positives in baseSecantSolver. ", "B_clean_title": ["fastmath", "fast", "math", "ab", "doesn", "allow", "fals", "posit", "basesecantsolv", "base", "secant", "solver"]},
{"A_title": "RegulaFalsiSolver failureThe following unit test:  @Test public void testBug()      final UnivariateRealFunction f = new UnivariateRealFunction()              @Override             public double value(double x)                  return Math.exp(x) - Math.pow(Math.PI 3.0);                      ;      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100 f 1 10);    fails with  illegal state: maximal count (100) exceeded: evaluations   Using PegasusSolver the answer is found after 17 evaluations.", "A_clean_title": ["regulafalsisolv", "regula", "falsi", "solver", "failureth", "failur", "follow", "unit", "test", "test", "public", "void", "testbug", "test", "bug", "final", "univariaterealfunct", "univari", "real", "function", "new", "univariaterealfunct", "univari", "real", "function", "overrid", "public", "doubl", "valu", "doubl", "return", "math", "exp", "math", "pow", "math", "pi", "univariaterealsolv", "univari", "real", "solver", "solver", "new", "regulafalsisolv", "regula", "falsi", "solver", "doubl", "root", "solver", "solv", "100", "10", "fail", "illeg", "state", "maxim", "count", "100", "exceed", "evalu", "pegasussolv", "pegasu", "solver", "answer", "found", "after", "17", "evalu"], "B_title": "Fix renegation of baseSecantSolver. ", "B_clean_title": ["fix", "reneg", "basesecantsolv", "base", "secant", "solver"]},
{"A_title": "RegulaFalsiSolver failureThe following unit test:  @Test public void testBug()      final UnivariateRealFunction f = new UnivariateRealFunction()              @Override             public double value(double x)                  return Math.exp(x) - Math.pow(Math.PI 3.0);                      ;      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100 f 1 10);    fails with  illegal state: maximal count (100) exceeded: evaluations   Using PegasusSolver the answer is found after 17 evaluations.", "A_clean_title": ["regulafalsisolv", "regula", "falsi", "solver", "failureth", "failur", "follow", "unit", "test", "test", "public", "void", "testbug", "test", "bug", "final", "univariaterealfunct", "univari", "real", "function", "new", "univariaterealfunct", "univari", "real", "function", "overrid", "public", "doubl", "valu", "doubl", "return", "math", "exp", "math", "pow", "math", "pi", "univariaterealsolv", "univari", "real", "solver", "solver", "new", "regulafalsisolv", "regula", "falsi", "solver", "doubl", "root", "solver", "solv", "100", "10", "fail", "illeg", "state", "maxim", "count", "100", "exceed", "evalu", "pegasussolv", "pegasu", "solver", "answer", "found", "after", "17", "evalu"], "B_title": "Remove false positives in 186 e . g .  186e3232 .. ", "B_clean_title": ["remov", "fals", "posit", "186", "186e3232"]},
{"A_title": "RegulaFalsiSolver failureThe following unit test:  @Test public void testBug()      final UnivariateRealFunction f = new UnivariateRealFunction()              @Override             public double value(double x)                  return Math.exp(x) - Math.pow(Math.PI 3.0);                      ;      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100 f 1 10);    fails with  illegal state: maximal count (100) exceeded: evaluations   Using PegasusSolver the answer is found after 17 evaluations.", "A_clean_title": ["regulafalsisolv", "regula", "falsi", "solver", "failureth", "failur", "follow", "unit", "test", "test", "public", "void", "testbug", "test", "bug", "final", "univariaterealfunct", "univari", "real", "function", "new", "univariaterealfunct", "univari", "real", "function", "overrid", "public", "doubl", "valu", "doubl", "return", "math", "exp", "math", "pow", "math", "pi", "univariaterealsolv", "univari", "real", "solver", "solver", "new", "regulafalsisolv", "regula", "falsi", "solver", "doubl", "root", "solver", "solv", "100", "10", "fail", "illeg", "state", "maxim", "count", "100", "exceed", "evalu", "pegasussolv", "pegasu", "solver", "answer", "found", "after", "17", "evalu"], "B_title": "Fix renegation of base secant solver .. ", "B_clean_title": ["fix", "reneg", "base", "secant", "solver"]},
{"A_title": "RegulaFalsiSolver failureThe following unit test:  @Test public void testBug()      final UnivariateRealFunction f = new UnivariateRealFunction()              @Override             public double value(double x)                  return Math.exp(x) - Math.pow(Math.PI 3.0);                      ;      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100 f 1 10);    fails with  illegal state: maximal count (100) exceeded: evaluations   Using PegasusSolver the answer is found after 17 evaluations.", "A_clean_title": ["regulafalsisolv", "regula", "falsi", "solver", "failureth", "failur", "follow", "unit", "test", "test", "public", "void", "testbug", "test", "bug", "final", "univariaterealfunct", "univari", "real", "function", "new", "univariaterealfunct", "univari", "real", "function", "overrid", "public", "doubl", "valu", "doubl", "return", "math", "exp", "math", "pow", "math", "pi", "univariaterealsolv", "univari", "real", "solver", "solver", "new", "regulafalsisolv", "regula", "falsi", "solver", "doubl", "root", "solver", "solv", "100", "10", "fail", "illeg", "state", "maxim", "count", "100", "exceed", "evalu", "pegasussolv", "pegasu", "solver", "answer", "found", "after", "17", "evalu"], "B_title": "Fix renegation of baseSecantSolver. ", "B_clean_title": ["fix", "reneg", "basesecantsolv", "base", "secant", "solver"]},
{"A_title": "RegulaFalsiSolver failureThe following unit test:  @Test public void testBug()      final UnivariateRealFunction f = new UnivariateRealFunction()              @Override             public double value(double x)                  return Math.exp(x) - Math.pow(Math.PI 3.0);                      ;      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100 f 1 10);    fails with  illegal state: maximal count (100) exceeded: evaluations   Using PegasusSolver the answer is found after 17 evaluations.", "A_clean_title": ["regulafalsisolv", "regula", "falsi", "solver", "failureth", "failur", "follow", "unit", "test", "test", "public", "void", "testbug", "test", "bug", "final", "univariaterealfunct", "univari", "real", "function", "new", "univariaterealfunct", "univari", "real", "function", "overrid", "public", "doubl", "valu", "doubl", "return", "math", "exp", "math", "pow", "math", "pi", "univariaterealsolv", "univari", "real", "solver", "solver", "new", "regulafalsisolv", "regula", "falsi", "solver", "doubl", "root", "solver", "solv", "100", "10", "fail", "illeg", "state", "maxim", "count", "100", "exceed", "evalu", "pegasussolv", "pegasu", "solver", "answer", "found", "after", "17", "evalu"], "B_title": "Fix renegation of boolean methods. ", "B_clean_title": ["fix", "reneg", "boolean", "method"]},
{"A_title": "RegulaFalsiSolver failureThe following unit test:  @Test public void testBug()      final UnivariateRealFunction f = new UnivariateRealFunction()              @Override             public double value(double x)                  return Math.exp(x) - Math.pow(Math.PI 3.0);                      ;      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100 f 1 10);    fails with  illegal state: maximal count (100) exceeded: evaluations   Using PegasusSolver the answer is found after 17 evaluations.", "A_clean_title": ["regulafalsisolv", "regula", "falsi", "solver", "failureth", "failur", "follow", "unit", "test", "test", "public", "void", "testbug", "test", "bug", "final", "univariaterealfunct", "univari", "real", "function", "new", "univariaterealfunct", "univari", "real", "function", "overrid", "public", "doubl", "valu", "doubl", "return", "math", "exp", "math", "pow", "math", "pi", "univariaterealsolv", "univari", "real", "solver", "solver", "new", "regulafalsisolv", "regula", "falsi", "solver", "doubl", "root", "solver", "solv", "100", "10", "fail", "illeg", "state", "maxim", "count", "100", "exceed", "evalu", "pegasussolv", "pegasu", "solver", "answer", "found", "after", "17", "evalu"], "B_title": "Fix the case for renegation in BaseSecantSolver. ", "B_clean_title": ["fix", "case", "reneg", "basesecantsolv", "base", "secant", "solver"]},
{"A_title": "RegulaFalsiSolver failureThe following unit test:  @Test public void testBug()      final UnivariateRealFunction f = new UnivariateRealFunction()              @Override             public double value(double x)                  return Math.exp(x) - Math.pow(Math.PI 3.0);                      ;      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100 f 1 10);    fails with  illegal state: maximal count (100) exceeded: evaluations   Using PegasusSolver the answer is found after 17 evaluations.", "A_clean_title": ["regulafalsisolv", "regula", "falsi", "solver", "failureth", "failur", "follow", "unit", "test", "test", "public", "void", "testbug", "test", "bug", "final", "univariaterealfunct", "univari", "real", "function", "new", "univariaterealfunct", "univari", "real", "function", "overrid", "public", "doubl", "valu", "doubl", "return", "math", "exp", "math", "pow", "math", "pi", "univariaterealsolv", "univari", "real", "solver", "solver", "new", "regulafalsisolv", "regula", "falsi", "solver", "doubl", "root", "solver", "solv", "100", "10", "fail", "illeg", "state", "maxim", "count", "100", "exceed", "evalu", "pegasussolv", "pegasu", "solver", "answer", "found", "after", "17", "evalu"], "B_title": "Fix a bug in the patch .. ", "B_clean_title": ["fix", "bug", "patch"]},
{"A_title": "RegulaFalsiSolver failureThe following unit test:  @Test public void testBug()      final UnivariateRealFunction f = new UnivariateRealFunction()              @Override             public double value(double x)                  return Math.exp(x) - Math.pow(Math.PI 3.0);                      ;      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100 f 1 10);    fails with  illegal state: maximal count (100) exceeded: evaluations   Using PegasusSolver the answer is found after 17 evaluations.", "A_clean_title": ["regulafalsisolv", "regula", "falsi", "solver", "failureth", "failur", "follow", "unit", "test", "test", "public", "void", "testbug", "test", "bug", "final", "univariaterealfunct", "univari", "real", "function", "new", "univariaterealfunct", "univari", "real", "function", "overrid", "public", "doubl", "valu", "doubl", "return", "math", "exp", "math", "pow", "math", "pi", "univariaterealsolv", "univari", "real", "solver", "solver", "new", "regulafalsisolv", "regula", "falsi", "solver", "doubl", "root", "solver", "solv", "100", "10", "fail", "illeg", "state", "maxim", "count", "100", "exceed", "evalu", "pegasussolv", "pegasu", "solver", "answer", "found", "after", "17", "evalu"], "B_title": "Fix renegation of baseSecantSolver. ", "B_clean_title": ["fix", "reneg", "basesecantsolv", "base", "secant", "solver"]},
{"A_title": "StrBuilder appendFixedWidth does not handle nullsAppending a null value with fixed width causes a null pointer exception if getNullText() has not been set.", "A_clean_title": ["strbuilder", "str", "builder", "appendfixedwidth", "append", "fix", "width", "not", "handl", "nullsappend", "null", "append", "null", "valu", "fix", "width", "caus", "null", "pointer", "except", "getnulltext", "get", "null", "text", "ha", "not", "been", "set"], "B_title": "Don  t set the length of a string to 0 if the object is null .. Don  t use the constructor of StrBuilder in some cases .. ", "B_clean_title": ["don", "set", "length", "string", "object", "null", "don", "use", "constructor", "strbuilder", "str", "builder", "some", "case"]},
{"A_title": "StrBuilder appendFixedWidth does not handle nullsAppending a null value with fixed width causes a null pointer exception if getNullText() has not been set.", "A_clean_title": ["strbuilder", "str", "builder", "appendfixedwidth", "append", "fix", "width", "not", "handl", "nullsappend", "null", "append", "null", "valu", "fix", "width", "caus", "null", "pointer", "except", "getnulltext", "get", "null", "text", "ha", "not", "been", "set"], "B_title": "Fix the bug in Hercules . fixed. Fix the bug in Hercules .. ", "B_clean_title": ["fix", "bug", "hercul", "fix", "fix", "bug", "hercul"]},
{"A_title": "StrBuilder appendFixedWidth does not handle nullsAppending a null value with fixed width causes a null pointer exception if getNullText() has not been set.", "A_clean_title": ["strbuilder", "str", "builder", "appendfixedwidth", "append", "fix", "width", "not", "handl", "nullsappend", "null", "append", "null", "valu", "fix", "width", "caus", "null", "pointer", "except", "getnulltext", "get", "null", "text", "ha", "not", "been", "set"], "B_title": "Fix an NPE in StrBuilder . toString ( ). ", "B_clean_title": ["fix", "npe", "strbuilder", "str", "builder", "tostr", "string"]},
{"A_title": "StrBuilder appendFixedWidth does not handle nullsAppending a null value with fixed width causes a null pointer exception if getNullText() has not been set.", "A_clean_title": ["strbuilder", "str", "builder", "appendfixedwidth", "append", "fix", "width", "not", "handl", "nullsappend", "null", "append", "null", "valu", "fix", "width", "caus", "null", "pointer", "except", "getnulltext", "get", "null", "text", "ha", "not", "been", "set"], "B_title": "Fix bug. ", "B_clean_title": ["fix", "bug"]},
{"A_title": "StrBuilder appendFixedWidth does not handle nullsAppending a null value with fixed width causes a null pointer exception if getNullText() has not been set.", "A_clean_title": ["strbuilder", "str", "builder", "appendfixedwidth", "append", "fix", "width", "not", "handl", "nullsappend", "null", "append", "null", "valu", "fix", "width", "caus", "null", "pointer", "except", "getnulltext", "get", "null", "text", "ha", "not", "been", "set"], "B_title": "Fix bug. ", "B_clean_title": ["fix", "bug"]},
{"A_title": "SimplexSolver not working as expected 2SimplexSolver didnt find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1 b = 1 value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double 7 3 0 0  0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double 1 0 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 0 1 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 3 0 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 2 0 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 2 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 3 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 3 2 0 0  Relationship.LEQ 5)); podmienky.add(new LinearConstraint(new double 2 3 0 0  Relationship.LEQ 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia podmienky GoalType.MAXIMIZE true); ===================== Results(incorrect): a = 1 b = 0.5 value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).", "A_clean_title": ["simplexsolv", "simplex", "solver", "not", "work", "as", "expect", "2simplexsolv", "2simplex", "solver", "didnt", "find", "optim", "solut", "program", "lpsolv", "object", "function", "max", "constraint", "r1", "+3", "r2", "+2", "r3", "+2", "r4", "+3", "r5", "+3", "+2", "r6", "+2", "+3", "variabl", "bound", "result", "correct", "valu", "10", "program", "simplexsolv", "simplex", "solv", "linearobjectivefunct", "linear", "object", "function", "kritfcia", "krit", "fcia", "new", "linearobjectivefunct", "linear", "object", "function", "new", "doubl", "collect", "linearconstraint", "linear", "constraint", "podmienki", "new", "arraylist", "array", "list", "linearconstraint", "linear", "constraint", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "simplexsolv", "simplex", "solver", "solver", "new", "simplexsolv", "simplex", "solver", "realpointvaluepair", "real", "point", "valu", "pair", "result", "solver", "optim", "kritfcia", "krit", "fcia", "podmienki", "goaltyp", "maxim", "goal", "type", "true", "result", "incorrect", "valu", "use", "latest", "softwar", "repositori", "includ", "math", "286", "fix"], "B_title": "removed test. ", "B_clean_title": ["remov", "test"]},
{"A_title": "SimplexSolver not working as expected 2SimplexSolver didnt find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1 b = 1 value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double 7 3 0 0  0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double 1 0 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 0 1 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 3 0 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 2 0 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 2 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 3 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 3 2 0 0  Relationship.LEQ 5)); podmienky.add(new LinearConstraint(new double 2 3 0 0  Relationship.LEQ 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia podmienky GoalType.MAXIMIZE true); ===================== Results(incorrect): a = 1 b = 0.5 value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).", "A_clean_title": ["simplexsolv", "simplex", "solver", "not", "work", "as", "expect", "2simplexsolv", "2simplex", "solver", "didnt", "find", "optim", "solut", "program", "lpsolv", "object", "function", "max", "constraint", "r1", "+3", "r2", "+2", "r3", "+2", "r4", "+3", "r5", "+3", "+2", "r6", "+2", "+3", "variabl", "bound", "result", "correct", "valu", "10", "program", "simplexsolv", "simplex", "solv", "linearobjectivefunct", "linear", "object", "function", "kritfcia", "krit", "fcia", "new", "linearobjectivefunct", "linear", "object", "function", "new", "doubl", "collect", "linearconstraint", "linear", "constraint", "podmienki", "new", "arraylist", "array", "list", "linearconstraint", "linear", "constraint", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "simplexsolv", "simplex", "solver", "solver", "new", "simplexsolv", "simplex", "solver", "realpointvaluepair", "real", "point", "valu", "pair", "result", "solver", "optim", "kritfcia", "krit", "fcia", "podmienki", "goaltyp", "maxim", "goal", "type", "true", "result", "incorrect", "valu", "use", "latest", "softwar", "repositori", "includ", "math", "286", "fix"], "B_title": "Fixed epsilon regression in tableau objective function. ", "B_clean_title": ["fix", "epsilon", "regress", "tableau", "object", "function"]},
{"A_title": "SimplexSolver not working as expected 2SimplexSolver didnt find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1 b = 1 value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double 7 3 0 0  0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double 1 0 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 0 1 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 3 0 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 2 0 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 2 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 3 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 3 2 0 0  Relationship.LEQ 5)); podmienky.add(new LinearConstraint(new double 2 3 0 0  Relationship.LEQ 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia podmienky GoalType.MAXIMIZE true); ===================== Results(incorrect): a = 1 b = 0.5 value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).", "A_clean_title": ["simplexsolv", "simplex", "solver", "not", "work", "as", "expect", "2simplexsolv", "2simplex", "solver", "didnt", "find", "optim", "solut", "program", "lpsolv", "object", "function", "max", "constraint", "r1", "+3", "r2", "+2", "r3", "+2", "r4", "+3", "r5", "+3", "+2", "r6", "+2", "+3", "variabl", "bound", "result", "correct", "valu", "10", "program", "simplexsolv", "simplex", "solv", "linearobjectivefunct", "linear", "object", "function", "kritfcia", "krit", "fcia", "new", "linearobjectivefunct", "linear", "object", "function", "new", "doubl", "collect", "linearconstraint", "linear", "constraint", "podmienki", "new", "arraylist", "array", "list", "linearconstraint", "linear", "constraint", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "simplexsolv", "simplex", "solver", "solver", "new", "simplexsolv", "simplex", "solver", "realpointvaluepair", "real", "point", "valu", "pair", "result", "solver", "optim", "kritfcia", "krit", "fcia", "podmienki", "goaltyp", "maxim", "goal", "type", "true", "result", "incorrect", "valu", "use", "latest", "softwar", "repositori", "includ", "math", "286", "fix"], "B_title": "Improved the garbage collection profile of TwoDimTableau .. ", "B_clean_title": ["improv", "garbag", "collect", "profil", "twodimtableau", "two", "dim", "tableau"]},
{"A_title": "SimplexSolver not working as expected 2SimplexSolver didnt find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1 b = 1 value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double 7 3 0 0  0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double 1 0 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 0 1 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 3 0 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 2 0 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 2 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 3 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 3 2 0 0  Relationship.LEQ 5)); podmienky.add(new LinearConstraint(new double 2 3 0 0  Relationship.LEQ 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia podmienky GoalType.MAXIMIZE true); ===================== Results(incorrect): a = 1 b = 0.5 value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).", "A_clean_title": ["simplexsolv", "simplex", "solver", "not", "work", "as", "expect", "2simplexsolv", "2simplex", "solver", "didnt", "find", "optim", "solut", "program", "lpsolv", "object", "function", "max", "constraint", "r1", "+3", "r2", "+2", "r3", "+2", "r4", "+3", "r5", "+3", "+2", "r6", "+2", "+3", "variabl", "bound", "result", "correct", "valu", "10", "program", "simplexsolv", "simplex", "solv", "linearobjectivefunct", "linear", "object", "function", "kritfcia", "krit", "fcia", "new", "linearobjectivefunct", "linear", "object", "function", "new", "doubl", "collect", "linearconstraint", "linear", "constraint", "podmienki", "new", "arraylist", "array", "list", "linearconstraint", "linear", "constraint", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "simplexsolv", "simplex", "solver", "solver", "new", "simplexsolv", "simplex", "solver", "realpointvaluepair", "real", "point", "valu", "pair", "result", "solver", "optim", "kritfcia", "krit", "fcia", "podmienki", "goaltyp", "maxim", "goal", "type", "true", "result", "incorrect", "valu", "use", "latest", "softwar", "repositori", "includ", "math", "286", "fix"], "B_title": "removed epsilon label from tableau objective function. ", "B_clean_title": ["remov", "epsilon", "label", "tableau", "object", "function"]},
{"A_title": "SimplexSolver not working as expected 2SimplexSolver didnt find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1 b = 1 value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double 7 3 0 0  0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double 1 0 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 0 1 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 3 0 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 2 0 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 2 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 3 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 3 2 0 0  Relationship.LEQ 5)); podmienky.add(new LinearConstraint(new double 2 3 0 0  Relationship.LEQ 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia podmienky GoalType.MAXIMIZE true); ===================== Results(incorrect): a = 1 b = 0.5 value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).", "A_clean_title": ["simplexsolv", "simplex", "solver", "not", "work", "as", "expect", "2simplexsolv", "2simplex", "solver", "didnt", "find", "optim", "solut", "program", "lpsolv", "object", "function", "max", "constraint", "r1", "+3", "r2", "+2", "r3", "+2", "r4", "+3", "r5", "+3", "+2", "r6", "+2", "+3", "variabl", "bound", "result", "correct", "valu", "10", "program", "simplexsolv", "simplex", "solv", "linearobjectivefunct", "linear", "object", "function", "kritfcia", "krit", "fcia", "new", "linearobjectivefunct", "linear", "object", "function", "new", "doubl", "collect", "linearconstraint", "linear", "constraint", "podmienki", "new", "arraylist", "array", "list", "linearconstraint", "linear", "constraint", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "simplexsolv", "simplex", "solver", "solver", "new", "simplexsolv", "simplex", "solver", "realpointvaluepair", "real", "point", "valu", "pair", "result", "solver", "optim", "kritfcia", "krit", "fcia", "podmienki", "goaltyp", "maxim", "goal", "type", "true", "result", "incorrect", "valu", "use", "latest", "softwar", "repositori", "includ", "math", "286", "fix"], "B_title": "Fixed an example of the regression in LinearSolver . . .. ", "B_clean_title": ["fix", "exampl", "regress", "linearsolv", "linear", "solver"]},
{"A_title": "SimplexSolver not working as expected 2SimplexSolver didnt find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1 b = 1 value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double 7 3 0 0  0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double 1 0 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 0 1 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 3 0 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 2 0 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 2 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 3 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 3 2 0 0  Relationship.LEQ 5)); podmienky.add(new LinearConstraint(new double 2 3 0 0  Relationship.LEQ 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia podmienky GoalType.MAXIMIZE true); ===================== Results(incorrect): a = 1 b = 0.5 value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).", "A_clean_title": ["simplexsolv", "simplex", "solver", "not", "work", "as", "expect", "2simplexsolv", "2simplex", "solver", "didnt", "find", "optim", "solut", "program", "lpsolv", "object", "function", "max", "constraint", "r1", "+3", "r2", "+2", "r3", "+2", "r4", "+3", "r5", "+3", "+2", "r6", "+2", "+3", "variabl", "bound", "result", "correct", "valu", "10", "program", "simplexsolv", "simplex", "solv", "linearobjectivefunct", "linear", "object", "function", "kritfcia", "krit", "fcia", "new", "linearobjectivefunct", "linear", "object", "function", "new", "doubl", "collect", "linearconstraint", "linear", "constraint", "podmienki", "new", "arraylist", "array", "list", "linearconstraint", "linear", "constraint", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "simplexsolv", "simplex", "solver", "solver", "new", "simplexsolv", "simplex", "solver", "realpointvaluepair", "real", "point", "valu", "pair", "result", "solver", "optim", "kritfcia", "krit", "fcia", "podmienki", "goaltyp", "maxim", "goal", "type", "true", "result", "incorrect", "valu", "use", "latest", "softwar", "repositori", "includ", "math", "286", "fix"], "B_title": "Using less restrictive conditionals .. ", "B_clean_title": ["less", "restrict", "condit"]},
{"A_title": "SimplexSolver not working as expected 2SimplexSolver didnt find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1 b = 1 value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double 7 3 0 0  0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double 1 0 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 0 1 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 3 0 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 2 0 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 2 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 3 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 3 2 0 0  Relationship.LEQ 5)); podmienky.add(new LinearConstraint(new double 2 3 0 0  Relationship.LEQ 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia podmienky GoalType.MAXIMIZE true); ===================== Results(incorrect): a = 1 b = 0.5 value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).", "A_clean_title": ["simplexsolv", "simplex", "solver", "not", "work", "as", "expect", "2simplexsolv", "2simplex", "solver", "didnt", "find", "optim", "solut", "program", "lpsolv", "object", "function", "max", "constraint", "r1", "+3", "r2", "+2", "r3", "+2", "r4", "+3", "r5", "+3", "+2", "r6", "+2", "+3", "variabl", "bound", "result", "correct", "valu", "10", "program", "simplexsolv", "simplex", "solv", "linearobjectivefunct", "linear", "object", "function", "kritfcia", "krit", "fcia", "new", "linearobjectivefunct", "linear", "object", "function", "new", "doubl", "collect", "linearconstraint", "linear", "constraint", "podmienki", "new", "arraylist", "array", "list", "linearconstraint", "linear", "constraint", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "simplexsolv", "simplex", "solver", "solver", "new", "simplexsolv", "simplex", "solver", "realpointvaluepair", "real", "point", "valu", "pair", "result", "solver", "optim", "kritfcia", "krit", "fcia", "podmienki", "goaltyp", "maxim", "goal", "type", "true", "result", "incorrect", "valu", "use", "latest", "softwar", "repositori", "includ", "math", "286", "fix"], "B_title": "Fixed epsilon. ", "B_clean_title": ["fix", "epsilon"]},
{"A_title": "MathUtils.gcd(Integer.MIN_VALUE 0) should throw an Exception instead of returning Integer.MIN_VALUEThe gcd method should throw an Exception for gcd(Integer.MIN_VALUE 0) like for gcd(Integer.MIN_VALUE Integer.MIN_VALUE). The method should only return nonnegative results.", "A_clean_title": ["mathutil", "gcd", "math", "util", "integ", "min", "valu", "throw", "except", "instead", "return", "integ", "min", "valueth", "valu", "gcd", "method", "throw", "except", "gcd", "integ", "min", "valu", "like", "gcd", "integ", "min", "valu", "integ", "min", "valu", "method", "onli", "return", "nonneg", "result"], "B_title": "Fix divide by zero error in MathUtils. Add a throw if it wasn  t possible to do this .. ", "B_clean_title": ["fix", "divid", "by", "zero", "error", "mathutil", "math", "util", "add", "throw", "it", "wasn", "possibl", "thi"]},
{"A_title": "WordUtils.abbreviate bug when lower is greater than str.lengthIn WordUtils.abbreviate upper is adjusted to the length of the string then to lower. But lower is never adjusted to the length of the string so if lower is greater than str.lengt() upper will be too... Then str.substring(0 upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string", "A_clean_title": ["wordutil", "abbrevi", "word", "util", "bug", "when", "lower", "greater", "than", "str", "lengthin", "length", "wordutil", "abbrevi", "word", "util", "upper", "adjust", "length", "string", "then", "lower", "but", "lower", "never", "adjust", "length", "string", "so", "lower", "greater", "than", "str", "lengt", "upper", "will", "too", "then", "str", "substr", "upper", "throw", "stringindexoutofboundsexcept", "string", "index", "out", "bound", "except", "fix", "adjust", "lower", "length", "string"], "B_title": "Fixed misc problems caused by the use of substringBefore and substringAfter methods in WordUtils. ", "B_clean_title": ["fix", "misc", "problem", "caus", "by", "use", "substringbefor", "substr", "befor", "substringaft", "substr", "after", "method", "wordutil", "word", "util"]},
{"A_title": "WordUtils.abbreviate bug when lower is greater than str.lengthIn WordUtils.abbreviate upper is adjusted to the length of the string then to lower. But lower is never adjusted to the length of the string so if lower is greater than str.lengt() upper will be too... Then str.substring(0 upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string", "A_clean_title": ["wordutil", "abbrevi", "word", "util", "bug", "when", "lower", "greater", "than", "str", "lengthin", "length", "wordutil", "abbrevi", "word", "util", "upper", "adjust", "length", "string", "then", "lower", "but", "lower", "never", "adjust", "length", "string", "so", "lower", "greater", "than", "str", "lengt", "upper", "will", "too", "then", "str", "substr", "upper", "throw", "stringindexoutofboundsexcept", "string", "index", "out", "bound", "except", "fix", "adjust", "lower", "length", "string"], "B_title": "Extend word utils upper bounds. ", "B_clean_title": ["extend", "word", "util", "upper", "bound"]},
{"A_title": "ArrayIndexOutOfBoundsException in MathArrays.linearCombinationWhen MathArrays.linearCombination is passed arguments with length 1 it throws an ArrayOutOfBoundsException. This is caused by this line: double prodHighNext = prodHigh1; linearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.", "A_clean_title": ["arrayindexoutofboundsexcept", "array", "index", "out", "bound", "except", "matharray", "linearcombinationwhen", "math", "array", "linear", "combin", "when", "matharray", "linearcombin", "math", "array", "linear", "combin", "pass", "argument", "length", "it", "throw", "arrayoutofboundsexcept", "array", "out", "bound", "except", "thi", "caus", "by", "thi", "line", "doubl", "prodhighnext", "prod", "high", "next", "prodhigh1", "prod", "high1", "linearcombin", "linear", "combin", "check", "length", "argument", "fall", "back", "simpl", "multipl", "length"], "B_title": "Added missing if (. ", "B_clean_title": ["ad", "miss"]},
{"A_title": "IndexOutOfBoundsException when receiving empty buffer at remote channelReceiving buffers from remote input channels with size 0 results in an IndexOutOfBoundsException.  code Caused by: java.lang.IndexOutOfBoundsException: index: 30 (expected: range(0 30)) at io.netty.buffer.AbstractByteBuf.checkIndex(AbstractByteBuf.java:1123) at io.netty.buffer.PooledUnsafeDirectByteBuf.getBytes(PooledUnsafeDirectByteBuf.java:156) at io.netty.buffer.PooledUnsafeDirectByteBuf.getBytes(PooledUnsafeDirectByteBuf.java:151) at io.netty.buffer.SlicedByteBuf.getBytes(SlicedByteBuf.java:179) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:717) at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeBufferOrEvent(PartitionRequestClientHandler.java:205) at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeMsg(PartitionRequestClientHandler.java:164) at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.channelRead(PartitionRequestClientHandler.java:118) code", "A_clean_title": ["indexoutofboundsexcept", "index", "out", "bound", "except", "when", "receiv", "empti", "buffer", "at", "remot", "channelreceiv", "channel", "receiv", "buffer", "remot", "input", "channel", "size", "result", "indexoutofboundsexcept", "index", "out", "bound", "except", "code", "caus", "by", "java", "lang", "indexoutofboundsexcept", "index", "out", "bound", "except", "index", "30", "expect", "rang", "30", "at", "io", "netti", "buffer", "abstractbytebuf", "checkindex", "abstract", "byte", "buf", "check", "index", "abstractbytebuf", "java:1123", "abstract", "byte", "buf", "at", "io", "netti", "buffer", "pooledunsafedirectbytebuf", "getbyt", "pool", "unsaf", "direct", "byte", "buf", "get", "byte", "pooledunsafedirectbytebuf", "java:156", "pool", "unsaf", "direct", "byte", "buf", "at", "io", "netti", "buffer", "pooledunsafedirectbytebuf", "getbyt", "pool", "unsaf", "direct", "byte", "buf", "get", "byte", "pooledunsafedirectbytebuf", "java:151", "pool", "unsaf", "direct", "byte", "buf", "at", "io", "netti", "buffer", "slicedbytebuf", "getbyt", "slice", "byte", "buf", "get", "byte", "slicedbytebuf", "java:179", "slice", "byte", "buf", "at", "io", "netti", "buffer", "abstractbytebuf", "readbyt", "abstract", "byte", "buf", "read", "byte", "abstractbytebuf", "java:717", "abstract", "byte", "buf", "at", "org", "apach", "flink", "runtim", "io", "network", "netti", "partitionrequestclienthandl", "decodebufferorev", "partit", "request", "client", "handler", "decod", "buffer", "or", "event", "partitionrequestclienthandl", "java:205", "partit", "request", "client", "handler", "at", "org", "apach", "flink", "runtim", "io", "network", "netti", "partitionrequestclienthandl", "decodemsg", "partit", "request", "client", "handler", "decod", "msg", "partitionrequestclienthandl", "java:164", "partit", "request", "client", "handler", "at", "org", "apach", "flink", "runtim", "io", "network", "netti", "partitionrequestclienthandl", "channelread", "partit", "request", "client", "handler", "channel", "read", "partitionrequestclienthandl", "java:118", "partit", "request", "client", "handler", "code"], "B_title": "Fix IndexOutOfBoundsException when receiving empty buffer", "B_clean_title": ["fix", "indexoutofboundsexcept", "index", "out", "bound", "except", "when", "receiv", "empti", "buffer"]},
{"A_title": "unescapeXml(&12345678;) should be &12345678;Following test (in EntitiesTest.java) fails:     public void testNumberOverflow() throws Exception           doTestUnescapeEntity(&#12345678; &#12345678;);         doTestUnescapeEntity(x&#12345678;y x&#12345678;y);         doTestUnescapeEntity(&#x12345678; &#x12345678;);         doTestUnescapeEntity(x&#x12345678;y x&#x12345678;y);       Maximim value for char is 0xFFFF so &#12345678; is invalid entity reference and so should be left as is.", "A_clean_title": ["unescapexml", "unescap", "xml", "12345678", "12345678", "follow", "test", "entitiestest", "java", "entiti", "test", "fail", "public", "void", "testnumberoverflow", "test", "number", "overflow", "throw", "except", "dotestunescapeent", "test", "unescap", "entiti", "12345678", "12345678", "dotestunescapeent", "test", "unescap", "entiti", "12345678", "12345678", "dotestunescapeent", "test", "unescap", "entiti", "x12345678", "x12345678", "dotestunescapeent", "test", "unescap", "entiti", "x12345678", "x12345678", "maximim", "valu", "char", "0xffff", "0x", "ffff", "so", "12345678", "invalid", "entiti", "refer", "so", "left", "as"], "B_title": "Applying unit test and fix for #LANG-292. Also fixes a couple of problems with the unescape(Writer..) overload that came up", "B_clean_title": ["appli", "unit", "test", "fix", "lang", "292", "also", "fix", "coupl", "problem", "unescap", "writer", "overload", "that", "came", "up"]},
{"A_title": "Adding behavior in component instantiation listener causes Page.onInitialize() being called even if constructor throws an exceptionPage.onInitialize() will be called even if constructor throws an exception in case below code is added in wicket WebApplication.init(): getComponentInstantiationListeners().add(new IComponentInstantiationListener()                @Override               public void onInstantiation(Component component)                    component.add(new Behavior()                     );                                        );  It seems that the instantiation listener adds the behavior to the page at very start of the page constructor and then the page is marked as dirty to cause onInitialize() being called afterwards.", "A_clean_title": ["ad", "behavior", "compon", "instanti", "listen", "caus", "page", "oniniti", "initi", "be", "call", "even", "constructor", "throw", "exceptionpag", "oniniti", "except", "page", "initi", "will", "call", "even", "constructor", "throw", "except", "case", "below", "code", "ad", "wicket", "webappl", "init", "web", "applic", "getcomponentinstantiationlisten", "get", "compon", "instanti", "listen", "add", "new", "icomponentinstantiationlisten", "compon", "instanti", "listen", "overrid", "public", "void", "oninstanti", "instanti", "compon", "compon", "compon", "add", "new", "behavior", "it", "seem", "that", "instanti", "listen", "add", "behavior", "page", "at", "veri", "start", "page", "constructor", "then", "page", "mark", "as", "dirti", "caus", "oniniti", "initi", "be", "call", "afterward"], "B_title": "initialize Page before calling into instantiationListeners", "B_clean_title": ["initi", "page", "befor", "call", "into", "instantiationlisten", "instanti", "listen"]},
{"A_title": "read is inefficient when there are many split documentsAs reported in OAK-2358 there is a potential problem with revisionGC not cleaning up split documents properly (in 1.0.8.r1644758 at least).   As a side-effect having many garbage-revisions renders the diffImpl algorithm to become very slow - normally it would take only a few millis but with nodes that have many split-documents I can see diffImpl take hundres of millis sometimes up to a few seconds. Which causes the observation dequeuing to be slower than the rate in which observation events are enqueued which results in observation queue never being cleaned up and event handling being delayed more and more.  Adding some logging showed that diffImpl would often read many split-documents which supports the assumption that the revisionGC not cleaning up revisions has the diffImpl-slowness as a side-effect. Having said that - diffImpl should probably still be able to run fast since all the revisions it should look at should be in the main document not in split documents.  I dont have a test case handy for this at the moment unfortunately - if more is coming up Ill add more details here.", "A_clean_title": ["read", "ineffici", "when", "there", "are", "mani", "split", "documentsa", "document", "as", "report", "oak", "2358", "there", "potenti", "problem", "revisiongc", "revis", "gc", "not", "clean", "up", "split", "document", "properli", "r1644758", "at", "least", "as", "side", "effect", "have", "mani", "garbag", "revis", "render", "diffimpl", "diff", "impl", "algorithm", "becom", "veri", "slow", "normal", "it", "would", "take", "onli", "few", "milli", "but", "node", "that", "have", "mani", "split", "document", "see", "diffimpl", "diff", "impl", "take", "hundr", "milli", "sometim", "up", "few", "second", "which", "caus", "observ", "dequeu", "slower", "than", "rate", "which", "observ", "event", "are", "enqueu", "which", "result", "observ", "queue", "never", "be", "clean", "up", "event", "handl", "be", "delay", "more", "more", "ad", "some", "log", "show", "that", "diffimpl", "diff", "impl", "would", "often", "read", "mani", "split", "document", "which", "support", "assumpt", "that", "revisiongc", "revis", "gc", "not", "clean", "up", "revis", "ha", "diffimpl", "slow", "diff", "impl", "as", "side", "effect", "have", "said", "that", "diffimpl", "diff", "impl", "probabl", "still", "abl", "run", "fast", "sinc", "all", "revis", "it", "look", "at", "main", "document", "not", "split", "document", "dont", "have", "test", "case", "handi", "thi", "at", "moment", "unfortun", "more", "come", "up", "ill", "add", "more", "detail", "here"], "B_title": "read is inefficient when there are many split documents", "B_clean_title": ["read", "ineffici", "when", "there", "are", "mani", "split", "document"]},
{"A_title": "Shell.config()s return value is ignored.Shell.config() returns a boolean which is true if there was an error configuring the shell but the value is never observed. This can result in other unintended errors (like trying to use the ConsoleReader member when its not initialized).", "A_clean_title": ["shell", "config", "return", "valu", "ignor", "shell", "config", "return", "boolean", "which", "true", "there", "wa", "error", "configur", "shell", "but", "valu", "never", "observ", "thi", "result", "other", "unintend", "error", "like", "tri", "use", "consoleread", "consol", "reader", "member", "when", "it", "not", "initi"], "B_title": "Remove configError and use config(String...) retval", "B_clean_title": ["remov", "configerror", "config", "error", "use", "config", "string", "retval"]},
{"A_title": "catch(e) yields JSC_UNDEFINED_NAME warning when e is used in catch in advanced modeNone", "A_clean_title": ["catch", "yield", "jsc", "undefin", "name", "warn", "when", "use", "catch", "advanc", "modenon", "mode", "none"], "B_title": "Fix for errors wrt exceptions in the global scope. Fixes issue 1070 R=blickly", "B_clean_title": ["fix", "error", "wrt", "except", "global", "scope", "fix", "issu", "1070", "r=blickli"]},
{"A_title": "UrlUtils.isRelative returns false if URL parameter contains an absolute URLI have a page that gets a return path for a back link as a parameter. A link to this page looks like this:  ./mypage?return=http://example.com  In WebRequestCodingStrategy.encode this URL is returned by pathForTarget. Then it is checked whether this URL is relative using UrlUtils.isRelative. The URL is apparently relative but UrlUtils.isRelative returns false since the check contains:  (url.indexOf(://) < 0  this is false for the above example. Thus an incorrect path is returned by WebRequestCodingStrategy.encode (relative path resolution does not take place).  A fix for the problem would be to check for   !(url.startsWith(http://) || url.startsWith(https://))  Or if other protocols should also be supported a regular expression like ^^/?*:// should work.", "A_clean_title": ["urlutil", "isrel", "url", "util", "rel", "return", "fals", "url", "paramet", "contain", "absolut", "urli", "have", "page", "that", "get", "return", "path", "back", "link", "as", "paramet", "link", "thi", "page", "look", "like", "thi", "mypag", "return=http", "com", "exampl", "webrequestcodingstrategi", "encod", "web", "request", "code", "strategi", "thi", "url", "return", "by", "pathfortarget", "path", "target", "then", "it", "check", "whether", "thi", "url", "rel", "urlutil", "isrel", "url", "util", "rel", "url", "appar", "rel", "but", "urlutil", "isrel", "url", "util", "rel", "return", "fals", "sinc", "check", "contain", "url", "indexof", "index", "thi", "fals", "abov", "exampl", "thu", "incorrect", "path", "return", "by", "webrequestcodingstrategi", "encod", "web", "request", "code", "strategi", "rel", "path", "resolut", "not", "take", "place", "fix", "problem", "would", "check", "url", "startswith", "start", "http", "url", "startswith", "start", "http", "or", "other", "protocol", "also", "support", "regular", "express", "like", "work"], "B_title": "UrlUtils.isRelative returns false if URL parameter contains an absolute URL", "B_clean_title": ["urlutil", "isrel", "url", "util", "rel", "return", "fals", "url", "paramet", "contain", "absolut", "url"]},
{"A_title": "Bounds error in PageableListView#getCurrentPage()In the getCurrentPage() method of class PageableListView the following code:  while ((currentPage * rowsPerPage) > getList().size())             currentPage--;   checks if first cell if out of range. However the index of that first cell is (currentPage * rowsPerPage) and then the comparison with getList().size() should use a >= instead a >.", "A_clean_title": ["bound", "error", "pageablelistview", "pageabl", "list", "view", "getcurrentpag", "get", "current", "page", "getcurrentpag", "get", "current", "page", "method", "class", "pageablelistview", "pageabl", "list", "view", "follow", "code", "while", "currentpag", "current", "page", "rowsperpag", "row", "per", "page", "getlist", "get", "list", "size", "currentpag", "current", "page", "check", "first", "cell", "out", "rang", "howev", "index", "that", "first", "cell", "currentpag", "current", "page", "rowsperpag", "row", "per", "page", "then", "comparison", "getlist", "get", "list", "size", "use", "instead"], "B_title": "fix needed additional adjustment Issue: WICKET-2181", "B_clean_title": ["fix", "need", "addit", "adjust", "issu", "wicket", "2181"]},
{"A_title": "Possible overflow in checkpoint creationCreating a checkpoint with Long.MAX_VALUE lifetime will overflow the value allowing the store to immediately release the checkpoint.", "A_clean_title": ["possibl", "overflow", "checkpoint", "creationcr", "creation", "creat", "checkpoint", "long", "max", "valu", "lifetim", "will", "overflow", "valu", "allow", "store", "immedi", "releas", "checkpoint"], "B_title": "Possible overflow in checkpoint creation", "B_clean_title": ["possibl", "overflow", "checkpoint", "creation"]},
{"A_title": "WAL handling fails to deal with 1.4 -> 1.5 -> 1.6After doing a 1.4 -> 1.5 -> 1.6 upgrade that still has WALs for some tables the 1.6 instance fails to correctly handle the 1.4 recovered WALs.  This can happen either through not waiting long enough after the upgrade to 1.5 or because of an offline table brought online on 1.6 (ala ACCUMULO-2816).", "A_clean_title": ["wal", "handl", "fail", "deal", "6after", "do", "upgrad", "that", "still", "ha", "wal", "wa", "ls", "some", "tabl", "instanc", "fail", "correctli", "handl", "recov", "wal", "wa", "ls", "thi", "happen", "either", "through", "not", "wait", "long", "enough", "after", "upgrad", "or", "becaus", "offlin", "tabl", "brought", "onlin", "ala", "accumulo", "2816"], "B_title": "Correctly handle older internals.", "B_clean_title": ["correctli", "handl", "older", "intern"]},
{"A_title": "Inconsistency in Node#setProperty in case of null valueSetting a null value to a single valued property will result in null being returned while executing the same on a multivalued property will return the removed property.  jr2 returned the removed property in both cases as far as i  remember and i would suggest that we dont change that behavior. in particular since the specification IMO doesnt allow to return null-values for these methods.", "A_clean_title": ["inconsist", "node", "setproperti", "set", "properti", "case", "null", "valueset", "valu", "set", "null", "valu", "singl", "valu", "properti", "will", "result", "null", "be", "return", "while", "execut", "same", "multivalu", "properti", "will", "return", "remov", "properti", "jr2", "return", "remov", "properti", "both", "case", "as", "far", "as", "rememb", "would", "suggest", "that", "we", "dont", "chang", "that", "behavior", "particular", "sinc", "specif", "imo", "doesnt", "allow", "return", "null", "valu", "these", "method"], "B_title": ": Inconsistency in Node#setProperty in case of null value", "B_clean_title": ["inconsist", "node", "setproperti", "set", "properti", "case", "null", "valu"]},
{"A_title": "Fragment and Component with same id fail with misleading exceptionA page having a component from inherited markup *and* fragment with the *same* id fails with misleading exception message.  Exception message: The component(s) below failed to render. Possible reasons could be that: 1) you have added a component in code but forgot to reference it in the markup (thus the component will never be rendered) 2) if your components were added in a parent container then make sure the markup for the child container includes them in <wicket:extend> ... and list of component ids from fragment multiplied by amount of rows in DataTable  Cause: The markup of the component is used by the fragment.", "A_clean_title": ["fragment", "compon", "same", "id", "fail", "mislead", "exceptiona", "except", "page", "have", "compon", "inherit", "markup", "fragment", "same", "id", "fail", "mislead", "except", "messag", "except", "messag", "compon", "below", "fail", "render", "possibl", "reason", "could", "that", "you", "have", "ad", "compon", "code", "but", "forgot", "refer", "it", "markup", "thu", "compon", "will", "never", "render", "your", "compon", "were", "ad", "parent", "contain", "then", "make", "sure", "markup", "child", "contain", "includ", "them", "wicket", "extend", "list", "compon", "id", "fragment", "multipli", "by", "amount", "row", "datat", "data", "tabl", "caus", "markup", "compon", "use", "by", "fragment"], "B_title": "fail early if fragment markup is no fragment tag", "B_clean_title": ["fail", "earli", "fragment", "markup", "no", "fragment", "tag"]},
{"A_title": "MarkupNotFoundException when refreshing a component with AJAX inside a TransparentWebMarkupContainerA component placed inside a TransparentWebMarkupContainer added to its parent cannot be refreshed with AJAX. See quickstart.", "A_clean_title": ["markupnotfoundexcept", "markup", "not", "found", "except", "when", "refresh", "compon", "ajax", "insid", "transparentwebmarkupcontainera", "transpar", "web", "markup", "contain", "compon", "place", "insid", "transparentwebmarkupcontain", "transpar", "web", "markup", "contain", "ad", "it", "parent", "not", "refresh", "ajax", "see", "quickstart"], "B_title": "fixed by letting the sourcing strategies also look for transparent containers Issue: WICKET-3989", "B_clean_title": ["fix", "by", "let", "sourc", "strategi", "also", "look", "transpar", "contain", "issu", "wicket", "3989"]},
{"A_title": "NPE in JsonUtils when the value is nullMost part of org.apache.wicket.ajax.json.JsonUtils.asArray(Map<String Object> map) is trying carefully avoid null value. But there is following line  else if (value.getClass().isArray())  which cause NPE in case of empty value for some key.   P.S. Will provide patch.", "A_clean_title": ["npe", "jsonutil", "json", "util", "when", "valu", "nullmost", "null", "most", "part", "org", "apach", "wicket", "ajax", "json", "jsonutil", "asarray", "json", "util", "as", "array", "map", "string", "object", "map", "tri", "care", "avoid", "null", "valu", "but", "there", "follow", "line", "valu", "getclass", "get", "class", "isarray", "array", "which", "caus", "npe", "case", "empti", "valu", "some", "key", "will", "provid", "patch"], "B_title": "NPE in JsonUtils when the value is null", "B_clean_title": ["npe", "jsonutil", "json", "util", "when", "valu", "null"]},
{"A_title": "@JsonProperty(access = Access.READ_ONLY) - unexpected behaviourHey  I was hoping to make use of @JsonProperty(access = Access.READ_ONLY) but failed.  Assume this class:   I couldnt find a way to stop the deserializer from attempting to deserialize the field fullName.  The only thing that helps is to create a setter and annotate it with @JsonIgnore . However that setter does not make sense and I dont want to have it. Is this a bug in behaviour or am I missing something? Thanks", "A_clean_title": ["jsonproperti", "json", "properti", "access", "access", "read", "onli", "unexpect", "behaviourhey", "behaviour", "hey", "wa", "hope", "make", "use", "jsonproperti", "json", "properti", "access", "access", "read", "onli", "but", "fail", "assum", "thi", "class", "couldnt", "find", "way", "stop", "deseri", "attempt", "deseri", "field", "fullnam", "full", "name", "onli", "thing", "that", "help", "creat", "setter", "annot", "it", "jsonignor", "json", "ignor", "howev", "that", "setter", "not", "make", "sens", "dont", "want", "have", "it", "thi", "bug", "behaviour", "or", "am", "miss", "someth", "thank"], "B_title": "Fix #935", "B_clean_title": ["fix", "935"]},
{"A_title": "StopWatch: suspend() acts as split() if followed by stop()In my opinion it is a bug that suspend() acts as split() if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause where time counter doesnt continue. So a following stop()-call shouldnt increase the time counter should it?", "A_clean_title": ["stopwatch", "stop", "watch", "suspend", "act", "as", "split", "follow", "by", "stop", "my", "opinion", "it", "bug", "that", "suspend", "act", "as", "split", "follow", "by", "stop", "see", "below", "stopwatch", "stop", "watch", "sw", "new", "stopwatch", "stop", "watch", "sw", "start", "thread", "sleep", "1000", "sw", "suspend", "time", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "thread", "sleep", "2000", "time", "again", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "sw", "resum", "thread", "sleep", "3000", "sw", "suspend", "time", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "thread", "sleep", "4000", "time", "again", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "thread", "sleep", "5000", "sw", "stop", "time", "but", "time", "not", "ok", "system", "out", "println", "sw", "gettim", "get", "time", "suspend", "resum", "like", "paus", "where", "time", "counter", "doesnt", "continu", "so", "follow", "stop", "call", "shouldnt", "increas", "time", "counter", "it"], "B_title": "Applying test and fix for LANG-315", "B_clean_title": ["appli", "test", "fix", "lang", "315"]},
{"A_title": "URL query parameter values containing equals sign get cut offWhen calling a page with a query parameter like param1=val1=val2 the value of param1 obtained from PageParameters will be val1. Everything after the equals sign inside the parameter value gets cut off.", "A_clean_title": ["url", "queri", "paramet", "valu", "contain", "equal", "sign", "get", "cut", "offwhen", "off", "when", "call", "page", "queri", "paramet", "like", "param1=val1=val2", "valu", "param1", "obtain", "pageparamet", "page", "paramet", "will", "val1", "everyth", "after", "equal", "sign", "insid", "paramet", "valu", "get", "cut", "off"], "B_title": "URL query parameter values containing equals sign get cut off", "B_clean_title": ["url", "queri", "paramet", "valu", "contain", "equal", "sign", "get", "cut", "off"]},
{"A_title": "support custom response headers in AbstractResource.ResourceResponseIm converting an application to Wicket 1.5 and I see some problems with resources.  There is a case I need to add headers (not present in ResourceResponse properties) and it looks ugly.  This is what I need to do:      @Override     protected void configureCache(ResourceResponse data Attributes attributes)              super.configureCache(data attributes);         ((WebResponse) attributes.getResponse()).setHeader(Accept-Ranges bytes);       Its a hack to use configureCache here but this cant be added to setResponseHeaders which seams a better apparent method name for it.", "A_clean_title": ["support", "custom", "respons", "header", "abstractresourc", "resourceresponseim", "abstract", "resourc", "resourc", "respons", "im", "convert", "applic", "wicket", "see", "some", "problem", "resourc", "there", "case", "need", "add", "header", "not", "present", "resourcerespons", "resourc", "respons", "properti", "it", "look", "ugli", "thi", "what", "need", "overrid", "protect", "void", "configurecach", "configur", "cach", "resourcerespons", "resourc", "respons", "data", "attribut", "attribut", "super", "configurecach", "configur", "cach", "data", "attribut", "webrespons", "web", "respons", "attribut", "getrespons", "get", "respons", "sethead", "set", "header", "accept", "rang", "byte", "it", "hack", "use", "configurecach", "configur", "cach", "here", "but", "thi", "cant", "ad", "setresponsehead", "set", "respons", "header", "which", "seam", "better", "appar", "method", "name", "it"], "B_title": "allow empty header values since they are valid based on RFC2616", "B_clean_title": ["allow", "empti", "header", "valu", "sinc", "they", "are", "valid", "base", "rfc2616"]},
{"A_title": "WebPageRenderer should honor RedirectPolicy.ALWAYS_REDIRECT more consistentlyIn WebPageRenderer shouldPreserveClientUrl() currently has precedence over RedirectPolicy.ALWAYS_REDIRECT.  This can lead to confusion or unexpected behavior when RedirectPolicy.ALWAYS_REDIRECT is explicitely set but for some reason shouldPreserveClientUrl() returns true and thus no redirect is performed due to the logic in WebPageRenderer.  A fix for this particular problem could be implemented in  WebPageRenderer as of Wicket 6.12.0 by changing line 211 to:                  || (shouldPreserveClientUrl && getRedirectPolicy() != RedirectPolicy.ALWAYS_REDIRECT)) //   Note that this problem is slightly related to WICKET-5484. Both fixes combined the line could look like this:                  || (shouldPreserveClientUrl && !isAjax && getRedirectPolicy() != RedirectPolicy.ALWAYS_REDIRECT)) //", "A_clean_title": ["webpagerender", "web", "page", "render", "honor", "redirectpolici", "redirect", "polici", "alway", "redirect", "more", "consistentlyin", "consist", "webpagerender", "web", "page", "render", "shouldpreserveclienturl", "preserv", "client", "url", "current", "ha", "preced", "over", "redirectpolici", "redirect", "polici", "alway", "redirect", "thi", "lead", "confus", "or", "unexpect", "behavior", "when", "redirectpolici", "redirect", "polici", "alway", "redirect", "explicit", "set", "but", "some", "reason", "shouldpreserveclienturl", "preserv", "client", "url", "return", "true", "thu", "no", "redirect", "perform", "due", "logic", "webpagerender", "web", "page", "render", "fix", "thi", "particular", "problem", "could", "implement", "webpagerender", "web", "page", "render", "as", "wicket", "12", "by", "chang", "line", "211", "shouldpreserveclienturl", "preserv", "client", "url", "getredirectpolici", "get", "redirect", "polici", "redirectpolici", "redirect", "polici", "alway", "redirect", "note", "that", "thi", "problem", "slightli", "relat", "wicket", "5484", "both", "fix", "combin", "line", "could", "look", "like", "thi", "shouldpreserveclienturl", "preserv", "client", "url", "isajax", "ajax", "getredirectpolici", "get", "redirect", "polici", "redirectpolici", "redirect", "polici", "alway", "redirect"], "B_title": "WebPageRenderer should honor RedirectPolicy.ALWAYS_REDIRECT more consistently", "B_clean_title": ["webpagerender", "web", "page", "render", "honor", "redirectpolici", "redirect", "polici", "alway", "redirect", "more", "consist"]},
{"A_title": "XPath to SQL-2 conversion fails due to escaping errorThe problem is that the comment is not properly escaped (a C-style comment) so that */ in the XPath query accidentally ends the comment in the SQL-2 query.  The following query cant be converted to SQL-2 because it contains */:  noformat /jcr:root/etc//*@type = product  and ((@size = M or */@size= M or */*/@size = M  or */*/*/@size = M or */*/*/*/@size = M or */*/*/*/*/@size = M)) noformat  I think this was introduced by OAK-2354  http://svn.apache.org/viewvc?view=revision&amp;revision=1645616", "A_clean_title": ["xpath", "path", "sql", "convers", "fail", "due", "escap", "errorth", "error", "problem", "that", "comment", "not", "properli", "escap", "style", "comment", "so", "that", "xpath", "path", "queri", "accident", "end", "comment", "sql", "queri", "follow", "queri", "cant", "convert", "sql", "becaus", "it", "contain", "noformat", "jcr", "root", "etc", "type", "product", "size", "or", "size=", "or", "size", "or", "size", "or", "size", "or", "size", "noformat", "think", "thi", "wa", "introduc", "by", "oak", "2354", "http", "apach", "svn", "org", "viewvc", "view=revis", "amp", "revision=1645616"], "B_title": "XPath to SQL-2 conversion fails due to escaping error", "B_clean_title": ["xpath", "path", "sql", "convers", "fail", "due", "escap", "error"]},
{"A_title": "Errors in BOBYQAOptimizer when numberOfInterpolationPoints is greater than 2*dim+1Ive been having trouble getting BOBYQA to minimize a function (actually a non-linear least squares fit) so as one change I increased the number of interpolation points.  It seems that anything larger than 2*dim+1 causes an error (typically at line 1662                    interpolationPoints.setEntry(nfm ipt interpolationPoints.getEntry(ipt ipt)); Im guessing there is an off by one error in the translation from FORTRAN.  Changing the BOBYQAOptimizerTest as follows (increasing number of interpolation points by one) will cause failures. Bruce Index: src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java =================================================================== — src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java(revision 1221065) +++ src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java(working copy) @@ -2587 +2587 @@  //        RealPointValuePair result = optim.optimize(100000 func goal startPoint);          final double lB = boundaries == null ? null : boundaries0;          final double uB = boundaries == null ? null : boundaries1;  BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 1); +        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 2);          RealPointValuePair result = optim.optimize(maxEvaluations func goal startPoint lB uB);  //        System.out.println(func.getClass().getName() +  =    //              + optim.getEvaluations() +  f();", "A_clean_title": ["error", "bobyqaoptim", "bobyqa", "optim", "when", "numberofinterpolationpoint", "number", "interpol", "point", "greater", "than", "dim+1iv", "been", "have", "troubl", "get", "bobyqa", "minim", "function", "actual", "non", "linear", "least", "squar", "fit", "so", "as", "one", "chang", "increas", "number", "interpol", "point", "it", "seem", "that", "anyth", "larger", "than", "dim+1", "caus", "error", "typic", "at", "line", "1662", "interpolationpoint", "setentri", "interpol", "point", "set", "entri", "nfm", "ipt", "interpolationpoint", "getentri", "interpol", "point", "get", "entri", "ipt", "ipt", "im", "guess", "there", "off", "by", "one", "error", "translat", "fortran", "chang", "bobyqaoptimizertest", "bobyqa", "optim", "test", "as", "follow", "increas", "number", "interpol", "point", "by", "one", "will", "caus", "failur", "bruce", "index", "java", "src", "test", "java", "org", "apach", "common", "math", "optim", "direct", "bobyqaoptimizertest", "bobyqa", "optim", "test", "java", "src", "test", "java", "org", "apach", "common", "math", "optim", "direct", "bobyqaoptimizertest", "bobyqa", "optim", "test", "revis", "1221065", "java", "src", "test", "java", "org", "apach", "common", "math", "optim", "direct", "bobyqaoptimizertest", "bobyqa", "optim", "test", "work", "copi", "2587", "+2587", "realpointvaluepair", "real", "point", "valu", "pair", "result", "optim", "optim", "100000", "func", "goal", "startpoint", "start", "point", "final", "doubl", "lb", "boundari", "null", "null", "boundaries0", "final", "doubl", "ub", "boundari", "null", "null", "boundaries1", "bobyqaoptim", "bobyqa", "optim", "optim", "new", "bobyqaoptim", "bobyqa", "optim", "dim", "bobyqaoptim", "bobyqa", "optim", "optim", "new", "bobyqaoptim", "bobyqa", "optim", "dim", "realpointvaluepair", "real", "point", "valu", "pair", "result", "optim", "optim", "maxevalu", "max", "evalu", "func", "goal", "startpoint", "start", "point", "lb", "ub", "system", "out", "println", "func", "getclass", "get", "class", "getnam", "get", "name", "optim", "getevalu", "get", "evalu"], "B_title": "Offset by one bug. Suggested fix provided by Bruce Johnson. Ive added a unit test that exercises the use of more interpolation points; two previously unexplored code paths are now being traversed.", "B_clean_title": ["offset", "by", "one", "bug", "suggest", "fix", "provid", "by", "bruce", "johnson", "ive", "ad", "unit", "test", "that", "exercis", "use", "more", "interpol", "point", "two", "previous", "unexplor", "code", "path", "are", "now", "be", "travers"]},
{"A_title": "Significant Performance Degradation From Wicket 6.20.0 to Wicket 7.0.0I am experiencing a significant performance degradation for component adds in Wicket 7.0.0 once the component tree for a page gets reasonably large.  The attached quick start can be used to reproduce the issue.  Please note that NUM_ROWS is set to 10000 to exaggerate the performance degradation as the size of the component tree increases.  The same degradation (to a lesser extent) can be viewed with a smaller NUM_ROWS variable.  In Wicket 6.20.0 as the size of the component tree increases the cost of add() remains relatively constant time-wise.  In Wicket 7.0.0 a component add () is much more expensive (and actually makes our internal web application unusable) with form submits taking more than two or three minutes to come back from the server.  Heres some timing examples.    =============================================================================================================  NUM_ROWS = 5000 Wicket 6.20.0 -> ~200 milliseconds of server side rendering (before browser paints HTML). Wicket 7.0.0 -> ~ 10 seconds of server side rendering  NUM_ROWS = 10000 Wicket 6.20.0 -> ~ 500 milliseconds of server side rendering Wicket 7.0.0 -> ~ 40 seconds of server side rendering  =============================================================================================================  The attached quickstart can be used to reproduce the issue on your side.  My guess is that this has to do with the new component queuing feature that was added as part of Wicket 7.0.0.", "A_clean_title": ["signific", "perform", "degrad", "wicket", "20", "wicket", "0i", "am", "experienc", "signific", "perform", "degrad", "compon", "add", "wicket", "onc", "compon", "tree", "page", "get", "reason", "larg", "attach", "quick", "start", "use", "reproduc", "issu", "pleas", "note", "that", "num", "row", "set", "10000", "exagger", "perform", "degrad", "as", "size", "compon", "tree", "increas", "same", "degrad", "lesser", "extent", "view", "smaller", "num", "row", "variabl", "wicket", "20", "as", "size", "compon", "tree", "increas", "cost", "add", "remain", "rel", "constant", "time", "wise", "wicket", "compon", "add", "much", "more", "expens", "actual", "make", "our", "intern", "web", "applic", "unus", "form", "submit", "take", "more", "than", "two", "or", "three", "minut", "come", "back", "server", "here", "some", "time", "exampl", "num", "row", "5000", "wicket", "20", "~200", "millisecond", "server", "side", "render", "befor", "browser", "paint", "html", "wicket", "10", "second", "server", "side", "render", "num", "row", "10000", "wicket", "20", "500", "millisecond", "server", "side", "render", "wicket", "40", "second", "server", "side", "render", "attach", "quickstart", "use", "reproduc", "issu", "your", "side", "my", "guess", "that", "thi", "ha", "new", "compon", "queu", "featur", "that", "wa", "ad", "as", "part", "wicket"], "B_title": "Fix the behavior of MarkupContainer#get(int) for non-empty MarkupContainer and non-existing index", "B_clean_title": ["fix", "behavior", "markupcontain", "markup", "contain", "get", "int", "non", "empti", "markupcontain", "markup", "contain", "non", "exist", "index"]},
{"A_title": "TarMK Cold Standby can corrupt bulk segmentsTheres a race condition on the segment transfer code that may introduce corrupted binary segments on the secondary instance. What can happen during the head sync phase is that the master may send the head segment twice which will make the client receive&store the second segment thinking its a different one.", "A_clean_title": ["tarmk", "tar", "mk", "cold", "standbi", "corrupt", "bulk", "segmentsther", "segment", "there", "race", "condit", "segment", "transfer", "code", "that", "may", "introduc", "corrupt", "binari", "segment", "secondari", "instanc", "what", "happen", "dure", "head", "sync", "phase", "that", "master", "may", "send", "head", "segment", "twice", "which", "will", "make", "client", "receiv", "store", "second", "segment", "think", "it", "differ", "one"], "B_title": "TarMK Cold Standby can corrupt bulk segments", "B_clean_title": ["tarmk", "tar", "mk", "cold", "standbi", "corrupt", "bulk", "segment"]},
{"A_title": "FormComponent.updateCollectionModel  does not handle unmodifiableListFormComponent.updateCollectionModel should handle situation when getter returns unmodifiable list.  Proposed solution:  formComponent.modelChanging(); booelan isChanged; try  collection.clear(); if (convertedInput != null)  collection.addAll(convertedInput);  isChanged = true; catch (Exception e)  // ignore this exception as Unmodifiable list does not allow change  logger.info(An error occurred while trying to modify list attached to  + formComponent e);   try  if(isChanged) formComponent.getModel().setObject(collection); else  // TODO: create here collection as non-abstract successor of setObject declared argument formComponent.getModel().setObject(new ArrayList(convertedInput)); isChanged = true;  catch (Exception e)  // ignore this exception because it could be that there // is not setter for this collection. logger.info(An error occurred while trying to set the new value for the property attached to  + formComponent e);  // at least one update method should pass successfully if(isChanged) formComponent.modelChanged(); else throw new RuntimeException(An error occurred while trying to modify value for the property attached to  + formComponent);", "A_clean_title": ["formcompon", "updatecollectionmodel", "form", "compon", "updat", "collect", "model", "not", "handl", "unmodifiablelistformcompon", "updatecollectionmodel", "unmodifi", "list", "form", "compon", "updat", "collect", "model", "handl", "situat", "when", "getter", "return", "unmodifi", "list", "propos", "solut", "formcompon", "modelchang", "form", "compon", "model", "chang", "booelan", "ischang", "chang", "tri", "collect", "clear", "convertedinput", "convert", "input", "null", "collect", "addal", "add", "all", "convertedinput", "convert", "input", "ischang", "chang", "true", "catch", "except", "ignor", "thi", "except", "as", "unmodifi", "list", "not", "allow", "chang", "logger", "info", "error", "occur", "while", "tri", "modifi", "list", "attach", "formcompon", "form", "compon", "tri", "ischang", "chang", "formcompon", "getmodel", "form", "compon", "get", "model", "setobject", "set", "object", "collect", "todo", "creat", "here", "collect", "as", "non", "abstract", "successor", "setobject", "set", "object", "declar", "argument", "formcompon", "getmodel", "form", "compon", "get", "model", "setobject", "set", "object", "new", "arraylist", "array", "list", "convertedinput", "convert", "input", "ischang", "chang", "true", "catch", "except", "ignor", "thi", "except", "becaus", "it", "could", "that", "there", "not", "setter", "thi", "collect", "logger", "info", "error", "occur", "while", "tri", "set", "new", "valu", "properti", "attach", "formcompon", "form", "compon", "at", "least", "one", "updat", "method", "pass", "success", "ischang", "chang", "formcompon", "modelchang", "form", "compon", "model", "chang", "throw", "new", "runtimeexcept", "runtim", "except", "error", "occur", "while", "tri", "modifi", "valu", "properti", "attach", "formcompon", "form", "compon"], "B_title": "support unmodifiable collections as well as models without setter", "B_clean_title": ["support", "unmodifi", "collect", "as", "well", "as", "model", "without", "setter"]},
{"A_title": "WrongTypeOfReturnValue when abstract class have two abstract method.This is strange behavior because the method lol() should not be called but when I delete one abstract method everything is good.", "A_clean_title": ["wrongtypeofreturnvalu", "wrong", "type", "return", "valu", "when", "abstract", "class", "have", "two", "abstract", "method", "thi", "strang", "behavior", "becaus", "method", "lol", "not", "call", "but", "when", "delet", "one", "abstract", "method", "everyth", "good"], "B_title": "Merge pull request #30 from marcingrzejszczak/issue399", "B_clean_title": ["merg", "pull", "request", "30", "marcingrzejszczak", "issue399"]},
{"A_title": "SplitOperations purges _commitRoot entries too eagerlyOAK-2528 introduced purging of _commitRoot entries without associated local changes on the document. Those _commitRoot entries are created when a child nodes is added and the _children flag is touched on the parent.  The purge operation is too eager and removes all such entries which may result in an undetected hierarchy conflict.", "A_clean_title": ["splitoper", "split", "oper", "purg", "commitroot", "commit", "root", "entri", "too", "eagerlyoak", "2528", "eagerli", "oak", "introduc", "purg", "commitroot", "commit", "root", "entri", "without", "associ", "local", "chang", "document", "those", "commitroot", "commit", "root", "entri", "are", "creat", "when", "child", "node", "ad", "children", "flag", "touch", "parent", "purg", "oper", "too", "eager", "remov", "all", "such", "entri", "which", "may", "result", "undetect", "hierarchi", "conflict"], "B_title": "SplitOperations purges _commitRoot entries too eagerly", "B_clean_title": ["splitoper", "split", "oper", "purg", "commitroot", "commit", "root", "entri", "too", "eagerli"]},
{"A_title": "Record type invalid property not reported on function with @this annotationNone", "A_clean_title": ["record", "type", "invalid", "properti", "not", "report", "function", "thi", "annotationnon", "annot", "none"], "B_title": "fix a bogus if branch. I have no idea what this was doing. Fixes issue 810", "B_clean_title": ["fix", "bogu", "branch", "have", "no", "idea", "what", "thi", "wa", "do", "fix", "issu", "810"]},
{"A_title": "Result of multiplying and equals for complex numbers is wrongHi. The bug relates on complex numbers. The methods multiply and equals of the class Complex are involved. mathematic background:  (0i) * (-10i) = (0-i). little java program + output that shows the bug: -----------------------------------------------------------------------  import org.apache.commons.math.complex.*; public class TestProg          public static void main(String args)                   ComplexFormat f = new ComplexFormat();                 Complex c1 = new Complex(01);                 Complex c2 = new Complex(-10);                  Complex res = c1.multiply(c2);                 Complex comp = new Complex(0-1);                  System.out.println(res:  +f.format(res));                 System.out.println(comp: +f.format(comp));                  System.out.println(res=comp: +res.equals(comp));             ----------------------------------------------------------------------- res:  -0 - 1i comp: 0 - 1i res=comp: false ----------------------------------------------------------------------- I think the equals should return true. The problem could either be the multiply method that gives (-0-1i) instead of (0-1i) or if you think thats right the equals method has to be modified. Good Luck Dieter", "A_clean_title": ["result", "multipli", "equal", "complex", "number", "wronghi", "wrong", "hi", "bug", "relat", "complex", "number", "method", "multipli", "equal", "class", "complex", "are", "involv", "mathemat", "background", "0i", "10i", "littl", "java", "program", "output", "that", "show", "bug", "import", "org", "apach", "common", "math", "complex", "public", "class", "testprog", "test", "prog", "public", "static", "void", "main", "string", "arg", "complexformat", "complex", "format", "new", "complexformat", "complex", "format", "complex", "c1", "new", "complex", "01", "complex", "c2", "new", "complex", "10", "complex", "re", "c1", "multipli", "c2", "complex", "comp", "new", "complex", "system", "out", "println", "re", "+f", "format", "re", "system", "out", "println", "comp", "+f", "format", "comp", "system", "out", "println", "res=comp", "+re", "equal", "comp", "re", "1i", "comp", "1i", "res=comp", "fals", "think", "equal", "return", "true", "problem", "could", "either", "multipli", "method", "that", "give", "1i", "instead", "1i", "or", "you", "think", "that", "right", "equal", "method", "ha", "modifi", "good", "luck", "dieter"], "B_title": "Changed the Complex.equals() method so that it considers +0 and -0 are equal as required by IEEE-754 standard. JIRA: MATH-221", "B_clean_title": ["chang", "complex", "equal", "method", "so", "that", "it", "consid", "+0", "are", "equal", "as", "requir", "by", "ieee", "754", "standard", "jira", "math", "221"]},
{"A_title": "rest api returns wrong address status isReady:truephase:Pendingaddress-space: standard  addresses: queue(sharded-queue)/topic(sharded-topic)  addresses deployed into address-space are ready to use (simple send/receive) but .status.phase is set to Pending reproducer    create     deploy     get all addresses     result:   address_space definition: standardSpace.json   addresses definition: standard_qt.json   however in standard-controller log you can see that addresses are in phase Active:   ConfigMap of myqueue contains phase Active as well   Ill try to reproduce with brokered...", "A_clean_title": ["rest", "api", "return", "wrong", "address", "statu", "isreadi", "readi", "truephas", "pendingaddress", "space", "standard", "address", "queue", "shard", "queue", "topic", "shard", "topic", "address", "deploy", "into", "address", "space", "are", "readi", "use", "simpl", "send", "receiv", "but", "statu", "phase", "set", "pend", "reproduc", "creat", "deploy", "get", "all", "address", "result", "address", "space", "definit", "standardspac", "json", "standard", "space", "address", "definit", "json", "standard", "qt", "howev", "standard", "control", "log", "you", "see", "that", "address", "are", "phase", "activ", "configmap", "config", "map", "myqueu", "contain", "phase", "activ", "as", "well", "ill", "tri", "reproduc", "broker"], "B_title": "Copy phase in Status copy constructor  This fixes #927", "B_clean_title": ["copi", "phase", "statu", "copi", "constructor", "thi", "fix", "927"]},
{"A_title": "Node.addNode(String String) may check nt-mgt-permission against the wrong nodeWhile I was troubleshooting an issue were having in AEM 6.1 Ive noticed an impossible access denied exception in the logs: the user had permission to add nodes under the node in question but still got an error.  Some testing narrowed the issue down to a difference in behavior between the following two invocations: someNode.getNode(child).addNode(grandchild nt:unstructured); someNode.addNode(child/grandchild nt:unstructured);  As far as I can tell both should behave identically per the JCR spec but the second one fails if the user doesnt have node type management permission to someNode even if they have that permission to someNode/child.  I believe the issue is in line 283 of NodeImpl|https://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-jcr/src/main/java/org/apache/jackrabbit/oak/jcr/session/NodeImpl.java: it is checking permissions against dlg.getTree() but it should really check against parent.getTree() or if possible the path of the node thats about to be created (so glob restrictions can be evaluated).", "A_clean_title": ["node", "addnod", "add", "node", "string", "string", "may", "check", "nt", "mgt", "permiss", "against", "wrong", "nodewhil", "node", "while", "wa", "troubleshoot", "issu", "were", "have", "aem", "ive", "notic", "imposs", "access", "deni", "except", "log", "user", "had", "permiss", "add", "node", "under", "node", "question", "but", "still", "got", "error", "some", "test", "narrow", "issu", "down", "differ", "behavior", "between", "follow", "two", "invoc", "somenod", "getnod", "some", "node", "get", "node", "child", "addnod", "add", "node", "grandchild", "nt", "unstructur", "somenod", "addnod", "some", "node", "add", "node", "child", "grandchild", "nt", "unstructur", "as", "far", "as", "tell", "both", "behav", "ident", "per", "jcr", "spec", "but", "second", "one", "fail", "user", "doesnt", "have", "node", "type", "manag", "permiss", "somenod", "some", "node", "even", "they", "have", "that", "permiss", "somenod", "child", "some", "node", "believ", "issu", "line", "283", "nodeimpl|http", "node", "impl|http", "apach", "java", "svn", "org", "repo", "asf", "jackrabbit", "oak", "trunk", "oak", "jcr", "src", "main", "java", "org", "apach", "jackrabbit", "oak", "jcr", "session", "nodeimpl", "node", "impl", "it", "check", "permiss", "against", "dlg", "gettre", "get", "tree", "but", "it", "realli", "check", "against", "parent", "gettre", "get", "tree", "or", "possibl", "path", "node", "that", "about", "creat", "so", "glob", "restrict", "evalu"], "B_title": ": Node.addNode(String String) may check permissions against the wrong node", "B_clean_title": ["node", "addnod", "add", "node", "string", "string", "may", "check", "permiss", "against", "wrong", "node"]},
{"A_title": "HypergeometricDistribution.sample suffers from integer overflowHi I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesnt work as well as it used to with large integer values – the example code below should return a sample between 0 and 50 but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo    public static void main(String args)      HypergeometricDistribution a = new HypergeometricDistribution(         43130568 42976365 50);     System.out.printf(%d %d%n a.getSupportLowerBound() a.getSupportUpperBound()); // Prints 0 50     System.out.printf(%d%na.sample());                                             // Prints -50       In the debugger I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean() – instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it based on a quick test.", "A_clean_title": ["hypergeometricdistribut", "sampl", "hypergeometr", "distribut", "suffer", "integ", "overflowhi", "overflow", "hi", "have", "applic", "which", "broke", "when", "port", "common", "math", "it", "look", "like", "hypergeometricdistribut", "sampl", "hypergeometr", "distribut", "method", "doesnt", "work", "as", "well", "as", "it", "use", "larg", "integ", "valu", "exampl", "code", "below", "return", "sampl", "between", "50", "but", "usual", "return", "50", "import", "org", "apach", "common", "math3", "distribut", "hypergeometricdistribut", "hypergeometr", "distribut", "public", "class", "foo", "public", "static", "void", "main", "string", "arg", "hypergeometricdistribut", "hypergeometr", "distribut", "new", "hypergeometricdistribut", "hypergeometr", "distribut", "43130568", "42976365", "50", "system", "out", "printf", "getsupportlowerbound", "get", "support", "lower", "bound", "getsupportupperbound", "get", "support", "upper", "bound", "print", "50", "system", "out", "printf", "na", "sampl", "print", "50", "debugg", "trace", "it", "as", "far", "as", "integ", "overflow", "hypergeometricdistribut", "getnumericalmean", "hypergeometr", "distribut", "get", "numer", "mean", "instead", "do", "return", "doubl", "getsamples", "get", "sampl", "size", "getnumberofsuccess", "get", "number", "success", "doubl", "getpopulations", "get", "popul", "size", "it", "could", "return", "getsamples", "get", "sampl", "size", "doubl", "getnumberofsuccess", "get", "number", "success", "doubl", "getpopulations", "get", "popul", "size", "thi", "seem", "fix", "it", "base", "quick", "test"], "B_title": "Reordering can prevent some overflow occurrences (fix suggested by Brian Bloniarz). Added unit test.", "B_clean_title": ["reorder", "prevent", "some", "overflow", "occurr", "fix", "suggest", "by", "brian", "bloniarz", "ad", "unit", "test"]},
{"A_title": "Component markup caching inconsistenciesIn WICKET-3891 we found that Component#markup field is not being reset between requests. The problem is that this field is transient and it is null-ified only when the page is read from the second level page cache (see https://cwiki.apache.org/confluence/x/qIaoAQ). If the page instance is read from first level cache (http session) then its non-serialized version is used and the markup field value is still non-null.  In WICKET-3891 this looked like a minor issue with the markup caching in development mode but actually this problem is valid even in production mode. See the attached application. When the panels variation is changed every MarkupContainer inside still uses its old markup.", "A_clean_title": ["compon", "markup", "cach", "inconsistenciesin", "inconsist", "wicket", "3891", "we", "found", "that", "compon", "markup", "field", "not", "be", "reset", "between", "request", "problem", "that", "thi", "field", "transient", "it", "null", "ifi", "onli", "when", "page", "read", "second", "level", "page", "cach", "see", "http", "apach", "cwiki", "org", "confluenc", "qiaoaq", "iao", "aq", "page", "instanc", "read", "first", "level", "cach", "http", "session", "then", "it", "non", "serial", "version", "use", "markup", "field", "valu", "still", "non", "null", "wicket", "3891", "thi", "look", "like", "minor", "issu", "markup", "cach", "develop", "mode", "but", "actual", "thi", "problem", "valid", "even", "product", "mode", "see", "attach", "applic", "when", "panel", "variat", "chang", "everi", "markupcontain", "markup", "contain", "insid", "still", "use", "it", "old", "markup"], "B_title": "Component markup caching inconsistencies", "B_clean_title": ["compon", "markup", "cach", "inconsist"]},
{"A_title": "Url#toString(StringMode.FULL) throws exception if a segment contains two dotsWhen invoking toString(StringMode.FULL) for a URL like /mountPoint/whatever.../ an IllegalStateException is thrown with message: Cannot render this url in FULL mode because it has a `..` segment: /mountPoint/whatever.../  The method does not actually check for `..` segments but rather checks whether path.contains(..)", "A_clean_title": ["url", "tostr", "string", "stringmod", "full", "string", "mode", "throw", "except", "segment", "contain", "two", "dotswhen", "dot", "when", "invok", "tostr", "string", "stringmod", "full", "string", "mode", "url", "like", "mountpoint", "whatev", "mount", "point", "illegalstateexcept", "illeg", "state", "except", "thrown", "messag", "not", "render", "thi", "url", "full", "mode", "becaus", "it", "ha", "segment", "mountpoint", "whatev", "mount", "point", "method", "not", "actual", "check", "segment", "but", "rather", "check", "whether", "path", "contain"], "B_title": "Url#toString(StringMode.FULL) throws exception if a segment contains two dots", "B_clean_title": ["url", "tostr", "string", "stringmod", "full", "string", "mode", "throw", "except", "segment", "contain", "two", "dot"]},
{"A_title": "too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(53) ...)Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type this step size is not checked against the integration range so if the integration range is extremely short this step size may evaluate the function out of the range (and in fact it tries afterward to go back and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem the step size is checked and truncated if needed.", "A_clean_title": ["too", "larg", "first", "step", "embed", "rung", "kutta", "integr", "dormand", "princ", "53", "adapt", "step", "size", "integr", "comput", "first", "step", "size", "by", "themselv", "it", "not", "provid", "embed", "rung", "kutta", "type", "thi", "step", "size", "not", "check", "against", "integr", "rang", "so", "integr", "rang", "extrem", "short", "thi", "step", "size", "may", "evalu", "function", "out", "rang", "fact", "it", "tri", "afterward", "go", "back", "fail", "stop", "gragg", "bulirsch", "stoer", "integr", "not", "have", "thi", "problem", "step", "size", "check", "truncat", "need"], "B_title": "Check first step size in embedded Runge-Kutta integrators.", "B_clean_title": ["check", "first", "step", "size", "embed", "rung", "kutta", "integr"]},
{"A_title": "Session Window State is Not CheckpointedThe merging window state in the WindowOperator is not checkpointed. This means that programs containing session windows will fail upon restore after a failure.  I propose adding a simulated snapshot/restore cycle to the tests in WindowOperatorTest to catch these problems in the future.", "A_clean_title": ["session", "window", "state", "not", "checkpointedth", "checkpoint", "merg", "window", "state", "windowoper", "window", "oper", "not", "checkpoint", "thi", "mean", "that", "program", "contain", "session", "window", "will", "fail", "upon", "restor", "after", "failur", "propos", "ad", "simul", "snapshot", "restor", "cycl", "test", "windowoperatortest", "window", "oper", "test", "catch", "these", "problem", "futur"], "B_title": "Make Session Window State Checkpointed", "B_clean_title": ["make", "session", "window", "state", "checkpoint"]},
{"A_title": "NamePathMapper should fail on absolute paths escaping rootThe name path mapper should no accept invalid paths of type  code /.. code  I.e. paths which escape beyond the root of the hierarchy.", "A_clean_title": ["namepathmapp", "name", "path", "mapper", "fail", "absolut", "path", "escap", "rootth", "root", "name", "path", "mapper", "no", "accept", "invalid", "path", "type", "code", "code", "path", "which", "escap", "beyond", "root", "hierarchi"], "B_title": "NamePathMapper should fail on absolute paths escaping root", "B_clean_title": ["namepathmapp", "name", "path", "mapper", "fail", "absolut", "path", "escap", "root"]},
{"A_title": "Add support for --manage_closure_dependencies and --only_closure_dependencies with compilation level WHITESPACE_ONLYNone", "A_clean_title": ["add", "support", "manag", "closur", "depend", "onli", "closur", "depend", "compil", "level", "whitespac", "onlynon", "onli", "none"], "B_title": "add dependency management in whitespace-only mode contributed by Chris Peisert fixes issue 703", "B_clean_title": ["add", "depend", "manag", "whitespac", "onli", "mode", "contribut", "by", "chri", "peisert", "fix", "issu", "703"]},
{"A_title": "Converting to Vavr Option fails for present value DATACMNS-1087opened and commented Curently  QueryExecutionConverters tries to invoke Vavrs Optional.of like an instance method rather than static one. This causes exception:    Affects: 1.13.4 (Ingalls SR4)  Referenced from: commits", "A_clean_title": ["convert", "vavr", "option", "fail", "present", "valu", "datacmn", "1087open", "comment", "curent", "queryexecutionconvert", "queri", "execut", "convert", "tri", "invok", "vavr", "option", "like", "instanc", "method", "rather", "than", "static", "one", "thi", "caus", "except", "affect", "13", "ingal", "sr4", "referenc", "commit"], "B_title": "DATACMNS-1087 - Fixed Vavr Option creation from present value.  We now reflectively invoke the static method Option.of(…) while we previously tried to invoke an instance method on the parameter value.", "B_clean_title": ["datacmn", "1087", "fix", "vavr", "option", "creation", "present", "valu", "we", "now", "reflect", "invok", "static", "method", "option", "while", "we", "previous", "tri", "invok", "instanc", "method", "paramet", "valu"]},
{"A_title": "FastMath.max(50.0f -50.0f) => -50.0f; should be +50.0fFastMath.max(50.0f -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case testMinMaxFloat() because that has a bug too - it tests doubles not floats.", "A_clean_title": ["fastmath", "max", "fast", "math", "50", "0f", "50", "0f", "50", "0f", "+50", "0ffastmath", "max", "0f", "fast", "math", "50", "0f", "50", "0f", "50", "0f", "+50", "0f", "thi", "becaus", "wrong", "variabl", "return", "bug", "wa", "not", "detect", "by", "test", "case", "testminmaxfloat", "test", "min", "max", "float", "becaus", "that", "ha", "bug", "too", "it", "test", "doubl", "not", "float"], "B_title": "FastMath.max(50.0f -50.0f) => -50.0f; should be +50.0f", "B_clean_title": ["fastmath", "max", "fast", "math", "50", "0f", "50", "0f", "50", "0f", "+50", "0f"]},
{"A_title": "DateTimeZone.getOffsetFromLocal error during DST transitionThis may be a failure of my understanding but the comments in DateTimeZone.getOffsetFromLocal lead me to believe that if an ambiguous local time is given the offset corresponding to the later of the two possible UTC instants will be returned - i.e. the greater offset.  This doesnt appear to tally with my experience. In fall 2009 America/Los_Angeles changed from -7 to -8 at 2am wall time on November 11. Thus 2am became 1am - so 1:30am is ambiguous. I would therefore expect that constructing a DateTime for November 11th 1:30am would give an instant corresponding with the later value (i.e. 9:30am UTC).", "A_clean_title": ["datetimezon", "getoffsetfromloc", "date", "time", "zone", "get", "offset", "local", "error", "dure", "dst", "transitionthi", "transit", "thi", "may", "failur", "my", "understand", "but", "comment", "datetimezon", "getoffsetfromloc", "date", "time", "zone", "get", "offset", "local", "lead", "me", "believ", "that", "ambigu", "local", "time", "given", "offset", "correspond", "later", "two", "possibl", "utc", "instant", "will", "return", "greater", "offset", "thi", "doesnt", "appear", "talli", "my", "experi", "fall", "2009", "angel", "america", "lo", "chang", "at", "2am", "wall", "time", "novemb", "11", "thu", "2am", "becam", "1am", "so", "1:30am", "ambigu", "would", "therefor", "expect", "that", "construct", "datetim", "date", "time", "novemb", "11th", "1:30am", "would", "give", "instant", "correspond", "later", "valu", "9:30am", "utc"], "B_title": "2952991 The behaviour during DST overlaps is now defined to always return the earlier instant which is normally known as daylight or summer time. Previously the result varied by hemisphere. This affects the constructor of DateTime and other methods", "B_clean_title": ["2952991", "behaviour", "dure", "dst", "overlap", "now", "defin", "alway", "return", "earlier", "instant", "which", "normal", "known", "as", "daylight", "or", "summer", "time", "previous", "result", "vari", "by", "hemispher", "thi", "affect", "constructor", "datetim", "date", "time", "other", "method"]},
{"A_title": "EnumeratedRealDistribution.inverseCumulativeProbability returns values not in the samples setThe method EnumeratedRealDistribution.inverseCumulativeProbability() sometimes returns values that are not in the initial samples domain... I will attach a test to exploit this bug.", "A_clean_title": ["enumeratedrealdistribut", "inversecumulativeprob", "enumer", "real", "distribut", "invers", "cumul", "probabl", "return", "valu", "not", "sampl", "setth", "set", "method", "enumeratedrealdistribut", "inversecumulativeprob", "enumer", "real", "distribut", "invers", "cumul", "probabl", "sometim", "return", "valu", "that", "are", "not", "initi", "sampl", "domain", "will", "attach", "test", "exploit", "thi", "bug"], "B_title": "Fix EnumeratedRealDistribution.inverseCumulativeProbability. Thanks to matteodg and Phil.", "B_clean_title": ["fix", "enumeratedrealdistribut", "inversecumulativeprob", "enumer", "real", "distribut", "invers", "cumul", "probabl", "thank", "matteodg", "phil"]},
{"A_title": "Blob GC throws NPEBlob GC when registered without a shared data store throws NPE. The ClusterRepositoryInfo#getId method should check if clusterId is registered or not.", "A_clean_title": ["blob", "gc", "throw", "npeblob", "npe", "blob", "gc", "when", "regist", "without", "share", "data", "store", "throw", "npe", "clusterrepositoryinfo", "cluster", "repositori", "info", "getid", "get", "id", "method", "check", "clusterid", "cluster", "id", "regist", "or", "not"], "B_title": "Blob GC throws NPE", "B_clean_title": ["blob", "gc", "throw", "npe"]},
{"A_title": "Jackson configuration is not used by ProjectingJackson2HttpMessageConverter  DATACMNS-1152opened and commented  ProjectingJackson2HttpMessageConverter is not using the default MappingJackson2HttpMessageConverter constructor to instantiate an ObjectMapper (that uses Jackson2ObjectMapperBuilder to create a Jackson ObjectMapper based on application configuration) instead ObjectMapper is created directly in SpringDataWebConfiguration.extendMessageConverters(…) . That causes  ProjectingJackson2HttpMessageConverter to not use Jackson configuration from application.properties to create the ObjectMapper and there is no possibility to configure Jackson ObjectMapper . That also breaks MappingJackson2HttpMessageConverter configuration functionality. To solve that issue ObjectMapper creation should be delegated to MappingJackson2HttpMessageConverter default constructor   Affects: 1.13.6 (Ingalls SR6) 2.0 RC2 (Kay)  Backported to:  1.13.7 (Ingalls SR7)", "A_clean_title": ["jackson", "configur", "not", "use", "by", "projectingjackson2httpmessageconvert", "project", "jackson2http", "messag", "convert", "datacmn", "1152open", "comment", "projectingjackson2httpmessageconvert", "project", "jackson2http", "messag", "convert", "not", "default", "mappingjackson2httpmessageconvert", "map", "jackson2http", "messag", "convert", "constructor", "instanti", "objectmapp", "object", "mapper", "that", "use", "jackson2objectmapperbuild", "jackson2object", "mapper", "builder", "creat", "jackson", "objectmapp", "object", "mapper", "base", "applic", "configur", "instead", "objectmapp", "object", "mapper", "creat", "directli", "springdatawebconfigur", "extendmessageconvert", "spring", "data", "web", "configur", "extend", "messag", "convert", "that", "caus", "projectingjackson2httpmessageconvert", "project", "jackson2http", "messag", "convert", "not", "use", "jackson", "configur", "applic", "properti", "creat", "objectmapp", "object", "mapper", "there", "no", "possibl", "configur", "jackson", "objectmapp", "object", "mapper", "that", "also", "break", "mappingjackson2httpmessageconvert", "map", "jackson2http", "messag", "convert", "configur", "function", "solv", "that", "issu", "objectmapp", "object", "mapper", "creation", "deleg", "mappingjackson2httpmessageconvert", "map", "jackson2http", "messag", "convert", "default", "constructor", "affect", "13", "ingal", "sr6", "rc2", "kay", "backport", "13", "ingal", "sr7"], "B_title": "DATACMNS-1152 - Setup of ProjectingJackson2HttpMessageConverter now tries to use unique ObjectMapper from ApplicationContext.  Were now trying to look up a uniquely available ObjectMapper instance from the application context falling back to a simple new instance in case none can be found.", "B_clean_title": ["datacmn", "1152", "setup", "projectingjackson2httpmessageconvert", "project", "jackson2http", "messag", "convert", "now", "tri", "use", "uniqu", "objectmapp", "object", "mapper", "applicationcontext", "applic", "context", "were", "now", "tri", "look", "up", "uniqu", "avail", "objectmapp", "object", "mapper", "instanc", "applic", "context", "fall", "back", "simpl", "new", "instanc", "case", "none", "found"]},
{"A_title": "IndexOutOfBoundsException in FileStore.writeStreamWhen writing streams of specific length I get  code java.lang.IndexOutOfBoundsException at java.nio.Buffer.checkIndex(Buffer.java:538) at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:359) at org.apache.jackrabbit.oak.segment.Segment.getGcGen(Segment.java:318) at org.apache.jackrabbit.oak.segment.file.FileStore.writeSegment(FileStore.java:1371) at org.apache.jackrabbit.oak.segment.SegmentWriter SegmentWriteOperation.internalWriteStream(SegmentWriter.java:661) code", "A_clean_title": ["indexoutofboundsexcept", "index", "out", "bound", "except", "filestor", "writestreamwhen", "file", "store", "write", "stream", "when", "write", "stream", "specif", "length", "get", "code", "java", "lang", "indexoutofboundsexcept", "index", "out", "bound", "except", "at", "java", "nio", "buffer", "checkindex", "check", "index", "buffer", "java:538", "at", "java", "nio", "heapbytebuff", "getint", "heap", "byte", "buffer", "get", "int", "heapbytebuff", "java:359", "heap", "byte", "buffer", "at", "org", "apach", "jackrabbit", "oak", "segment", "segment", "getgcgen", "get", "gc", "gen", "segment", "java:318", "at", "org", "apach", "jackrabbit", "oak", "segment", "file", "filestor", "writeseg", "file", "store", "write", "segment", "filestor", "java:1371", "file", "store", "at", "org", "apach", "jackrabbit", "oak", "segment", "segmentwrit", "segment", "writer", "segmentwriteoper", "internalwritestream", "segment", "write", "oper", "intern", "write", "stream", "segmentwrit", "java:661", "segment", "writer", "code"], "B_title": "IndexOutOfBoundsException in FileStore.writeStream Define segment gc generation = 0 for bulk segments and consider the type of the segment in the gc generation methods accordingly", "B_clean_title": ["indexoutofboundsexcept", "index", "out", "bound", "except", "filestor", "writestream", "file", "store", "write", "stream", "defin", "segment", "gc", "gener", "bulk", "segment", "consid", "type", "segment", "gc", "gener", "method", "accordingli"]},
{"A_title": "Bad type inference with goog.isFunction and friendsNone", "A_clean_title": ["bad", "type", "infer", "goog", "isfunct", "function", "friendsnon", "friend", "none"], "B_title": "Fix goog.isFunction typeof x == function and similiar type inference. Fixes issue 841.", "B_clean_title": ["fix", "goog", "isfunct", "function", "typeof", "function", "similiar", "type", "infer", "fix", "issu", "841"]},
{"A_title": "ContentMirrorStoreStrategy #insert fails to enforce uniqueness and is slowFollowing OAK-734 Ive noticed that the _ContentMirrorStoreStrategy_ fails to enforce the uniqueness constraints assumed on the #insert method.  It is also responsible for a slowdown on the #insert method because of the behavior change of the Property2Index (very frequent saves instead of a bulk one).", "A_clean_title": ["contentmirrorstorestrategi", "content", "mirror", "store", "strategi", "insert", "fail", "enforc", "uniqu", "slowfollow", "slow", "follow", "oak", "734", "ive", "notic", "that", "contentmirrorstorestrategi", "content", "mirror", "store", "strategi", "fail", "enforc", "uniqu", "constraint", "assum", "insert", "method", "it", "also", "respons", "slowdown", "insert", "method", "becaus", "behavior", "chang", "property2index", "veri", "frequent", "save", "instead", "bulk", "one"], "B_title": "ContentMirrorStoreStrategy fails to enforce uniqueness and is slow", "B_clean_title": ["contentmirrorstorestrategi", "content", "mirror", "store", "strategi", "fail", "enforc", "uniqu", "slow"]},
{"A_title": "Unable to add days to a MonthDay set to the ISO leap dateIts not possible to add days to a MonthDay set to the ISO leap date (February 29th). This is even more bizarre given the exact error message thrown.", "A_clean_title": ["unabl", "add", "day", "monthday", "month", "day", "set", "iso", "leap", "dateit", "date", "it", "not", "possibl", "add", "day", "monthday", "month", "day", "set", "iso", "leap", "date", "februari", "29th", "thi", "even", "more", "bizarr", "given", "exact", "error", "messag", "thrown"], "B_title": "Fix MonthDay add/subtract around Feb29 3528941", "B_clean_title": ["fix", "monthday", "month", "day", "add", "subtract", "around", "feb29", "3528941"]},
{"A_title": "Empty branch commit returns head revision on trunkMicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.", "A_clean_title": ["empti", "branch", "commit", "return", "head", "revis", "trunkmicrokernelimpl", "trunk", "micro", "kernel", "impl", "return", "head", "revis", "trunk", "when", "empti", "commit", "happen", "branch", "revis"], "B_title": "Empty branch commit returns head revision on trunk", "B_clean_title": ["empti", "branch", "commit", "return", "head", "revis", "trunk"]},
{"A_title": "WebRequestCodingStrategy: path mounting and matchingAssuming a mount path to /p it will match /pxyz  Assuming this is the desired behavior of matching (warning) then to avoid this match it should be declared /p/ but it will create urls such as /app/p//SomePage. which is wrong.  In the servlet specs  the mapping syntax /p is an exact match this is not what you want in your case since youre doing path mapping so the syntax if you want to stick close to the servlet specs should be /p/* or if you wan to get close to mod_proxy syntax it would be /p/  Note that the examples are also using this wrong mapping declaration. In the example below: both should throw a 404: http://www.wicket-library.com/wicket-examples/niceurl/my/mounted/packageXXX http://www.wicket-library.com/wicket-examples/niceurl/my/mounted/Xpackage", "A_clean_title": ["webrequestcodingstrategi", "web", "request", "code", "strategi", "path", "mount", "matchingassum", "match", "assum", "mount", "path", "it", "will", "match", "pxyz", "assum", "thi", "desir", "behavior", "match", "warn", "then", "avoid", "thi", "match", "it", "declar", "but", "it", "will", "creat", "url", "such", "as", "app", "somepag", "some", "page", "which", "wrong", "servlet", "spec", "map", "syntax", "exact", "match", "thi", "not", "what", "you", "want", "your", "case", "sinc", "your", "do", "path", "map", "so", "syntax", "you", "want", "stick", "close", "servlet", "spec", "or", "you", "wan", "get", "close", "mod", "proxi", "syntax", "it", "would", "note", "that", "exampl", "are", "also", "thi", "wrong", "map", "declar", "exampl", "below", "both", "throw", "404", "http", "wicket", "librari", "exampl", "niceurl", "my", "mount", "packagexxx", "www", "com", "wicket", "packag", "xxx", "http", "wicket", "librari", "exampl", "niceurl", "my", "mount", "xpackag", "www", "com", "wicket"], "B_title": "WebRequestCodingStrategy: path mounting and matching", "B_clean_title": ["webrequestcodingstrategi", "web", "request", "code", "strategi", "path", "mount", "match"]},
{"A_title": "arguments is moved to another scopeNone", "A_clean_title": ["argument", "move", "anoth", "scopenon", "scope", "none"], "B_title": "Dont attempt to inline aliases of external names during collapse properties as all the sets can not be accounted for. Fixes issue 931 ------------- Created by MOE: http://code.google.com/p/moe-java MOE_MIGRATED_REVID=43187988", "B_clean_title": ["dont", "attempt", "inlin", "alias", "extern", "name", "dure", "collaps", "properti", "as", "all", "set", "not", "account", "fix", "issu", "931", "creat", "by", "moe", "http", "java", "googl", "code", "com", "moe", "moe", "migrat", "revid=43187988"]},
{"A_title": "MiniAccumuloConfig doesnt set 0 for monitor log4j portMonitorLoggingIT will fail on a host if the monitor is already running because MAC doesnt configure itself to use an ephemeral port. We havent really noticed this because MAC doesnt start a monitor by default.", "A_clean_title": ["miniaccumuloconfig", "mini", "accumulo", "config", "doesnt", "set", "monitor", "log4j", "portmonitorloggingit", "port", "monitor", "log", "it", "will", "fail", "host", "monitor", "alreadi", "run", "becaus", "mac", "doesnt", "configur", "itself", "use", "ephemer", "port", "we", "havent", "realli", "notic", "thi", "becaus", "mac", "doesnt", "start", "monitor", "by", "default"], "B_title": "Set 0 for monitor log4j port config value", "B_clean_title": ["set", "monitor", "log4j", "port", "config", "valu"]},
{"A_title": "POST params ignored by IPageParametersEncoder#decodePageParameters()As per this conversation: http://apache-wicket.1842946.n4.nabble.com/how-to-get-https-port-number-in-Wicket-1-5-td4295139.html  it seems that POST params are not properly processed and made available as PageParameters. Can anyone say whether this is intended behavior or not? I will attach a Quickstart to demonstrate.  Martins proposed fix is straightforward but I am not comfortable enough with Wicket internals to say whether or not this would break something.  Thanks", "A_clean_title": ["post", "param", "ignor", "by", "ipageparametersencod", "page", "paramet", "encod", "decodepageparamet", "decod", "page", "paramet", "as", "per", "thi", "convers", "http", "get", "http", "port", "number", "wicket", "apach", "wicket", "1842946", "n4", "nabbl", "td4295139", "html", "com", "how", "it", "seem", "that", "post", "param", "are", "not", "properli", "process", "made", "avail", "as", "pageparamet", "page", "paramet", "anyon", "say", "whether", "thi", "intend", "behavior", "or", "not", "will", "attach", "quickstart", "demonstr", "martin", "propos", "fix", "straightforward", "but", "am", "not", "comfort", "enough", "wicket", "intern", "say", "whether", "or", "not", "thi", "would", "break", "someth", "thank"], "B_title": "POST params ignored by IPageParametersEncoder#decodePageParameters()", "B_clean_title": ["post", "param", "ignor", "by", "ipageparametersencod", "page", "paramet", "encod", "decodepageparamet", "decod", "page", "paramet"]},
{"A_title": "inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem.  System.out.println(new BinomialDistributionImpl(1000000 0.5).inverseCumulativeProbability(0.5));  This returns 499525 though it should be 499999.  Im not sure how it should be fixed but the cause is that the cumulativeProbability method returns Infinity not NaN.  As the result the checkedCumulativeProbability method doesnt work as expected.", "A_clean_title": ["inversecumulativeprob", "invers", "cumul", "probabl", "binomialdistribut", "binomi", "distribut", "return", "wrong", "valu", "larg", "trial", "inversecumulativeprob", "invers", "cumul", "probabl", "method", "binomialdistributionimpl", "binomi", "distribut", "impl", "class", "return", "wrong", "valu", "larg", "trial", "follow", "code", "will", "reproduc", "problem", "system", "out", "println", "new", "binomialdistributionimpl", "binomi", "distribut", "impl", "1000000", "inversecumulativeprob", "invers", "cumul", "probabl", "thi", "return", "499525", "though", "it", "499999", "im", "not", "sure", "how", "it", "fix", "but", "caus", "that", "cumulativeprob", "cumul", "probabl", "method", "return", "infin", "not", "nan", "na", "as", "result", "checkedcumulativeprob", "check", "cumul", "probabl", "method", "doesnt", "work", "as", "expect"], "B_title": "Use modified Lentz-Thompson algorithm for continued fraction evaluation.", "B_clean_title": ["use", "modifi", "lentz", "thompson", "algorithm", "continu", "fraction", "evalu"]},
{"A_title": "MockHttpServletRequest is broken when used with CryptedUrlWebRequestCodingStrategyUpgraded to 1.3.6. One of my test cases started to fail with  org.apache.wicket.WicketRuntimeException: Internal error parsing wicket:interface = ?x=GR7uTj8e-D8FE0tmM9vvYcwdiASd9OJ5GgveAhSNaig       I tracked down the issue to MockHttpServletRequest .setRequestToComponent() In line 1253 it check for url starting with 6*. However in CryptedUrlWebRequestCodingStrategy following encryption is employed:  198:queryString = shortenUrl(queryString).toString(); 199: 200:// encrypt the query string 201:String encryptedQueryString = urlCrypt.encryptUrlSafe(queryString);   shortenUrl will replace wicket:interface= with 6* but then it gets immediately encrypted consequently MockHttpServletRequest  will never recognize it correctly.", "A_clean_title": ["mockhttpservletrequest", "mock", "http", "servlet", "request", "broken", "when", "use", "cryptedurlwebrequestcodingstrategyupgrad", "crypt", "url", "web", "request", "code", "strategi", "upgrad", "one", "my", "test", "case", "start", "fail", "org", "apach", "wicket", "wicketruntimeexcept", "wicket", "runtim", "except", "intern", "error", "pars", "wicket", "interfac", "x=gr7utj8", "d8fe0tmm9vvycwdiasd9oj5ggveahsnaig", "x=gr7u", "tj8e", "d8fe0tm", "m9vv", "ycwdi", "sd9oj5ggv", "ah", "naig", "track", "down", "issu", "mockhttpservletrequest", "mock", "http", "servlet", "request", "setrequesttocompon", "set", "request", "compon", "line", "1253", "it", "check", "url", "start", "howev", "cryptedurlwebrequestcodingstrategi", "crypt", "url", "web", "request", "code", "strategi", "follow", "encrypt", "employ", "198", "querystr", "queri", "string", "shortenurl", "shorten", "url", "querystr", "queri", "string", "tostr", "string", "199", "200", "encrypt", "queri", "string", "201", "string", "encryptedquerystr", "encrypt", "queri", "string", "urlcrypt", "encrypturlsaf", "url", "crypt", "encrypt", "url", "safe", "querystr", "queri", "string", "shortenurl", "shorten", "url", "will", "replac", "wicket", "interface=", "but", "then", "it", "get", "immedi", "encrypt", "consequ", "mockhttpservletrequest", "mock", "http", "servlet", "request", "will", "never", "recogn", "it", "correctli"], "B_title": "fixed: MockHttpServletRequest is broken when used with CryptedUrlWebRequestCodingStrategy Issue: WICKET-2281", "B_clean_title": ["fix", "mockhttpservletrequest", "mock", "http", "servlet", "request", "broken", "when", "use", "cryptedurlwebrequestcodingstrategi", "crypt", "url", "web", "request", "code", "strategi", "issu", "wicket", "2281"]},
{"A_title": "AutocompleteTextField after Submit does not workI use an AutocompleteTextfield together with a submit-Button. After once submitting the content oft the AutocompleteTextField the parameter q is added to the URL. After that the autocompletion will only complete the parameter q in the url and not the parameter given by ajax.  I tracked the problem down to the callbackURL.  It contains a pattern looking as follows: ....&q=<paramproducedbysubmit>&q=<paramproducedbyajaxautocomplete>  The callbackurl is build of the parameter q and the extraction of parameters only accepts the first parameter", "A_clean_title": ["autocompletetextfield", "autocomplet", "text", "field", "after", "submit", "not", "worki", "work", "use", "autocompletetextfield", "autocomplet", "textfield", "togeth", "submit", "button", "after", "onc", "submit", "content", "oft", "autocompletetextfield", "autocomplet", "text", "field", "paramet", "ad", "url", "after", "that", "autocomplet", "will", "onli", "complet", "paramet", "url", "not", "paramet", "given", "by", "ajax", "track", "problem", "down", "callbackurl", "callback", "url", "it", "contain", "pattern", "look", "as", "follow", "q=", "paramproducedbysubmit", "q=", "paramproducedbyajaxautocomplet", "callbackurl", "build", "paramet", "extract", "paramet", "onli", "accept", "first", "paramet"], "B_title": "AutocompleteTextField after Submit does not work - PageProvider no longer pushes PageParameters into previously stored pages.", "B_clean_title": ["autocompletetextfield", "autocomplet", "text", "field", "after", "submit", "not", "work", "pageprovid", "page", "provid", "no", "longer", "push", "pageparamet", "page", "paramet", "into", "previous", "store", "page"]},
{"A_title": "Dynamically adding component via an IComponentResolver fails within an enclosure for versions after 1.4.1We have been using an IComponentResolver implementation for a long time to allow the inclusion of certain panels to be determined by the markup. Some panels are included inside enclosures and some are not. Both cases worked fine in wicket 1.4.1 but in versions 1.4.2 and later a Tag expected error occurs if the component is wrapped inside a wicket enclosure.  A quickstart example has been included to demonstrate the problem.", "A_clean_title": ["dynam", "ad", "compon", "via", "icomponentresolv", "compon", "resolv", "fail", "within", "enclosur", "version", "after", "1we", "have", "been", "icomponentresolv", "compon", "resolv", "implement", "long", "time", "allow", "inclus", "certain", "panel", "determin", "by", "markup", "some", "panel", "are", "includ", "insid", "enclosur", "some", "are", "not", "both", "case", "work", "fine", "wicket", "but", "version", "later", "tag", "expect", "error", "occur", "compon", "wrap", "insid", "wicket", "enclosur", "quickstart", "exampl", "ha", "been", "includ", "demonstr", "problem"], "B_title": "fixed WICKET-2882: IComponentResolver usage with Enclosure Issue: WICKET-2882", "B_clean_title": ["fix", "wicket", "2882", "icomponentresolv", "compon", "resolv", "usag", "enclosur", "issu", "wicket", "2882"]},
{"A_title": "Unnecessary invocations of LastRevRecovery when recovery already done.Even after _lastRev recovery executed on a cluster node there are unnecessary  invocations of recovery happening on that cluster node till that cluster node comes online again.", "A_clean_title": ["unnecessari", "invoc", "lastrevrecoveri", "last", "rev", "recoveri", "when", "recoveri", "alreadi", "done", "even", "after", "lastrev", "last", "rev", "recoveri", "execut", "cluster", "node", "there", "are", "unnecessari", "invoc", "recoveri", "happen", "that", "cluster", "node", "till", "that", "cluster", "node", "come", "onlin", "again"], "B_title": "- Unnecessary invocations of LastRevRecovery when recovery already done.", "B_clean_title": ["unnecessari", "invoc", "lastrevrecoveri", "last", "rev", "recoveri", "when", "recoveri", "alreadi", "done"]},
{"A_title": "RegulaFalsiSolver failureThe following unit test:  @Test public void testBug()      final UnivariateRealFunction f = new UnivariateRealFunction()              @Override             public double value(double x)                  return Math.exp(x) - Math.pow(Math.PI 3.0);                      ;      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100 f 1 10);    fails with  illegal state: maximal count (100) exceeded: evaluations   Using PegasusSolver the answer is found after 17 evaluations.", "A_clean_title": ["regulafalsisolv", "regula", "falsi", "solver", "failureth", "failur", "follow", "unit", "test", "test", "public", "void", "testbug", "test", "bug", "final", "univariaterealfunct", "univari", "real", "function", "new", "univariaterealfunct", "univari", "real", "function", "overrid", "public", "doubl", "valu", "doubl", "return", "math", "exp", "math", "pow", "math", "pi", "univariaterealsolv", "univari", "real", "solver", "solver", "new", "regulafalsisolv", "regula", "falsi", "solver", "doubl", "root", "solver", "solv", "100", "10", "fail", "illeg", "state", "maxim", "count", "100", "exceed", "evalu", "pegasussolv", "pegasu", "solver", "answer", "found", "after", "17", "evalu"], "B_title": "Reverted to original behaviour.", "B_clean_title": ["revert", "origin", "behaviour"]},
{"A_title": "UpdateOp.Key.equals() incorrectAs reported on the dev list 0 the equals implementation of UpdateOp.Key is incorrect.  0 http://markmail.org/message/acpg2mhbxjn4lglu", "A_clean_title": ["updateop", "key", "equal", "updat", "op", "incorrecta", "incorrect", "as", "report", "dev", "list", "equal", "implement", "updateop", "key", "updat", "op", "incorrect", "http", "markmail", "org", "messag", "acpg2mhbxjn4lglu"], "B_title": "UpdateOp.Key.equals() incorrect", "B_clean_title": ["updateop", "key", "equal", "updat", "op", "incorrect"]},
{"A_title": "UrlValidator failes to validate urls that containt multiple dots in pathrefer to UrlValidator.java:466 (isValidPath). if we have an url that contains more than two consequent dots for example http://www.somedomain.com/this_one_is_tricky...but...still.....valid validator will fail. btw the other side effect is that countTokens actually counts ... a two 2dots. One possible workaround is not just count .. tokens but count them along with slash like ../.", "A_clean_title": ["urlvalid", "url", "valid", "fail", "valid", "url", "that", "containt", "multipl", "dot", "pathref", "urlvalid", "java:466", "url", "valid", "isvalidpath", "valid", "path", "we", "have", "url", "that", "contain", "more", "than", "two", "consequ", "dot", "exampl", "http", "somedomain", "www", "one", "tricki", "com", "thi", "but", "still", "valid", "valid", "will", "fail", "btw", "other", "side", "effect", "that", "counttoken", "count", "token", "actual", "count", "two", "2dot", "one", "possibl", "workaround", "not", "just", "count", "token", "but", "count", "them", "along", "slash", "like"], "B_title": "Issue: WICKET-3196", "B_clean_title": ["issu", "wicket", "3196"]},
{"A_title": "StrBuilder appendFixedWidth does not handle nullsAppending a null value with fixed width causes a null pointer exception if getNullText() has not been set.", "A_clean_title": ["strbuilder", "str", "builder", "appendfixedwidth", "append", "fix", "width", "not", "handl", "nullsappend", "null", "append", "null", "valu", "fix", "width", "caus", "null", "pointer", "except", "getnulltext", "get", "null", "text", "ha", "not", "been", "set"], "B_title": "Applying my patch from LANG-412; fixing Peter Oxenhams report that the appendFixedWidthPadRight and appendFixedWidthPadLeft are not null safe if the nullText has not been set", "B_clean_title": ["appli", "my", "patch", "lang", "412", "fix", "peter", "oxenham", "report", "that", "appendfixedwidthpadright", "append", "fix", "width", "pad", "right", "appendfixedwidthpadleft", "append", "fix", "width", "pad", "left", "are", "not", "null", "safe", "nulltext", "null", "text", "ha", "not", "been", "set"]},
{"A_title": "Closure removes needed code.None", "A_clean_title": ["closur", "remov", "need", "code", "none"], "B_title": "Automated g4 rollback.", "B_clean_title": ["autom", "g4", "rollback"]},
{"A_title": "TypeExtractor.analyzePojo has some problems around the default constructor detectionIf a class does have a default constructor but the user forgot to make it public then TypeExtractor.analyzePojo still thinks everything is OK so it creates a PojoTypeInfo. Then PojoSerializer.createInstance blows up.  Furthermore a return null seems to be missing from the then case of the if after catching the NoSuchMethodException which would also cause a headache for PojoSerializer.  An additional minor issue is that the word class is printed twice in several places because class.toString also prepends it to the class name.", "A_clean_title": ["typeextractor", "analyzepojo", "type", "extractor", "analyz", "pojo", "ha", "some", "problem", "around", "default", "constructor", "detectionif", "detect", "class", "have", "default", "constructor", "but", "user", "forgot", "make", "it", "public", "then", "typeextractor", "analyzepojo", "type", "extractor", "analyz", "pojo", "still", "think", "everyth", "ok", "so", "it", "creat", "pojotypeinfo", "pojo", "type", "info", "then", "pojoseri", "createinst", "pojo", "serial", "creat", "instanc", "blow", "up", "furthermor", "return", "null", "seem", "miss", "then", "case", "after", "catch", "nosuchmethodexcept", "no", "such", "method", "except", "which", "would", "also", "caus", "headach", "pojoseri", "pojo", "serial", "addit", "minor", "issu", "that", "word", "class", "print", "twice", "sever", "place", "becaus", "class", "tostr", "string", "also", "prepend", "it", "class", "name"], "B_title": "handle the case of a non-public default ctor in TypeExtractor.analyzePojo", "B_clean_title": ["handl", "case", "non", "public", "default", "ctor", "typeextractor", "analyzepojo", "type", "extractor", "analyz", "pojo"]},
{"A_title": "-0.0 becomes 0 even in whitespace modeNone", "A_clean_title": ["becom", "even", "whitespac", "modenon", "mode", "none"], "B_title": "Correct output of -0.0. Fixes issue 582.", "B_clean_title": ["correct", "output", "fix", "issu", "582"]},
{"A_title": "Invalid javascript when setStripJavascriptCommentsAndWhitespace is enabledWhen setStripJavascriptCommentsAndWhitespace is enabled (for example in deployment mode) some javascript files get corrupted. For example the following line (notice the 2 spaces after return) return  this.__unbind__(type fn); is compacted to return this.__unbind__(type fn); which does not execute the unbind function.", "A_clean_title": ["invalid", "javascript", "when", "setstripjavascriptcommentsandwhitespac", "set", "strip", "javascript", "comment", "whitespac", "enabledwhen", "enabl", "when", "setstripjavascriptcommentsandwhitespac", "set", "strip", "javascript", "comment", "whitespac", "enabl", "exampl", "deploy", "mode", "some", "javascript", "file", "get", "corrupt", "exampl", "follow", "line", "notic", "space", "after", "return", "return", "thi", "unbind", "type", "fn", "compact", "return", "thi", "unbind", "type", "fn", "which", "not", "execut", "unbind", "function"], "B_title": "", "B_clean_title": []},
{"A_title": "SimplexSolver not working as expected 2SimplexSolver didnt find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1 b = 1 value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double 7 3 0 0  0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double 1 0 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 0 1 0 0  Relationship.LEQ 1)); podmienky.add(new LinearConstraint(new double 3 0 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 2 0 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 2 -5 0  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 0 3 0 -5  Relationship.LEQ 0)); podmienky.add(new LinearConstraint(new double 3 2 0 0  Relationship.LEQ 5)); podmienky.add(new LinearConstraint(new double 2 3 0 0  Relationship.LEQ 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia podmienky GoalType.MAXIMIZE true); ===================== Results(incorrect): a = 1 b = 0.5 value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).", "A_clean_title": ["simplexsolv", "simplex", "solver", "not", "work", "as", "expect", "2simplexsolv", "2simplex", "solver", "didnt", "find", "optim", "solut", "program", "lpsolv", "object", "function", "max", "constraint", "r1", "+3", "r2", "+2", "r3", "+2", "r4", "+3", "r5", "+3", "+2", "r6", "+2", "+3", "variabl", "bound", "result", "correct", "valu", "10", "program", "simplexsolv", "simplex", "solv", "linearobjectivefunct", "linear", "object", "function", "kritfcia", "krit", "fcia", "new", "linearobjectivefunct", "linear", "object", "function", "new", "doubl", "collect", "linearconstraint", "linear", "constraint", "podmienki", "new", "arraylist", "array", "list", "linearconstraint", "linear", "constraint", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "podmienki", "add", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "relationship", "leq", "simplexsolv", "simplex", "solver", "solver", "new", "simplexsolv", "simplex", "solver", "realpointvaluepair", "real", "point", "valu", "pair", "result", "solver", "optim", "kritfcia", "krit", "fcia", "podmienki", "goaltyp", "maxim", "goal", "type", "true", "result", "incorrect", "valu", "use", "latest", "softwar", "repositori", "includ", "math", "286", "fix"], "B_title": "fixed an error induced by zero entries in simplex solver JIRA: MATH-288", "B_clean_title": ["fix", "error", "induc", "by", "zero", "entri", "simplex", "solver", "jira", "math", "288"]},
{"A_title": "true/false are not always replaced for !0/!1None", "A_clean_title": ["true", "fals", "are", "not", "alway", "replac", "1none"], "B_title": "Fixes issue 787.  Removing or replacing a Node is the AST caused any following Function nodes to be skipped in the late peephole folding. To prevent this save off the next node before visiting a Node.", "B_clean_title": ["fix", "issu", "787", "remov", "or", "replac", "node", "ast", "caus", "ani", "follow", "function", "node", "skip", "late", "peephol", "fold", "prevent", "thi", "save", "off", "next", "node", "befor", "visit", "node"]},
{"A_title": "Missing properties when deserializing using a builder class with a non-default constructor and a mutator annotated with  @JsonUnwrappedWhen deserializing using a builder class with a non-default constructor and any number of mutator methods annotated with @JsonUnwrapped the  BuilderBasedDeserializer::deserializeUsingPropertyBasedWithUnwrapped method cuts short the process of adding SettableBeanProperties. The logic dictates that once all properties necessary to construct the builder have been found the builder is constructed using all known SettableBeanProperties that have been found up to that point in the tokenizing process.  Therefore in the case that the builder has a single property required for construction and that property is found anywhere other than at the end of the JSON content any properties subsequent to the constructor property are not evaluated and are left with their default values.  Given the following classes:   And given the following JSON string:    We will see the following output:   However if we place the  emp_id property at the end of the JSON string we would get the following output:  If we were to place  emp_age and emp_first_name and emp_last_name all after the emp_id property in the JSON string we would get the following output:", "A_clean_title": ["miss", "properti", "when", "deseri", "builder", "class", "non", "default", "constructor", "mutat", "annot", "jsonunwrappedwhen", "json", "unwrap", "when", "deseri", "builder", "class", "non", "default", "constructor", "ani", "number", "mutat", "method", "annot", "jsonunwrap", "json", "unwrap", "builderbaseddeseri", "builder", "base", "deseri", ":deserializeusingpropertybasedwithunwrap", ":deseri", "properti", "base", "unwrap", "method", "cut", "short", "process", "ad", "settablebeanproperti", "settabl", "bean", "properti", "logic", "dictat", "that", "onc", "all", "properti", "necessari", "construct", "builder", "have", "been", "found", "builder", "construct", "all", "known", "settablebeanproperti", "settabl", "bean", "properti", "that", "have", "been", "found", "up", "that", "point", "token", "process", "therefor", "case", "that", "builder", "ha", "singl", "properti", "requir", "construct", "that", "properti", "found", "anywher", "other", "than", "at", "end", "json", "content", "ani", "properti", "subsequ", "constructor", "properti", "are", "not", "evalu", "are", "left", "their", "default", "valu", "given", "follow", "class", "given", "follow", "json", "string", "we", "will", "see", "follow", "output", "howev", "we", "place", "emp", "id", "properti", "at", "end", "json", "string", "we", "would", "get", "follow", "output", "we", "were", "place", "emp", "age", "emp", "first", "name", "emp", "last", "name", "all", "after", "emp", "id", "properti", "json", "string", "we", "would", "get", "follow", "output"], "B_title": "Merge pull request #1574 from jjware/2.8  Fixes #1573", "B_clean_title": ["merg", "pull", "request", "1574", "jjware", "fix", "1573"]},
{"A_title": "Type checking error when replacing a function with a stub after calling.None", "A_clean_title": ["type", "check", "error", "when", "replac", "function", "stub", "after", "call", "none"], "B_title": "Fix issue 586. Distinguishing declared functions from inferred functions is really hard!", "B_clean_title": ["fix", "issu", "586", "distinguish", "declar", "function", "infer", "function", "realli", "hard"]},
{"A_title": "MathUtils.equals(double double) does not work properly for floatsMathUtils.equals(double double) does not work properly for floats.  There is no equals(floatfloat) so float parameters are automatically promoted to double. However that is not necessarily appropriate given that the ULP for a double is much smaller than the ULP for a float.  So for example:  code double oneDouble = 1.0d; assertTrue(MathUtils.equals(oneDouble Double.longBitsToDouble(1 + Double.doubleToLongBits(oneDouble)))); // OK float oneFloat = 1.0f; assertTrue(MathUtils.equals(oneFloat Float.intBitsToFloat(1 + Float.floatToIntBits(oneFloat)))); // FAILS float  f1 = 333.33334f; double d1 = 333.33334d; assertTrue(MathUtils.equals(d1 f1)); // FAILS code  I think the equals() methods need to be duplicated with the appropriate changes for floats to avoid any problems with the promotion of floats.", "A_clean_title": ["mathutil", "equal", "math", "util", "doubl", "doubl", "not", "work", "properli", "floatsmathutil", "equal", "float", "math", "util", "doubl", "doubl", "not", "work", "properli", "float", "there", "no", "equal", "floatfloat", "so", "float", "paramet", "are", "automat", "promot", "doubl", "howev", "that", "not", "necessarili", "appropri", "given", "that", "ulp", "doubl", "much", "smaller", "than", "ulp", "float", "so", "exampl", "code", "doubl", "onedoubl", "one", "doubl", "0d", "asserttru", "assert", "true", "mathutil", "equal", "math", "util", "onedoubl", "one", "doubl", "doubl", "longbitstodoubl", "long", "bit", "doubl", "doubl", "doubletolongbit", "doubl", "long", "bit", "onedoubl", "one", "doubl", "ok", "float", "onefloat", "one", "float", "0f", "asserttru", "assert", "true", "mathutil", "equal", "math", "util", "onefloat", "one", "float", "float", "intbitstofloat", "int", "bit", "float", "float", "floattointbit", "float", "int", "bit", "onefloat", "one", "float", "fail", "float", "f1", "333", "33334f", "doubl", "d1", "333", "33334d", "asserttru", "assert", "true", "mathutil", "equal", "math", "util", "d1", "f1", "fail", "code", "think", "equal", "method", "need", "duplic", "appropri", "chang", "float", "avoid", "ani", "problem", "promot", "float"], "B_title": "MathUtils.equals(double double) does not work properly for floats - add equivalent (float float) methods and basic tests", "B_clean_title": ["mathutil", "equal", "math", "util", "doubl", "doubl", "not", "work", "properli", "float", "add", "equival", "float", "float", "method", "basic", "test"]},
{"A_title": "ArrayIndexOutOfBoundException in EigenDecompositionImplThe following test triggers an ArrayIndexOutOfBoundException:      public void testMath308()           double mainTridiagonal =              22.330154644539597 46.65485522478641 17.393672330044705 54.46687435351116 80.17800767709437         ;         double secondaryTridiagonal =              13.04450406501361 -5.977590941539671 2.9040909856707517 7.1570352792841225         ;          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double refEigenValues =              14.138204224043099 18.847969733754262 52.536278520113882 53.456697699894512 82.044413207204002         ;         RealVector refEigenVectors =              new ArrayRealVector(new double   0.584677060845929 -0.367177264979103 -0.721453187784497  0.052971054621812 -0.005740715188257 )             new ArrayRealVector(new double   0.713933751051495 -0.190582113553930  0.671410443368332 -0.056056055955050  0.006541576993581 )             new ArrayRealVector(new double   0.222368839324646  0.514921891363332 -0.021377019336614  0.801196801016305 -0.207446991247740 )             new ArrayRealVector(new double   0.314647769490148  0.750806415553905 -0.167700312025760 -0.537092972407375  0.143854968127780 )             new ArrayRealVector(new double  -0.000462690386766 -0.002118073109055  0.011530080757413  0.252322434584915  0.967572088232592 )         ;          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal secondaryTridiagonal MathUtils.SAFE_MIN);          double eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i)              assertEquals(refEigenValuesi eigenValuesi 1.0e-6);             if (refEigenVectorsi.dotProduct(decomposition.getEigenvector(i)) < 0)                  assertEquals(0 refEigenVectorsi.add(decomposition.getEigenvector(i)).getNorm() 1.0e-6);              else                  assertEquals(0 refEigenVectorsi.subtract(decomposition.getEigenvector(i)).getNorm() 1.0e-6);                               Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   Im currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.", "A_clean_title": ["arrayindexoutofboundexcept", "array", "index", "out", "bound", "except", "eigendecompositionimplth", "eigen", "decomposit", "impl", "follow", "test", "trigger", "arrayindexoutofboundexcept", "array", "index", "out", "bound", "except", "public", "void", "testmath308", "test", "math308", "doubl", "maintridiagon", "main", "tridiagon", "22", "330154644539597", "46", "65485522478641", "17", "393672330044705", "54", "46687435351116", "80", "17800767709437", "doubl", "secondarytridiagon", "secondari", "tridiagon", "13", "04450406501361", "977590941539671", "9040909856707517", "1570352792841225", "refer", "valu", "have", "been", "comput", "routin", "dstemr", "fortran", "librari", "lapack", "version", "doubl", "refeigenvalu", "ref", "eigen", "valu", "14", "138204224043099", "18", "847969733754262", "52", "536278520113882", "53", "456697699894512", "82", "044413207204002", "realvector", "real", "vector", "refeigenvector", "ref", "eigen", "vector", "new", "arrayrealvector", "array", "real", "vector", "new", "doubl", "584677060845929", "367177264979103", "721453187784497", "052971054621812", "005740715188257", "new", "arrayrealvector", "array", "real", "vector", "new", "doubl", "713933751051495", "190582113553930", "671410443368332", "056056055955050", "006541576993581", "new", "arrayrealvector", "array", "real", "vector", "new", "doubl", "222368839324646", "514921891363332", "021377019336614", "801196801016305", "207446991247740", "new", "arrayrealvector", "array", "real", "vector", "new", "doubl", "314647769490148", "750806415553905", "167700312025760", "537092972407375", "143854968127780", "new", "arrayrealvector", "array", "real", "vector", "new", "doubl", "000462690386766", "002118073109055", "011530080757413", "252322434584915", "967572088232592", "follow", "line", "trigger", "except", "eigendecomposit", "eigen", "decomposit", "decomposit", "new", "eigendecompositionimpl", "eigen", "decomposit", "impl", "maintridiagon", "main", "tridiagon", "secondarytridiagon", "secondari", "tridiagon", "mathutil", "math", "util", "safe", "min", "doubl", "eigenvalu", "eigen", "valu", "decomposit", "getrealeigenvalu", "get", "real", "eigenvalu", "int", "refeigenvalu", "length", "ref", "eigen", "valu", "++i", "assertequ", "assert", "equal", "refeigenvaluesi", "ref", "eigen", "valuesi", "eigenvaluesi", "eigen", "valuesi", "0e", "refeigenvectorsi", "dotproduct", "ref", "eigen", "vectorsi", "dot", "product", "decomposit", "geteigenvector", "get", "eigenvector", "assertequ", "assert", "equal", "refeigenvectorsi", "add", "ref", "eigen", "vectorsi", "decomposit", "geteigenvector", "get", "eigenvector", "getnorm", "get", "norm", "0e", "assertequ", "assert", "equal", "refeigenvectorsi", "subtract", "ref", "eigen", "vectorsi", "decomposit", "geteigenvector", "get", "eigenvector", "getnorm", "get", "norm", "0e", "run", "previou", "method", "as", "junit", "test", "trigger", "except", "when", "eigendecompositionimpl", "eigen", "decomposit", "impl", "instanc", "built", "first", "few", "line", "stack", "trace", "are", "java", "lang", "arrayindexoutofboundsexcept", "array", "index", "out", "bound", "except", "at", "org", "apach", "common", "math", "linear", "eigendecompositionimpl", "computeshiftincr", "eigen", "decomposit", "impl", "comput", "shift", "increment", "eigendecompositionimpl", "java:1545", "eigen", "decomposit", "impl", "at", "org", "apach", "common", "math", "linear", "eigendecompositionimpl", "goodstep", "eigen", "decomposit", "impl", "good", "step", "eigendecompositionimpl", "java:1072", "eigen", "decomposit", "impl", "at", "org", "apach", "common", "math", "linear", "eigendecompositionimpl", "processgeneralblock", "eigen", "decomposit", "impl", "process", "gener", "block", "eigendecompositionimpl", "java:894", "eigen", "decomposit", "impl", "at", "org", "apach", "common", "math", "linear", "eigendecompositionimpl", "findeigenvalu", "eigen", "decomposit", "impl", "find", "eigenvalu", "eigendecompositionimpl", "java:658", "eigen", "decomposit", "impl", "at", "org", "apach", "common", "math", "linear", "eigendecompositionimpl", "decompos", "eigen", "decomposit", "impl", "eigendecompositionimpl", "java:246", "eigen", "decomposit", "impl", "at", "org", "apach", "common", "math", "linear", "eigendecompositionimpl", "eigen", "decomposit", "impl", "init", "eigendecompositionimpl", "java:205", "eigen", "decomposit", "impl", "at", "org", "apach", "common", "math", "linear", "eigendecompositionimpltest", "testmath308", "eigen", "decomposit", "impl", "test", "test", "math308", "eigendecompositionimpltest", "java:136", "eigen", "decomposit", "impl", "test", "im", "current", "investig", "thi", "bug", "it", "not", "simpl", "index", "translat", "error", "between", "origin", "fortran", "lapack", "common", "math", "implement"], "B_title": "fixed an ArrayIndexOutOfBoundsException Kudos to Dimitri who debugged this mess of fortran/java array indices translation JIRA: MATH-308", "B_clean_title": ["fix", "arrayindexoutofboundsexcept", "array", "index", "out", "bound", "except", "kudo", "dimitri", "who", "debug", "thi", "mess", "fortran", "java", "array", "indic", "translat", "jira", "math", "308"]},
{"A_title": "implementation of smallest enclosing ball algorithm sometime failsThe algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases it is not true and one iteration has a smaller ball. In most cases there is no consequence there is just one or two more iterations. However in rare cases discovered while testing 3D this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples", "A_clean_title": ["implement", "smallest", "enclos", "ball", "algorithm", "sometim", "failsth", "fail", "algorithm", "find", "smallest", "ball", "design", "such", "way", "radiu", "strictli", "increas", "at", "each", "iter", "some", "case", "it", "not", "true", "one", "iter", "ha", "smaller", "ball", "most", "case", "there", "no", "consequ", "there", "just", "one", "or", "two", "more", "iter", "howev", "rare", "case", "discov", "while", "test", "3d", "thi", "gener", "infinit", "loop", "some", "veri", "short", "offend", "case", "have", "alreadi", "been", "identifi", "ad", "test", "suit", "these", "case", "are", "current", "deactiv", "main", "repositori", "while", "am", "alreadi", "work", "them", "test", "case", "are", "welzlencloser2dtest", "testreducingbal", "welzl", "encloser2d", "test", "test", "reduc", "ball", "welzlencloser2dtest", "testlargesampl", "welzl", "encloser2d", "test", "test", "larg", "sampl", "welzlencloser3dtest", "testinfiniteloop", "welzl", "encloser3d", "test", "test", "infinit", "loop", "welzlencloser3dtest", "testlargesampl", "welzl", "encloser3d", "test", "test", "larg", "sampl"], "B_title": "Fixed sphere generation in degenerated cases.", "B_clean_title": ["fix", "sphere", "gener", "degener", "case"]},
{"A_title": "math Function math.fraction.ProperFractionFormat.parse(String ParsePosition) return illogical resultHello I find illogical returned result from function Fraction parse(String source  ParsePostion pos) (in class ProperFractionFormat of the Fraction Package) of  the Commons Math library. Please see the following code segment for more  details:  ProperFractionFormat properFormat = new ProperFractionFormat(); result = null; String source = 1 -1 / 2; ParsePosition pos = new ParsePosition(0); //Test 1 : fail  public void testParseNegative()    String source = -1 -2 / 3;    ParsePosition pos = new ParsePosition(0);    Fraction actual = properFormat.parse(source pos);    assertNull(actual);  // Test2: success public void testParseNegative()    String source = -1 -2 / 3;    ParsePosition pos = new ParsePosition(0);    Fraction actual = properFormat.parse(source pos);  // return Fraction 1/3    assertEquals(1 source.getNumerator());    assertEquals(3 source.getDenominator());   Note: Similarly when I passed in the following inputs:    input 2: (source = “1 2 / -3” pos = 0)   input 3: ( source = ” -1 -2 / 3” pos = 0) Function Fraction parse(String ParsePosition) returned Fraction 1/3 (means  the result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs  above. I think the function does not handle parsing the numberator/ denominator  properly incase input string provide invalid numerator/denominator.  Thank you!", "A_clean_title": ["math", "function", "math", "fraction", "properfractionformat", "pars", "proper", "fraction", "format", "string", "parseposit", "pars", "posit", "return", "illog", "resulthello", "result", "hello", "find", "illog", "return", "result", "function", "fraction", "pars", "string", "sourc", "parsepost", "pars", "postion", "po", "class", "properfractionformat", "proper", "fraction", "format", "fraction", "packag", "common", "math", "librari", "pleas", "see", "follow", "code", "segment", "more", "detail", "properfractionformat", "proper", "fraction", "format", "properformat", "proper", "format", "new", "properfractionformat", "proper", "fraction", "format", "result", "null", "string", "sourc", "parseposit", "pars", "posit", "po", "new", "parseposit", "pars", "posit", "test", "fail", "public", "void", "testparseneg", "test", "pars", "neg", "string", "sourc", "parseposit", "pars", "posit", "po", "new", "parseposit", "pars", "posit", "fraction", "actual", "properformat", "pars", "proper", "format", "sourc", "po", "assertnul", "assert", "null", "actual", "test2", "success", "public", "void", "testparseneg", "test", "pars", "neg", "string", "sourc", "parseposit", "pars", "posit", "po", "new", "parseposit", "pars", "posit", "fraction", "actual", "properformat", "pars", "proper", "format", "sourc", "po", "return", "fraction", "assertequ", "assert", "equal", "sourc", "getnumer", "get", "numer", "assertequ", "assert", "equal", "sourc", "getdenomin", "get", "denomin", "note", "similarli", "when", "pass", "follow", "input", "input", "sourc", "po", "input", "sourc", "po", "function", "fraction", "pars", "string", "parseposit", "pars", "posit", "return", "fraction", "mean", "result", "fraction", "had", "numer", "denomin", "all", "input", "abov", "think", "function", "not", "handl", "pars", "number", "denomin", "properli", "incas", "input", "string", "provid", "invalid", "numer", "denomin", "thank", "you"], "B_title": "Modified ProperFractionFormat to reject embedded minus signs. JIRA: MATH-60 Reported by Nhung Nnguyen", "B_clean_title": ["modifi", "properfractionformat", "proper", "fraction", "format", "reject", "embed", "minu", "sign", "jira", "math", "60", "report", "by", "nhung", "nnguyen"]},
{"A_title": "LevenbergMarquardtOptimizer reports 0 iterationsThe method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount() Ive put a test case below. Notice how the evaluations count is correctly incremented but the iterations count is not.      @Test     public void testGetIterations()          // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100) new Target(new double  1 )                 new Weight(new double  1 ) new InitialGuess(                         new double  3 ) new ModelFunction(                         new MultivariateVectorFunction()                              @Override                             public double value(double point)                                     throws IllegalArgumentException                                  return new double  FastMath.pow(point0 4) ;                                                      ) new ModelFunctionJacobian(                         new MultivariateMatrixFunction()                              @Override                             public double value(double point)                                     throws IllegalArgumentException                                  return new double   0.25 * FastMath.pow(                                         point0 3)  ;                                                      ));          // verify         assertThat(otim.getEvaluations() greaterThan(1));         assertThat(otim.getIterations() greaterThan(1));", "A_clean_title": ["levenbergmarquardtoptim", "levenberg", "marquardt", "optim", "report", "iterationsth", "iter", "method", "levenbergmarquardtoptim", "getiter", "levenberg", "marquardt", "optim", "get", "iter", "not", "report", "correct", "number", "iter", "it", "alway", "return", "quick", "look", "at", "code", "show", "that", "onli", "simplexoptim", "simplex", "optim", "call", "baseoptim", "incrementevaluationscount", "base", "optim", "increment", "evalu", "count", "ive", "put", "test", "case", "below", "notic", "how", "evalu", "count", "correctli", "increment", "but", "iter", "count", "not", "test", "public", "void", "testgetiter", "test", "get", "iter", "setup", "levenbergmarquardtoptim", "levenberg", "marquardt", "optim", "otim", "new", "levenbergmarquardtoptim", "levenberg", "marquardt", "optim", "action", "otim", "optim", "new", "maxev", "max", "eval", "100", "new", "target", "new", "doubl", "new", "weight", "new", "doubl", "new", "initialguess", "initi", "guess", "new", "doubl", "new", "modelfunct", "model", "function", "new", "multivariatevectorfunct", "multivari", "vector", "function", "overrid", "public", "doubl", "valu", "doubl", "point", "throw", "illegalargumentexcept", "illeg", "argument", "except", "return", "new", "doubl", "fastmath", "pow", "fast", "math", "point0", "new", "modelfunctionjacobian", "model", "function", "jacobian", "new", "multivariatematrixfunct", "multivari", "matrix", "function", "overrid", "public", "doubl", "valu", "doubl", "point", "throw", "illegalargumentexcept", "illeg", "argument", "except", "return", "new", "doubl", "25", "fastmath", "pow", "fast", "math", "point0", "verifi", "assertthat", "assert", "that", "otim", "getevalu", "get", "evalu", "greaterthan", "greater", "than", "assertthat", "assert", "that", "otim", "getiter", "get", "iter", "greaterthan", "greater", "than"], "B_title": "Increment iteration counter. By default the maximum number of iterations is Integer.MAX_VALUE.", "B_clean_title": ["increment", "iter", "counter", "by", "default", "maximum", "number", "iter", "integ", "max", "valu"]},
{"A_title": "Incorrect output if a function is assigned to a variable and the function contains a variable with the same nameNone", "A_clean_title": ["incorrect", "output", "function", "assign", "variabl", "function", "contain", "variabl", "same", "namenon", "name", "none"], "B_title": "Modify normalization to distinguish function expression names from parameters and local variables. Fixes issue 539.", "B_clean_title": ["modifi", "normal", "distinguish", "function", "express", "name", "paramet", "local", "variabl", "fix", "issu", "539"]},
{"A_title": "XPath: Query with mixed full-text and or conditions failsWhen performing a query like   noformat         //element(* test:Asset)             (                 jcr:contains(. summer)                 or                 jcr:content/metadata/@tags = namespace:season/summer             ) and                 jcr:contains(jcr:content/metadata/@format image)               noformat  The Lucene/Aggregate returns as well nodes that does not match all the criterias.", "A_clean_title": ["xpath", "path", "queri", "mix", "full", "text", "or", "condit", "failswhen", "fail", "when", "perform", "queri", "like", "noformat", "element", "test", "asset", "jcr", "contain", "summer", "or", "jcr", "content", "metadata", "tag", "namespac", "season", "summer", "jcr", "contain", "jcr", "content", "metadata", "format", "imag", "noformat", "lucen", "aggreg", "return", "as", "well", "node", "that", "not", "match", "all", "criteria"], "B_title": "Query with mixed full-text and or conditions fails", "B_clean_title": ["queri", "mix", "full", "text", "or", "condit", "fail"]},
{"A_title": "Initial read of _lastRev creates incorrect RevisionComparatorThe logic in backgroundRead(false) orders the local lastRev  before external lastRev. This the last change done by the local cluster node will look as if it happend before a potentially older external change.", "A_clean_title": ["initi", "read", "lastrev", "last", "rev", "creat", "incorrect", "revisioncomparatorth", "revis", "compar", "logic", "backgroundread", "background", "read", "fals", "order", "local", "lastrev", "last", "rev", "befor", "extern", "lastrev", "last", "rev", "thi", "last", "chang", "done", "by", "local", "cluster", "node", "will", "look", "as", "it", "happend", "befor", "potenti", "older", "extern", "chang"], "B_title": "Initial read of _lastRev creates incorrect RevisionComparator", "B_clean_title": ["initi", "read", "lastrev", "last", "rev", "creat", "incorrect", "revisioncompar", "revis", "compar"]},
{"A_title": "Oak Lucene index doesnt get notified about updates when index is stored on the file systemIt looks like the the lucene IndexTracked class responsible for refreshing the in-memory cache of the lucene index doesnt get the update notification when the index is stored on the file system. This results in searches not working until the next restart", "A_clean_title": ["oak", "lucen", "index", "doesnt", "get", "notifi", "about", "updat", "when", "index", "store", "file", "systemit", "system", "it", "look", "like", "lucen", "indextrack", "index", "track", "class", "respons", "refresh", "memori", "cach", "lucen", "index", "doesnt", "get", "updat", "notif", "when", "index", "store", "file", "system", "thi", "result", "search", "not", "work", "until", "next", "restart"], "B_title": "- Oak Lucene index doesnt get notified about updates when index is stored on the file system", "B_clean_title": ["oak", "lucen", "index", "doesnt", "get", "notifi", "about", "updat", "when", "index", "store", "file", "system"]},
{"A_title": "ExecutionGraph gets stuck in state FAILINGIt is a bit of a rare case but the following can currently happen:    1. Jobs runs for a while some tasks are already finished.   2. Job fails goes to state failing and restarting. Non-finished tasks fail or are canceled.   3. For the finished tasks ask-futures from certain messages (for example for releasing intermediate result partitions) can fail (timeout) and cause the execution to go from FINISHED to FAILED   4. This triggers the execution graph to go to FAILING without ever going further into RESTARTING again   5. The job is stuck  It initially looks like this is mainly an issue for batch jobs (jobs where tasks do finish rather than run infinitely).  The log that shows how this manifests: code -------------------------------------------------------------------------------- 17:19:19782 INFO  akka.event.slf4j.Slf4jLogger                                  - Slf4jLogger started 17:19:19844 INFO  Remoting                                                      - Starting remoting 17:19:20065 INFO  Remoting                                                      - Remoting started; listening on addresses :akka.tcp://flink@127.0.0.1:56722 17:19:20090 INFO  org.apache.flink.runtime.blob.BlobServer                      - Created BLOB server storage directory /tmp/blobStore-6766f51a-1c51-4a03-acfb-08c2c29c11f0 17:19:20096 INFO  org.apache.flink.runtime.blob.BlobServer                      - Started BLOB server at 0.0.0.0:43327 - max concurrent requests: 50 - max backlog: 1000 17:19:20113 INFO  org.apache.flink.runtime.jobmanager.MemoryArchivist           - Started memory archivist akka://flink/user/archive 17:19:20115 INFO  org.apache.flink.runtime.checkpoint.SavepointStoreFactory     - No savepoint state backend configured. Using job manager savepoint state backend. 17:19:20118 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager at akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:20123 INFO  org.apache.flink.runtime.jobmanager.JobManager                - JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager was granted leadership with leader session ID None. 17:19:25605 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:43702/user/taskmanager) as f213232054587f296a12140d56f63ed1. Current number of registered hosts is 1. Current number of alive task slots is 2. 17:19:26758 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:43956/user/taskmanager) as f9e78baa14fb38c69517fb1bcf4f419c. Current number of registered hosts is 2. Current number of alive task slots is 4. 17:19:27064 INFO  org.apache.flink.api.java.ExecutionEnvironment                - The job has 0 registered types and 0 default Kryo serializers 17:19:27071 INFO  org.apache.flink.client.program.Client                        - Starting client actor system 17:19:27072 INFO  org.apache.flink.runtime.client.JobClient                     - Starting JobClient actor system 17:19:27110 INFO  akka.event.slf4j.Slf4jLogger                                  - Slf4jLogger started 17:19:27121 INFO  Remoting                                                      - Starting remoting 17:19:27143 INFO  org.apache.flink.runtime.client.JobClient                     - Started JobClient actor system at 127.0.0.1:51198 17:19:27145 INFO  Remoting                                                      - Remoting started; listening on addresses :akka.tcp://flink@127.0.0.1:51198 17:19:27325 INFO  org.apache.flink.runtime.client.JobClientActor                - Disconnect from JobManager null. 17:19:27362 INFO  org.apache.flink.runtime.client.JobClientActor                - Received job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d). 17:19:27362 INFO  org.apache.flink.runtime.client.JobClientActor                - Could not submit job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d) because there is no connection to a JobManager. 17:19:27379 INFO  org.apache.flink.runtime.client.JobClientActor                - Connect to JobManager Actorakka.tcp://flink@127.0.0.1:56722/user/jobmanager#-1489998809. 17:19:27379 INFO  org.apache.flink.runtime.client.JobClientActor                - Connected to new JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27379 INFO  org.apache.flink.runtime.client.JobClientActor                - Sending message to JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager to submit job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d) and wait for progress 17:19:27380 INFO  org.apache.flink.runtime.client.JobClientActor                - Upload jar files to job manager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27380 INFO  org.apache.flink.runtime.client.JobClientActor                - Submit job to the job manager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27453 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Submitting job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016). 17:19:27591 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Scheduling job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016). 17:19:27592 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from CREATED to SCHEDULED 17:19:27596 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from SCHEDULED to DEPLOYING 17:19:27597 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27606 INFO  org.apache.flink.runtime.client.JobClientActor                - Job was successfully submitted to the JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27630 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to RUNNING. 17:19:27637 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from CREATED to SCHEDULED 17:19:27654 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27Job execution switched to status RUNNING. 17:19:27655 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to SCHEDULED  17:19:27656 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to DEPLOYING  17:19:27666 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from SCHEDULED to DEPLOYING 17:19:27667 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27667 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to SCHEDULED  17:19:27669 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to DEPLOYING  17:19:27681 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from CREATED to SCHEDULED 17:19:27682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from SCHEDULED to DEPLOYING 17:19:27682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from CREATED to SCHEDULED 17:19:27682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from SCHEDULED to DEPLOYING 17:19:27685 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27686 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to SCHEDULED  17:19:27687 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to DEPLOYING  17:19:27687 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to SCHEDULED  17:19:27692 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to DEPLOYING  17:19:27833 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from DEPLOYING to RUNNING 17:19:27839 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to RUNNING  17:19:27840 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from DEPLOYING to RUNNING 17:19:27852 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to RUNNING  17:19:27896 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from DEPLOYING to RUNNING 17:19:27898 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from DEPLOYING to RUNNING 17:19:27901 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to RUNNING  17:19:27905 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to RUNNING  17:19:28114 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from CREATED to SCHEDULED 17:19:28126 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from CREATED to SCHEDULED 17:19:28134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from SCHEDULED to DEPLOYING 17:19:28134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28126 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from CREATED to SCHEDULED 17:19:28139 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from SCHEDULED to DEPLOYING 17:19:28139 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28117 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from CREATED to SCHEDULED 17:19:28134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from SCHEDULED to DEPLOYING 17:19:28140 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28140 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from SCHEDULED to DEPLOYING 17:19:28141 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28147 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to SCHEDULED  17:19:28153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to SCHEDULED  17:19:28153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to DEPLOYING  17:19:28153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to SCHEDULED  17:19:28153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to DEPLOYING  17:19:28156 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to DEPLOYING  17:19:28158 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to SCHEDULED  17:19:28165 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to DEPLOYING  17:19:28238 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from RUNNING to FINISHED 17:19:28242 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to FINISHED  17:19:28308 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from RUNNING to FINISHED 17:19:28315 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from RUNNING to FINISHED 17:19:28317 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to FINISHED  17:19:28318 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to FINISHED  17:19:28328 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from DEPLOYING to RUNNING 17:19:28336 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to RUNNING  17:19:28338 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from DEPLOYING to RUNNING 17:19:28341 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to RUNNING  17:19:28459 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from RUNNING to FINISHED 17:19:28463 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to FINISHED  17:19:28520 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from DEPLOYING to RUNNING 17:19:28529 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to RUNNING  17:19:28540 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from DEPLOYING to RUNNING 17:19:28545 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to RUNNING  17:19:32384 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:60852/user/taskmanager) as 5848d44035a164a0302da6c8701ff748. Current number of registered hosts is 3. Current number of alive task slots is 6. 17:19:32598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from CREATED to SCHEDULED 17:19:32598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from SCHEDULED to DEPLOYING 17:19:32598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:32605 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to SCHEDULED  17:19:32605 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to DEPLOYING  17:19:32611 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from RUNNING to FINISHED 17:19:32614 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to FINISHED  17:19:32717 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from RUNNING to FINISHED 17:19:32719 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to FINISHED  17:19:32724 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from DEPLOYING to RUNNING 17:19:32726 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to RUNNING  17:19:32843 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from RUNNING to FINISHED 17:19:32845 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to FINISHED  17:19:33092 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system akka.tcp://flink@172.17.0.253:43702 has failed address is now gated for 5000 ms. Reason is: Disassociated. 17:19:39111 WARN  Remoting                                                      - Tried to associate with unreachable remote address akka.tcp://flink@172.17.0.253:43702. Address is now gated for 5000 ms all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:19:39113 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Task manager akka.tcp://flink@172.17.0.253:43702/user/taskmanager terminated. 17:19:39114 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from RUNNING to FAILED 17:19:39120 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to FAILED  java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) at akka.actor.Actor class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) at akka.actor.ActorCell.invoke(ActorCell.scala:486) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  17:19:39129 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from RUNNING to CANCELING 17:19:39132 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSink (collect()) (1/1) (895e1ea552281a665ae390c966cdb3b7) switched from CREATED to CANCELED 17:19:39149 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39Job execution switched to status FAILING. java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) at akka.actor.Actor class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) at akka.actor.ActorCell.invoke(ActorCell.scala:486) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) 17:19:39173 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to CANCELING  17:19:39173 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39DataSink (collect())(1/1) switched to CANCELED  17:19:39174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from CANCELING to FAILED 17:19:39177 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to FAILED  java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) at akka.actor.Actor class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) at akka.actor.ActorCell.invoke(ActorCell.scala:486) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  17:19:39179 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39Job execution switched to status RESTARTING. 17:19:39179 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Delaying retry of job execution for 10000 ms ... 17:19:39179 INFO  org.apache.flink.runtime.instance.InstanceManager             - Unregistered task manager akka.tcp://flink@172.17.0.253:43702/user/taskmanager. Number of registered task managers 2. Number of available slots 4. 17:19:39179 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to FAILING. java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) at akka.actor.Actor class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) at akka.actor.ActorCell.invoke(ActorCell.scala:486) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) 17:19:39180 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to RESTARTING. 17:19:42766 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from FINISHED to FAILED 17:19:42773 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:42CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to FAILED  java.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to: at org.apache.flink.runtime.executiongraph.Execution 5.onFailure(Execution.java:915) at akka.dispatch.OnFailure.internal(Future.scala:228) at akka.dispatch.OnFailure.internal(Future.scala:227) at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) at scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25) at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:136) at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:134) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on Actorakka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384 after 10000 ms at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) at java.lang.Thread.run(Thread.java:745)  17:19:42774 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to FAILING. java.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to: at org.apache.flink.runtime.executiongraph.Execution 5.onFailure(Execution.java:915) at akka.dispatch.OnFailure.internal(Future.scala:228) at akka.dispatch.OnFailure.internal(Future.scala:227) at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) at scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25) at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:136) at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:134) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on Actorakka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384 after 10000 ms at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) at java.lang.Thread.run(Thread.java:745) 17:19:42780 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:42Job execution switched to status FAILING. java.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to: at org.apache.flink.runtime.executiongraph.Execution 5.onFailure(Execution.java:915) at akka.dispatch.OnFailure.internal(Future.scala:228) at akka.dispatch.OnFailure.internal(Future.scala:227) at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) at scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25) at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:136) at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:134) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on Actorakka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384 after 10000 ms at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) at java.lang.Thread.run(Thread.java:745) 17:19:49152 WARN  Remoting                                                      - Tried to associate with unreachable remote address akka.tcp://flink@172.17.0.253:43702. Address is now gated for 5000 ms all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:19:59172 WARN  Remoting                                                      - Tried to associate with unreachable remote address akka.tcp://flink@172.17.0.253:43702. Address is now gated for 5000 ms all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:20:09191 WARN  Remoting                                                      - Tried to associate with unreachable remote address akka.tcp://flink@172.17.0.253:43702. Address is now gated for 5000 ms all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:24:32423 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Stopping JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:24:32440 ERROR org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase  -  -------------------------------------------------------------------------------- Test testTaskManagerProcessFailure0(org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase) failed with: java.lang.AssertionError: The program did not finish in time at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertFalse(Assert.java:64) at org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.testTaskManagerProcessFailure(AbstractTaskManagerProcessFailureRecoveryTest.java:212) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.junit.runners.model.FrameworkMethod 1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.rules.TestWatcher 1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) at org.junit.runners.ParentRunner 3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access 000(ParentRunner.java:53) at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:229) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.junit.runners.Suite.runChild(Suite.java:127) at org.junit.runners.Suite.runChild(Suite.java:26) at org.junit.runners.ParentRunner 3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access 000(ParentRunner.java:53) at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:229) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128) code", "A_clean_title": ["executiongraph", "execut", "graph", "get", "stuck", "state", "failingit", "fail", "it", "bit", "rare", "case", "but", "follow", "current", "happen", "job", "run", "while", "some", "task", "are", "alreadi", "finish", "job", "fail", "goe", "state", "fail", "restart", "non", "finish", "task", "fail", "or", "are", "cancel", "finish", "task", "ask", "futur", "certain", "messag", "exampl", "releas", "intermedi", "result", "partit", "fail", "timeout", "caus", "execut", "go", "finish", "fail", "thi", "trigger", "execut", "graph", "go", "fail", "without", "ever", "go", "further", "into", "restart", "again", "job", "stuck", "it", "initi", "look", "like", "thi", "mainli", "issu", "batch", "job", "job", "where", "task", "finish", "rather", "than", "run", "infinit", "log", "that", "show", "how", "thi", "manifest", "code", "17:19:19782", "info", "akka", "event", "slf4j", "slf4jlogger", "slf4j", "logger", "slf4jlogger", "slf4j", "logger", "start", "17:19:19844", "info", "remot", "start", "remot", "17:19:20065", "info", "remot", "remot", "start", "listen", "address", "akka", "tcp", "flink", "127", "1:56722", "17:19:20090", "info", "org", "apach", "flink", "runtim", "blob", "blobserv", "blob", "server", "creat", "blob", "server", "storag", "directori", "6766f51a", "1c51", "4a03", "acfb", "08c2c29c11f0", "tmp", "blobstor", "blob", "store", "17:19:20096", "info", "org", "apach", "flink", "runtim", "blob", "blobserv", "blob", "server", "start", "blob", "server", "at", "0:43327", "max", "concurr", "request", "50", "max", "backlog", "1000", "17:19:20113", "info", "org", "apach", "flink", "runtim", "jobmanag", "memoryarchivist", "memori", "archivist", "start", "memori", "archivist", "akka", "flink", "user", "archiv", "17:19:20115", "info", "org", "apach", "flink", "runtim", "checkpoint", "savepointstorefactori", "savepoint", "store", "factori", "no", "savepoint", "state", "backend", "configur", "job", "manag", "savepoint", "state", "backend", "17:19:20118", "info", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "start", "jobmanag", "job", "manag", "at", "akka", "tcp", "flink", "127", "1:56722", "user", "jobmanag", "17:19:20123", "info", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "jobmanag", "job", "manag", "akka", "tcp", "flink", "127", "1:56722", "user", "jobmanag", "wa", "grant", "leadership", "leader", "session", "id", "none", "17:19:25605", "info", "org", "apach", "flink", "runtim", "instanc", "instancemanag", "instanc", "manag", "regist", "taskmanag", "task", "manag", "at", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "akka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "as", "f213232054587f296a12140d56f63ed1", "current", "number", "regist", "host", "current", "number", "aliv", "task", "slot", "17:19:26758", "info", "org", "apach", "flink", "runtim", "instanc", "instancemanag", "instanc", "manag", "regist", "taskmanag", "task", "manag", "at", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "akka", "tcp", "flink", "172", "17", "253:43956", "user", "taskmanag", "as", "f9e78baa14fb38c69517fb1bcf4f419c", "current", "number", "regist", "host", "current", "number", "aliv", "task", "slot", "17:19:27064", "info", "org", "apach", "flink", "api", "java", "executionenviron", "execut", "environ", "job", "ha", "regist", "type", "default", "kryo", "serial", "17:19:27071", "info", "org", "apach", "flink", "client", "program", "client", "start", "client", "actor", "system", "17:19:27072", "info", "org", "apach", "flink", "runtim", "client", "jobclient", "job", "client", "start", "jobclient", "job", "client", "actor", "system", "17:19:27110", "info", "akka", "event", "slf4j", "slf4jlogger", "slf4j", "logger", "slf4jlogger", "slf4j", "logger", "start", "17:19:27121", "info", "remot", "start", "remot", "17:19:27143", "info", "org", "apach", "flink", "runtim", "client", "jobclient", "job", "client", "start", "jobclient", "job", "client", "actor", "system", "at", "127", "1:51198", "17:19:27145", "info", "remot", "remot", "start", "listen", "address", "akka", "tcp", "flink", "127", "1:51198", "17:19:27325", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "disconnect", "jobmanag", "job", "manag", "null", "17:19:27362", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "receiv", "job", "flink", "java", "job", "at", "mon", "jan", "18", "17:19:27", "utc", "2016", "fa05fd25993a8742da09cc5023c1e38d", "17:19:27362", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "could", "not", "submit", "job", "flink", "java", "job", "at", "mon", "jan", "18", "17:19:27", "utc", "2016", "fa05fd25993a8742da09cc5023c1e38d", "becaus", "there", "no", "connect", "jobmanag", "job", "manag", "17:19:27379", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "connect", "jobmanag", "job", "manag", "actorakka", "tcp", "flink", "127", "1:56722", "user", "jobmanag", "1489998809", "17:19:27379", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "connect", "new", "jobmanag", "job", "manag", "akka", "tcp", "flink", "127", "1:56722", "user", "jobmanag", "17:19:27379", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "send", "messag", "jobmanag", "job", "manag", "akka", "tcp", "flink", "127", "1:56722", "user", "jobmanag", "submit", "job", "flink", "java", "job", "at", "mon", "jan", "18", "17:19:27", "utc", "2016", "fa05fd25993a8742da09cc5023c1e38d", "wait", "progress", "17:19:27380", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "upload", "jar", "file", "job", "manag", "akka", "tcp", "flink", "127", "1:56722", "user", "jobmanag", "17:19:27380", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "submit", "job", "job", "manag", "akka", "tcp", "flink", "127", "1:56722", "user", "jobmanag", "17:19:27453", "info", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "submit", "job", "fa05fd25993a8742da09cc5023c1e38d", "flink", "java", "job", "at", "mon", "jan", "18", "17:19:27", "utc", "2016", "17:19:27591", "info", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "schedul", "job", "fa05fd25993a8742da09cc5023c1e38d", "flink", "java", "job", "at", "mon", "jan", "18", "17:19:27", "utc", "2016", "17:19:27592", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "c79bf4381462c690f5999f2d1949ab50", "switch", "creat", "schedul", "17:19:27596", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "c79bf4381462c690f5999f2d1949ab50", "switch", "schedul", "deploy", "17:19:27597", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "deploy", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "attempt", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "17:19:27606", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "job", "wa", "success", "submit", "jobmanag", "job", "manag", "akka", "tcp", "flink", "127", "1:56722", "user", "jobmanag", "17:19:27630", "info", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "statu", "job", "fa05fd25993a8742da09cc5023c1e38d", "flink", "java", "job", "at", "mon", "jan", "18", "17:19:27", "utc", "2016", "chang", "run", "17:19:27637", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "e73af91028cb76f7d3cd887cb6d66755", "switch", "creat", "schedul", "17:19:27654", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "job", "execut", "switch", "statu", "run", "17:19:27655", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "schedul", "17:19:27656", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "deploy", "17:19:27666", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "e73af91028cb76f7d3cd887cb6d66755", "switch", "schedul", "deploy", "17:19:27667", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "deploy", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "attempt", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "17:19:27667", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "schedul", "17:19:27669", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "deploy", "17:19:27681", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "807daf978da9dc347dca930822c78f8f", "switch", "creat", "schedul", "17:19:27682", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "807daf978da9dc347dca930822c78f8f", "switch", "schedul", "deploy", "17:19:27682", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "deploy", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "attempt", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "17:19:27682", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "ba45c37065b67fc8f5005a50d0e88fff", "switch", "creat", "schedul", "17:19:27682", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "ba45c37065b67fc8f5005a50d0e88fff", "switch", "schedul", "deploy", "17:19:27685", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "deploy", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "attempt", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "17:19:27686", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "schedul", "17:19:27687", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "deploy", "17:19:27687", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "schedul", "17:19:27692", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "deploy", "17:19:27833", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "ba45c37065b67fc8f5005a50d0e88fff", "switch", "deploy", "run", "17:19:27839", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "run", "17:19:27840", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "e73af91028cb76f7d3cd887cb6d66755", "switch", "deploy", "run", "17:19:27852", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "run", "17:19:27896", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "c79bf4381462c690f5999f2d1949ab50", "switch", "deploy", "run", "17:19:27898", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "807daf978da9dc347dca930822c78f8f", "switch", "deploy", "run", "17:19:27901", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "run", "17:19:27905", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:27", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "run", "17:19:28114", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "7997918330ecf2610b3298a8c8ef2852", "switch", "creat", "schedul", "17:19:28126", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "6421c8f88b191ea844619a40a523773b", "switch", "creat", "schedul", "17:19:28134", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "6421c8f88b191ea844619a40a523773b", "switch", "schedul", "deploy", "17:19:28134", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "deploy", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "attempt", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "17:19:28126", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "d0d011dc0a0823bcec5a57a369b334", "switch", "creat", "schedul", "17:19:28139", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "d0d011dc0a0823bcec5a57a369b334", "switch", "schedul", "deploy", "17:19:28139", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "deploy", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "attempt", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "17:19:28117", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "c928d19f73d700e80cdfad650689febb", "switch", "creat", "schedul", "17:19:28134", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "7997918330ecf2610b3298a8c8ef2852", "switch", "schedul", "deploy", "17:19:28140", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "deploy", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "attempt", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "17:19:28140", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "c928d19f73d700e80cdfad650689febb", "switch", "schedul", "deploy", "17:19:28141", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "deploy", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "attempt", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "17:19:28147", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "schedul", "17:19:28153", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "schedul", "17:19:28153", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "deploy", "17:19:28153", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "schedul", "17:19:28153", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "deploy", "17:19:28156", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "deploy", "17:19:28158", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "schedul", "17:19:28165", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "deploy", "17:19:28238", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "e73af91028cb76f7d3cd887cb6d66755", "switch", "run", "finish", "17:19:28242", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "finish", "17:19:28308", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "807daf978da9dc347dca930822c78f8f", "switch", "run", "finish", "17:19:28315", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "c79bf4381462c690f5999f2d1949ab50", "switch", "run", "finish", "17:19:28317", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "finish", "17:19:28318", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "finish", "17:19:28328", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "6421c8f88b191ea844619a40a523773b", "switch", "deploy", "run", "17:19:28336", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "run", "17:19:28338", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "7997918330ecf2610b3298a8c8ef2852", "switch", "deploy", "run", "17:19:28341", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "run", "17:19:28459", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "ba45c37065b67fc8f5005a50d0e88fff", "switch", "run", "finish", "17:19:28463", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "datasourc", "data", "sourc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "org", "apach", "flink", "api", "java", "io", "paralleliteratorinputformat", "parallel", "iter", "input", "format", "switch", "finish", "17:19:28520", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "c928d19f73d700e80cdfad650689febb", "switch", "deploy", "run", "17:19:28529", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "run", "17:19:28540", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "d0d011dc0a0823bcec5a57a369b334", "switch", "deploy", "run", "17:19:28545", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:28", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "run", "17:19:32384", "info", "org", "apach", "flink", "runtim", "instanc", "instancemanag", "instanc", "manag", "regist", "taskmanag", "task", "manag", "at", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "akka", "tcp", "flink", "172", "17", "253:60852", "user", "taskmanag", "as", "5848d44035a164a0302da6c8701ff748", "current", "number", "regist", "host", "current", "number", "aliv", "task", "slot", "17:19:32598", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "reduc", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "d0f8f69f9047c3154b860850955de20f", "switch", "creat", "schedul", "17:19:32598", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "reduc", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "d0f8f69f9047c3154b860850955de20f", "switch", "schedul", "deploy", "17:19:32598", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "deploy", "reduc", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "attempt", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "17:19:32605", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:32", "reduc", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "schedul", "17:19:32605", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:32", "reduc", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "deploy", "17:19:32611", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "c928d19f73d700e80cdfad650689febb", "switch", "run", "finish", "17:19:32614", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:32", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "finish", "17:19:32717", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "6421c8f88b191ea844619a40a523773b", "switch", "run", "finish", "17:19:32719", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:32", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "finish", "17:19:32724", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "reduc", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "d0f8f69f9047c3154b860850955de20f", "switch", "deploy", "run", "17:19:32726", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:32", "reduc", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "run", "17:19:32843", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "d0d011dc0a0823bcec5a57a369b334", "switch", "run", "finish", "17:19:32845", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:32", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "finish", "17:19:33092", "warn", "akka", "remot", "reliabledeliverysupervisor", "reliabl", "deliveri", "supervisor", "associ", "remot", "system", "akka", "tcp", "flink", "172", "17", "253:43702", "ha", "fail", "address", "now", "gate", "5000", "ms", "reason", "disassoci", "17:19:39111", "warn", "remot", "tri", "associ", "unreach", "remot", "address", "akka", "tcp", "flink", "172", "17", "253:43702", "address", "now", "gate", "5000", "ms", "all", "messag", "thi", "address", "will", "deliv", "dead", "letter", "reason", "connect", "refus", "17", "253:43702", "172", "17:19:39113", "info", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "task", "manag", "akka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "termin", "17:19:39114", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "7997918330ecf2610b3298a8c8ef2852", "switch", "run", "fail", "17:19:39120", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:39", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "fail", "java", "lang", "except", "slot", "which", "task", "wa", "execut", "ha", "been", "releas", "probabl", "loss", "taskmanag", "task", "manag", "f213232054587f296a12140d56f63ed1", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "slot", "url", "akka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "at", "org", "apach", "flink", "runtim", "instanc", "simpleslot", "releaseslot", "simpl", "slot", "releas", "slot", "simpleslot", "java:151", "simpl", "slot", "at", "org", "apach", "flink", "runtim", "instanc", "slotsharinggroupassign", "releasesharedslot", "slot", "share", "group", "assign", "releas", "share", "slot", "slotsharinggroupassign", "java:547", "slot", "share", "group", "assign", "at", "org", "apach", "flink", "runtim", "instanc", "sharedslot", "releaseslot", "share", "slot", "releas", "slot", "sharedslot", "java:119", "share", "slot", "at", "org", "apach", "flink", "runtim", "instanc", "instanc", "markdead", "mark", "dead", "instanc", "java:156", "at", "org", "apach", "flink", "runtim", "instanc", "instancemanag", "unregistertaskmanag", "instanc", "manag", "unregist", "task", "manag", "instancemanag", "java:215", "instanc", "manag", "at", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "anonfun", "handlemessag", "handl", "messag", "applyorels", "appli", "or", "jobmanag", "scala:792", "job", "manag", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "mcvl", "mc", "vl", "sp", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:25", "abstract", "partial", "function", "at", "org", "apach", "flink", "runtim", "leadersessionmessagefilt", "leader", "session", "messag", "filter", "anonfun", "receiv", "applyorels", "appli", "or", "leadersessionmessagefilt", "scala:44", "leader", "session", "messag", "filter", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "mcvl", "mc", "vl", "sp", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:25", "abstract", "partial", "function", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "appli", "logmessag", "scala:33", "log", "messag", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "appli", "logmessag", "scala:28", "log", "messag", "at", "scala", "partialfunct", "partial", "function", "class", "applyorels", "appli", "or", "partialfunct", "scala:118", "partial", "function", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "applyorels", "appli", "or", "logmessag", "scala:28", "log", "messag", "at", "akka", "actor", "actor", "class", "aroundrec", "around", "receiv", "actor", "scala:465", "at", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "aroundrec", "job", "manag", "around", "receiv", "jobmanag", "scala:100", "job", "manag", "at", "akka", "actor", "actorcel", "receivemessag", "actor", "cell", "receiv", "messag", "actorcel", "scala:516", "actor", "cell", "at", "akka", "actor", "dungeon", "deathwatch", "death", "watch", "class", "receivedtermin", "receiv", "termin", "deathwatch", "scala:46", "death", "watch", "at", "akka", "actor", "actorcel", "receivedtermin", "actor", "cell", "receiv", "termin", "actorcel", "scala:369", "actor", "cell", "at", "akka", "actor", "actorcel", "autoreceivemessag", "actor", "cell", "auto", "receiv", "messag", "actorcel", "scala:501", "actor", "cell", "at", "akka", "actor", "actorcel", "invok", "actor", "cell", "actorcel", "scala:486", "actor", "cell", "at", "akka", "dispatch", "mailbox", "processmailbox", "process", "mailbox", "mailbox", "scala:254", "at", "akka", "dispatch", "mailbox", "run", "mailbox", "scala:221", "at", "akka", "dispatch", "mailbox", "exec", "mailbox", "scala:231", "at", "scala", "concurr", "forkjoin", "forkjointask", "doexec", "fork", "join", "task", "exec", "forkjointask", "java:260", "fork", "join", "task", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "fork", "join", "pool", "workqueu", "runtask", "work", "queue", "run", "task", "forkjoinpool", "java:1339", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "runwork", "fork", "join", "pool", "run", "worker", "forkjoinpool", "java:1979", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinworkerthread", "run", "fork", "join", "worker", "thread", "forkjoinworkerthread", "java:107", "fork", "join", "worker", "thread", "17:19:39129", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "reduc", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "d0f8f69f9047c3154b860850955de20f", "switch", "run", "cancel", "17:19:39132", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "datasink", "data", "sink", "collect", "895e1ea552281a665ae390c966cdb3b7", "switch", "creat", "cancel", "17:19:39149", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:39", "job", "execut", "switch", "statu", "fail", "java", "lang", "except", "slot", "which", "task", "wa", "execut", "ha", "been", "releas", "probabl", "loss", "taskmanag", "task", "manag", "f213232054587f296a12140d56f63ed1", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "slot", "url", "akka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "at", "org", "apach", "flink", "runtim", "instanc", "simpleslot", "releaseslot", "simpl", "slot", "releas", "slot", "simpleslot", "java:151", "simpl", "slot", "at", "org", "apach", "flink", "runtim", "instanc", "slotsharinggroupassign", "releasesharedslot", "slot", "share", "group", "assign", "releas", "share", "slot", "slotsharinggroupassign", "java:547", "slot", "share", "group", "assign", "at", "org", "apach", "flink", "runtim", "instanc", "sharedslot", "releaseslot", "share", "slot", "releas", "slot", "sharedslot", "java:119", "share", "slot", "at", "org", "apach", "flink", "runtim", "instanc", "instanc", "markdead", "mark", "dead", "instanc", "java:156", "at", "org", "apach", "flink", "runtim", "instanc", "instancemanag", "unregistertaskmanag", "instanc", "manag", "unregist", "task", "manag", "instancemanag", "java:215", "instanc", "manag", "at", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "anonfun", "handlemessag", "handl", "messag", "applyorels", "appli", "or", "jobmanag", "scala:792", "job", "manag", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "mcvl", "mc", "vl", "sp", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:25", "abstract", "partial", "function", "at", "org", "apach", "flink", "runtim", "leadersessionmessagefilt", "leader", "session", "messag", "filter", "anonfun", "receiv", "applyorels", "appli", "or", "leadersessionmessagefilt", "scala:44", "leader", "session", "messag", "filter", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "mcvl", "mc", "vl", "sp", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:25", "abstract", "partial", "function", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "appli", "logmessag", "scala:33", "log", "messag", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "appli", "logmessag", "scala:28", "log", "messag", "at", "scala", "partialfunct", "partial", "function", "class", "applyorels", "appli", "or", "partialfunct", "scala:118", "partial", "function", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "applyorels", "appli", "or", "logmessag", "scala:28", "log", "messag", "at", "akka", "actor", "actor", "class", "aroundrec", "around", "receiv", "actor", "scala:465", "at", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "aroundrec", "job", "manag", "around", "receiv", "jobmanag", "scala:100", "job", "manag", "at", "akka", "actor", "actorcel", "receivemessag", "actor", "cell", "receiv", "messag", "actorcel", "scala:516", "actor", "cell", "at", "akka", "actor", "dungeon", "deathwatch", "death", "watch", "class", "receivedtermin", "receiv", "termin", "deathwatch", "scala:46", "death", "watch", "at", "akka", "actor", "actorcel", "receivedtermin", "actor", "cell", "receiv", "termin", "actorcel", "scala:369", "actor", "cell", "at", "akka", "actor", "actorcel", "autoreceivemessag", "actor", "cell", "auto", "receiv", "messag", "actorcel", "scala:501", "actor", "cell", "at", "akka", "actor", "actorcel", "invok", "actor", "cell", "actorcel", "scala:486", "actor", "cell", "at", "akka", "dispatch", "mailbox", "processmailbox", "process", "mailbox", "mailbox", "scala:254", "at", "akka", "dispatch", "mailbox", "run", "mailbox", "scala:221", "at", "akka", "dispatch", "mailbox", "exec", "mailbox", "scala:231", "at", "scala", "concurr", "forkjoin", "forkjointask", "doexec", "fork", "join", "task", "exec", "forkjointask", "java:260", "fork", "join", "task", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "fork", "join", "pool", "workqueu", "runtask", "work", "queue", "run", "task", "forkjoinpool", "java:1339", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "runwork", "fork", "join", "pool", "run", "worker", "forkjoinpool", "java:1979", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinworkerthread", "run", "fork", "join", "worker", "thread", "forkjoinworkerthread", "java:107", "fork", "join", "worker", "thread", "17:19:39173", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:39", "reduc", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "cancel", "17:19:39173", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:39", "datasink", "data", "sink", "collect", "switch", "cancel", "17:19:39174", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "reduc", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "d0f8f69f9047c3154b860850955de20f", "switch", "cancel", "fail", "17:19:39177", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:39", "reduc", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "fail", "java", "lang", "except", "slot", "which", "task", "wa", "execut", "ha", "been", "releas", "probabl", "loss", "taskmanag", "task", "manag", "f213232054587f296a12140d56f63ed1", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "slot", "url", "akka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "at", "org", "apach", "flink", "runtim", "instanc", "simpleslot", "releaseslot", "simpl", "slot", "releas", "slot", "simpleslot", "java:151", "simpl", "slot", "at", "org", "apach", "flink", "runtim", "instanc", "slotsharinggroupassign", "releasesharedslot", "slot", "share", "group", "assign", "releas", "share", "slot", "slotsharinggroupassign", "java:547", "slot", "share", "group", "assign", "at", "org", "apach", "flink", "runtim", "instanc", "sharedslot", "releaseslot", "share", "slot", "releas", "slot", "sharedslot", "java:119", "share", "slot", "at", "org", "apach", "flink", "runtim", "instanc", "instanc", "markdead", "mark", "dead", "instanc", "java:156", "at", "org", "apach", "flink", "runtim", "instanc", "instancemanag", "unregistertaskmanag", "instanc", "manag", "unregist", "task", "manag", "instancemanag", "java:215", "instanc", "manag", "at", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "anonfun", "handlemessag", "handl", "messag", "applyorels", "appli", "or", "jobmanag", "scala:792", "job", "manag", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "mcvl", "mc", "vl", "sp", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:25", "abstract", "partial", "function", "at", "org", "apach", "flink", "runtim", "leadersessionmessagefilt", "leader", "session", "messag", "filter", "anonfun", "receiv", "applyorels", "appli", "or", "leadersessionmessagefilt", "scala:44", "leader", "session", "messag", "filter", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "mcvl", "mc", "vl", "sp", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:25", "abstract", "partial", "function", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "appli", "logmessag", "scala:33", "log", "messag", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "appli", "logmessag", "scala:28", "log", "messag", "at", "scala", "partialfunct", "partial", "function", "class", "applyorels", "appli", "or", "partialfunct", "scala:118", "partial", "function", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "applyorels", "appli", "or", "logmessag", "scala:28", "log", "messag", "at", "akka", "actor", "actor", "class", "aroundrec", "around", "receiv", "actor", "scala:465", "at", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "aroundrec", "job", "manag", "around", "receiv", "jobmanag", "scala:100", "job", "manag", "at", "akka", "actor", "actorcel", "receivemessag", "actor", "cell", "receiv", "messag", "actorcel", "scala:516", "actor", "cell", "at", "akka", "actor", "dungeon", "deathwatch", "death", "watch", "class", "receivedtermin", "receiv", "termin", "deathwatch", "scala:46", "death", "watch", "at", "akka", "actor", "actorcel", "receivedtermin", "actor", "cell", "receiv", "termin", "actorcel", "scala:369", "actor", "cell", "at", "akka", "actor", "actorcel", "autoreceivemessag", "actor", "cell", "auto", "receiv", "messag", "actorcel", "scala:501", "actor", "cell", "at", "akka", "actor", "actorcel", "invok", "actor", "cell", "actorcel", "scala:486", "actor", "cell", "at", "akka", "dispatch", "mailbox", "processmailbox", "process", "mailbox", "mailbox", "scala:254", "at", "akka", "dispatch", "mailbox", "run", "mailbox", "scala:221", "at", "akka", "dispatch", "mailbox", "exec", "mailbox", "scala:231", "at", "scala", "concurr", "forkjoin", "forkjointask", "doexec", "fork", "join", "task", "exec", "forkjointask", "java:260", "fork", "join", "task", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "fork", "join", "pool", "workqueu", "runtask", "work", "queue", "run", "task", "forkjoinpool", "java:1339", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "runwork", "fork", "join", "pool", "run", "worker", "forkjoinpool", "java:1979", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinworkerthread", "run", "fork", "join", "worker", "thread", "forkjoinworkerthread", "java:107", "fork", "join", "worker", "thread", "17:19:39179", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:39", "job", "execut", "switch", "statu", "restart", "17:19:39179", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "delay", "retri", "job", "execut", "10000", "ms", "17:19:39179", "info", "org", "apach", "flink", "runtim", "instanc", "instancemanag", "instanc", "manag", "unregist", "task", "manag", "akka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "number", "regist", "task", "manag", "number", "avail", "slot", "17:19:39179", "info", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "statu", "job", "fa05fd25993a8742da09cc5023c1e38d", "flink", "java", "job", "at", "mon", "jan", "18", "17:19:27", "utc", "2016", "chang", "fail", "java", "lang", "except", "slot", "which", "task", "wa", "execut", "ha", "been", "releas", "probabl", "loss", "taskmanag", "task", "manag", "f213232054587f296a12140d56f63ed1", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "slot", "url", "akka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "at", "org", "apach", "flink", "runtim", "instanc", "simpleslot", "releaseslot", "simpl", "slot", "releas", "slot", "simpleslot", "java:151", "simpl", "slot", "at", "org", "apach", "flink", "runtim", "instanc", "slotsharinggroupassign", "releasesharedslot", "slot", "share", "group", "assign", "releas", "share", "slot", "slotsharinggroupassign", "java:547", "slot", "share", "group", "assign", "at", "org", "apach", "flink", "runtim", "instanc", "sharedslot", "releaseslot", "share", "slot", "releas", "slot", "sharedslot", "java:119", "share", "slot", "at", "org", "apach", "flink", "runtim", "instanc", "instanc", "markdead", "mark", "dead", "instanc", "java:156", "at", "org", "apach", "flink", "runtim", "instanc", "instancemanag", "unregistertaskmanag", "instanc", "manag", "unregist", "task", "manag", "instancemanag", "java:215", "instanc", "manag", "at", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "anonfun", "handlemessag", "handl", "messag", "applyorels", "appli", "or", "jobmanag", "scala:792", "job", "manag", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "mcvl", "mc", "vl", "sp", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:25", "abstract", "partial", "function", "at", "org", "apach", "flink", "runtim", "leadersessionmessagefilt", "leader", "session", "messag", "filter", "anonfun", "receiv", "applyorels", "appli", "or", "leadersessionmessagefilt", "scala:44", "leader", "session", "messag", "filter", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "mcvl", "mc", "vl", "sp", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:33", "abstract", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "abstract", "partial", "function", "mcvl", "mc", "vl", "sp", "appli", "abstractpartialfunct", "scala:25", "abstract", "partial", "function", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "appli", "logmessag", "scala:33", "log", "messag", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "appli", "logmessag", "scala:28", "log", "messag", "at", "scala", "partialfunct", "partial", "function", "class", "applyorels", "appli", "or", "partialfunct", "scala:118", "partial", "function", "at", "org", "apach", "flink", "runtim", "logmessag", "log", "messag", "anon", "applyorels", "appli", "or", "logmessag", "scala:28", "log", "messag", "at", "akka", "actor", "actor", "class", "aroundrec", "around", "receiv", "actor", "scala:465", "at", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "aroundrec", "job", "manag", "around", "receiv", "jobmanag", "scala:100", "job", "manag", "at", "akka", "actor", "actorcel", "receivemessag", "actor", "cell", "receiv", "messag", "actorcel", "scala:516", "actor", "cell", "at", "akka", "actor", "dungeon", "deathwatch", "death", "watch", "class", "receivedtermin", "receiv", "termin", "deathwatch", "scala:46", "death", "watch", "at", "akka", "actor", "actorcel", "receivedtermin", "actor", "cell", "receiv", "termin", "actorcel", "scala:369", "actor", "cell", "at", "akka", "actor", "actorcel", "autoreceivemessag", "actor", "cell", "auto", "receiv", "messag", "actorcel", "scala:501", "actor", "cell", "at", "akka", "actor", "actorcel", "invok", "actor", "cell", "actorcel", "scala:486", "actor", "cell", "at", "akka", "dispatch", "mailbox", "processmailbox", "process", "mailbox", "mailbox", "scala:254", "at", "akka", "dispatch", "mailbox", "run", "mailbox", "scala:221", "at", "akka", "dispatch", "mailbox", "exec", "mailbox", "scala:231", "at", "scala", "concurr", "forkjoin", "forkjointask", "doexec", "fork", "join", "task", "exec", "forkjointask", "java:260", "fork", "join", "task", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "fork", "join", "pool", "workqueu", "runtask", "work", "queue", "run", "task", "forkjoinpool", "java:1339", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "runwork", "fork", "join", "pool", "run", "worker", "forkjoinpool", "java:1979", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinworkerthread", "run", "fork", "join", "worker", "thread", "forkjoinworkerthread", "java:107", "fork", "join", "worker", "thread", "17:19:39180", "info", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "statu", "job", "fa05fd25993a8742da09cc5023c1e38d", "flink", "java", "job", "at", "mon", "jan", "18", "17:19:27", "utc", "2016", "chang", "restart", "17:19:42766", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "d0d011dc0a0823bcec5a57a369b334", "switch", "finish", "fail", "17:19:42773", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:42", "chain", "partit", "map", "map", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "combin", "reduc", "at", "testtaskmanagerfailur", "test", "task", "manag", "failur", "taskmanagerprocessfailurebatchrecoveryitcas", "java:73", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "switch", "fail", "java", "lang", "illegalstateexcept", "illeg", "state", "except", "updat", "task", "instanc", "f213232054587f296a12140d56f63ed1", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "slot", "url", "akka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "fail", "due", "at", "org", "apach", "flink", "runtim", "executiongraph", "execut", "onfailur", "failur", "execut", "java:915", "at", "akka", "dispatch", "onfailur", "intern", "failur", "futur", "scala:228", "at", "akka", "dispatch", "onfailur", "intern", "failur", "futur", "scala:227", "at", "akka", "dispatch", "japi", "callbackbridg", "appli", "callback", "bridg", "futur", "scala:174", "at", "akka", "dispatch", "japi", "callbackbridg", "appli", "callback", "bridg", "futur", "scala:171", "at", "scala", "partialfunct", "partial", "function", "class", "applyorels", "appli", "or", "partialfunct", "scala:118", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "applyorels", "abstract", "partial", "function", "appli", "or", "abstractpartialfunct", "scala:25", "abstract", "partial", "function", "at", "scala", "concurr", "futur", "anonfun", "onfailur", "failur", "appli", "futur", "scala:136", "at", "scala", "concurr", "futur", "anonfun", "onfailur", "failur", "appli", "futur", "scala:134", "at", "scala", "concurr", "impl", "callbackrunn", "run", "callback", "runnabl", "promis", "scala:32", "at", "scala", "concurr", "impl", "executioncontextimpl", "execut", "context", "impl", "anon", "exec", "executioncontextimpl", "scala:107", "execut", "context", "impl", "at", "scala", "concurr", "forkjoin", "forkjointask", "doexec", "fork", "join", "task", "exec", "forkjointask", "java:260", "fork", "join", "task", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "fork", "join", "pool", "workqueu", "runtask", "work", "queue", "run", "task", "forkjoinpool", "java:1339", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "runwork", "fork", "join", "pool", "run", "worker", "forkjoinpool", "java:1979", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinworkerthread", "run", "fork", "join", "worker", "thread", "forkjoinworkerthread", "java:107", "fork", "join", "worker", "thread", "caus", "by", "akka", "pattern", "asktimeoutexcept", "ask", "timeout", "except", "ask", "time", "out", "actorakka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "1712955384", "after", "10000", "ms", "at", "akka", "pattern", "promiseactorref", "promis", "actor", "ref", "anonfun", "appli", "mcv", "mc", "sp", "asksupport", "scala:333", "ask", "support", "at", "akka", "actor", "schedul", "anon", "run", "schedul", "scala:117", "at", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "unbatchedexecut", "unbatch", "execut", "futur", "scala:694", "at", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "execut", "futur", "scala:691", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "taskhold", "executetask", "task", "holder", "execut", "task", "schedul", "scala:467", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "executebucket", "execut", "bucket", "schedul", "scala:419", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "nexttick", "next", "tick", "schedul", "scala:423", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "run", "schedul", "scala:375", "at", "java", "lang", "thread", "run", "thread", "java:745", "17:19:42774", "info", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "statu", "job", "fa05fd25993a8742da09cc5023c1e38d", "flink", "java", "job", "at", "mon", "jan", "18", "17:19:27", "utc", "2016", "chang", "fail", "java", "lang", "illegalstateexcept", "illeg", "state", "except", "updat", "task", "instanc", "f213232054587f296a12140d56f63ed1", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "slot", "url", "akka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "fail", "due", "at", "org", "apach", "flink", "runtim", "executiongraph", "execut", "onfailur", "failur", "execut", "java:915", "at", "akka", "dispatch", "onfailur", "intern", "failur", "futur", "scala:228", "at", "akka", "dispatch", "onfailur", "intern", "failur", "futur", "scala:227", "at", "akka", "dispatch", "japi", "callbackbridg", "appli", "callback", "bridg", "futur", "scala:174", "at", "akka", "dispatch", "japi", "callbackbridg", "appli", "callback", "bridg", "futur", "scala:171", "at", "scala", "partialfunct", "partial", "function", "class", "applyorels", "appli", "or", "partialfunct", "scala:118", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "applyorels", "abstract", "partial", "function", "appli", "or", "abstractpartialfunct", "scala:25", "abstract", "partial", "function", "at", "scala", "concurr", "futur", "anonfun", "onfailur", "failur", "appli", "futur", "scala:136", "at", "scala", "concurr", "futur", "anonfun", "onfailur", "failur", "appli", "futur", "scala:134", "at", "scala", "concurr", "impl", "callbackrunn", "run", "callback", "runnabl", "promis", "scala:32", "at", "scala", "concurr", "impl", "executioncontextimpl", "execut", "context", "impl", "anon", "exec", "executioncontextimpl", "scala:107", "execut", "context", "impl", "at", "scala", "concurr", "forkjoin", "forkjointask", "doexec", "fork", "join", "task", "exec", "forkjointask", "java:260", "fork", "join", "task", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "fork", "join", "pool", "workqueu", "runtask", "work", "queue", "run", "task", "forkjoinpool", "java:1339", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "runwork", "fork", "join", "pool", "run", "worker", "forkjoinpool", "java:1979", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinworkerthread", "run", "fork", "join", "worker", "thread", "forkjoinworkerthread", "java:107", "fork", "join", "worker", "thread", "caus", "by", "akka", "pattern", "asktimeoutexcept", "ask", "timeout", "except", "ask", "time", "out", "actorakka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "1712955384", "after", "10000", "ms", "at", "akka", "pattern", "promiseactorref", "promis", "actor", "ref", "anonfun", "appli", "mcv", "mc", "sp", "asksupport", "scala:333", "ask", "support", "at", "akka", "actor", "schedul", "anon", "run", "schedul", "scala:117", "at", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "unbatchedexecut", "unbatch", "execut", "futur", "scala:694", "at", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "execut", "futur", "scala:691", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "taskhold", "executetask", "task", "holder", "execut", "task", "schedul", "scala:467", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "executebucket", "execut", "bucket", "schedul", "scala:419", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "nexttick", "next", "tick", "schedul", "scala:423", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "run", "schedul", "scala:375", "at", "java", "lang", "thread", "run", "thread", "java:745", "17:19:42780", "info", "org", "apach", "flink", "runtim", "client", "jobclientactor", "job", "client", "actor", "01", "18", "2016", "17:19:42", "job", "execut", "switch", "statu", "fail", "java", "lang", "illegalstateexcept", "illeg", "state", "except", "updat", "task", "instanc", "f213232054587f296a12140d56f63ed1", "test", "worker", "linux", "docker", "e6d6931f", "3200", "linux", "slot", "url", "akka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "fail", "due", "at", "org", "apach", "flink", "runtim", "executiongraph", "execut", "onfailur", "failur", "execut", "java:915", "at", "akka", "dispatch", "onfailur", "intern", "failur", "futur", "scala:228", "at", "akka", "dispatch", "onfailur", "intern", "failur", "futur", "scala:227", "at", "akka", "dispatch", "japi", "callbackbridg", "appli", "callback", "bridg", "futur", "scala:174", "at", "akka", "dispatch", "japi", "callbackbridg", "appli", "callback", "bridg", "futur", "scala:171", "at", "scala", "partialfunct", "partial", "function", "class", "applyorels", "appli", "or", "partialfunct", "scala:118", "partial", "function", "at", "scala", "runtim", "abstractpartialfunct", "applyorels", "abstract", "partial", "function", "appli", "or", "abstractpartialfunct", "scala:25", "abstract", "partial", "function", "at", "scala", "concurr", "futur", "anonfun", "onfailur", "failur", "appli", "futur", "scala:136", "at", "scala", "concurr", "futur", "anonfun", "onfailur", "failur", "appli", "futur", "scala:134", "at", "scala", "concurr", "impl", "callbackrunn", "run", "callback", "runnabl", "promis", "scala:32", "at", "scala", "concurr", "impl", "executioncontextimpl", "execut", "context", "impl", "anon", "exec", "executioncontextimpl", "scala:107", "execut", "context", "impl", "at", "scala", "concurr", "forkjoin", "forkjointask", "doexec", "fork", "join", "task", "exec", "forkjointask", "java:260", "fork", "join", "task", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "fork", "join", "pool", "workqueu", "runtask", "work", "queue", "run", "task", "forkjoinpool", "java:1339", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "runwork", "fork", "join", "pool", "run", "worker", "forkjoinpool", "java:1979", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinworkerthread", "run", "fork", "join", "worker", "thread", "forkjoinworkerthread", "java:107", "fork", "join", "worker", "thread", "caus", "by", "akka", "pattern", "asktimeoutexcept", "ask", "timeout", "except", "ask", "time", "out", "actorakka", "tcp", "flink", "172", "17", "253:43702", "user", "taskmanag", "1712955384", "after", "10000", "ms", "at", "akka", "pattern", "promiseactorref", "promis", "actor", "ref", "anonfun", "appli", "mcv", "mc", "sp", "asksupport", "scala:333", "ask", "support", "at", "akka", "actor", "schedul", "anon", "run", "schedul", "scala:117", "at", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "unbatchedexecut", "unbatch", "execut", "futur", "scala:694", "at", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "execut", "futur", "scala:691", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "taskhold", "executetask", "task", "holder", "execut", "task", "schedul", "scala:467", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "executebucket", "execut", "bucket", "schedul", "scala:419", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "nexttick", "next", "tick", "schedul", "scala:423", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "run", "schedul", "scala:375", "at", "java", "lang", "thread", "run", "thread", "java:745", "17:19:49152", "warn", "remot", "tri", "associ", "unreach", "remot", "address", "akka", "tcp", "flink", "172", "17", "253:43702", "address", "now", "gate", "5000", "ms", "all", "messag", "thi", "address", "will", "deliv", "dead", "letter", "reason", "connect", "refus", "17", "253:43702", "172", "17:19:59172", "warn", "remot", "tri", "associ", "unreach", "remot", "address", "akka", "tcp", "flink", "172", "17", "253:43702", "address", "now", "gate", "5000", "ms", "all", "messag", "thi", "address", "will", "deliv", "dead", "letter", "reason", "connect", "refus", "17", "253:43702", "172", "17:20:09191", "warn", "remot", "tri", "associ", "unreach", "remot", "address", "akka", "tcp", "flink", "172", "17", "253:43702", "address", "now", "gate", "5000", "ms", "all", "messag", "thi", "address", "will", "deliv", "dead", "letter", "reason", "connect", "refus", "17", "253:43702", "172", "17:24:32423", "info", "org", "apach", "flink", "runtim", "jobmanag", "jobmanag", "job", "manag", "stop", "jobmanag", "job", "manag", "akka", "tcp", "flink", "127", "1:56722", "user", "jobmanag", "17:24:32440", "error", "org", "apach", "flink", "test", "recoveri", "taskmanagerprocessfailurebatchrecoveryitcas", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "test", "testtaskmanagerprocessfailure0", "test", "task", "manag", "process", "failure0", "org", "apach", "flink", "test", "recoveri", "taskmanagerprocessfailurebatchrecoveryitcas", "task", "manag", "process", "failur", "batch", "recoveri", "it", "case", "fail", "java", "lang", "assertionerror", "assert", "error", "program", "did", "not", "finish", "time", "at", "org", "junit", "assert", "fail", "assert", "java:88", "at", "org", "junit", "assert", "asserttru", "assert", "true", "assert", "java:41", "at", "org", "junit", "assert", "assertfals", "assert", "fals", "assert", "java:64", "at", "org", "apach", "flink", "test", "recoveri", "abstracttaskmanagerprocessfailurerecoverytest", "testtaskmanagerprocessfailur", "abstract", "task", "manag", "process", "failur", "recoveri", "test", "test", "task", "manag", "process", "failur", "abstracttaskmanagerprocessfailurerecoverytest", "java:212", "abstract", "task", "manag", "process", "failur", "recoveri", "test", "at", "sun", "reflect", "nativemethodaccessorimpl", "invoke0", "nativ", "method", "accessor", "impl", "nativ", "method", "at", "sun", "reflect", "nativemethodaccessorimpl", "invok", "nativ", "method", "accessor", "impl", "nativemethodaccessorimpl", "java:57", "nativ", "method", "accessor", "impl", "at", "sun", "reflect", "delegatingmethodaccessorimpl", "invok", "deleg", "method", "accessor", "impl", "delegatingmethodaccessorimpl", "java:43", "deleg", "method", "accessor", "impl", "at", "java", "lang", "reflect", "method", "invok", "method", "java:606", "at", "org", "junit", "runner", "model", "frameworkmethod", "framework", "method", "runreflectivecal", "run", "reflect", "call", "frameworkmethod", "java:47", "framework", "method", "at", "org", "junit", "intern", "runner", "model", "reflectivecal", "run", "reflect", "callabl", "reflectivecal", "java:12", "reflect", "callabl", "at", "org", "junit", "runner", "model", "frameworkmethod", "invokeexplos", "framework", "method", "invok", "explos", "frameworkmethod", "java:44", "framework", "method", "at", "org", "junit", "intern", "runner", "statement", "invokemethod", "evalu", "invok", "method", "invokemethod", "java:17", "invok", "method", "at", "org", "junit", "rule", "testwatch", "test", "watcher", "evalu", "testwatch", "java:55", "test", "watcher", "at", "org", "junit", "rule", "runrul", "evalu", "run", "rule", "runrul", "java:20", "run", "rule", "at", "org", "junit", "runner", "parentrunn", "runleaf", "parent", "runner", "run", "leaf", "parentrunn", "java:271", "parent", "runner", "at", "org", "junit", "runner", "blockjunit4classrunn", "runchild", "block", "unit4class", "runner", "run", "child", "blockjunit4classrunn", "java:70", "block", "unit4class", "runner", "at", "org", "junit", "runner", "blockjunit4classrunn", "runchild", "block", "unit4class", "runner", "run", "child", "blockjunit4classrunn", "java:50", "block", "unit4class", "runner", "at", "org", "junit", "runner", "parentrunn", "parent", "runner", "run", "parentrunn", "java:238", "parent", "runner", "at", "org", "junit", "runner", "parentrunn", "parent", "runner", "schedul", "parentrunn", "java:63", "parent", "runner", "at", "org", "junit", "runner", "parentrunn", "runchildren", "parent", "runner", "run", "children", "parentrunn", "java:236", "parent", "runner", "at", "org", "junit", "runner", "parentrunn", "access", "parent", "runner", "000", "parentrunn", "java:53", "parent", "runner", "at", "org", "junit", "runner", "parentrunn", "parent", "runner", "evalu", "parentrunn", "java:229", "parent", "runner", "at", "org", "junit", "runner", "parentrunn", "run", "parent", "runner", "parentrunn", "java:309", "parent", "runner", "at", "org", "junit", "runner", "suit", "runchild", "run", "child", "suit", "java:127", "at", "org", "junit", "runner", "suit", "runchild", "run", "child", "suit", "java:26", "at", "org", "junit", "runner", "parentrunn", "parent", "runner", "run", "parentrunn", "java:238", "parent", "runner", "at", "org", "junit", "runner", "parentrunn", "parent", "runner", "schedul", "parentrunn", "java:63", "parent", "runner", "at", "org", "junit", "runner", "parentrunn", "runchildren", "parent", "runner", "run", "children", "parentrunn", "java:236", "parent", "runner", "at", "org", "junit", "runner", "parentrunn", "access", "parent", "runner", "000", "parentrunn", "java:53", "parent", "runner", "at", "org", "junit", "runner", "parentrunn", "parent", "runner", "evalu", "parentrunn", "java:229", "parent", "runner", "at", "org", "junit", "runner", "parentrunn", "run", "parent", "runner", "parentrunn", "java:309", "parent", "runner", "at", "org", "apach", "maven", "surefir", "junit4", "junit4provid", "execut", "unit4provid", "junit4provid", "java:283", "unit4provid", "at", "org", "apach", "maven", "surefir", "junit4", "junit4provid", "executewithrerun", "unit4provid", "execut", "rerun", "junit4provid", "java:173", "unit4provid", "at", "org", "apach", "maven", "surefir", "junit4", "junit4provid", "executetestset", "unit4provid", "execut", "test", "set", "junit4provid", "java:153", "unit4provid", "at", "org", "apach", "maven", "surefir", "junit4", "junit4provid", "invok", "unit4provid", "junit4provid", "java:128", "unit4provid", "code"], "B_title": "runtime Enforce terminal state of Executions", "B_clean_title": ["runtim", "enforc", "termin", "state", "execut"]},
{"A_title": "SplitOperations may not retain most recent committed _commitRoot entryIn some rare cases it may happen that SplitOperations does not retain the most recent committed _commitRoot entry on a document. This may result in an undetected hierarchy conflict.", "A_clean_title": ["splitoper", "split", "oper", "may", "not", "retain", "most", "recent", "commit", "commitroot", "commit", "root", "entryin", "entri", "some", "rare", "case", "it", "may", "happen", "that", "splitoper", "split", "oper", "not", "retain", "most", "recent", "commit", "commitroot", "commit", "root", "entri", "document", "thi", "may", "result", "undetect", "hierarchi", "conflict"], "B_title": "SplitOperations may not retain most recent committed _commitRoot entry", "B_clean_title": ["splitoper", "split", "oper", "may", "not", "retain", "most", "recent", "commit", "commitroot", "commit", "root", "entri"]},
{"A_title": "Generates code with invalid for/in left-hand assignmentNone", "A_clean_title": ["gener", "code", "invalid", "left", "hand", "assignmentnon", "assign", "none"], "B_title": "Printing of IN statement inside HOOK inside FOR.", "B_clean_title": ["print", "statement", "insid", "hook", "insid"]},
{"A_title": "Node#setProperty(String Calendar) doesnt take time zone in accountNode#setProperty(String Calendar) doesnt take time zone in account.  It looks the Calendar value is straightly stored as a long without take in consideration the time zone  Unit test to follow", "A_clean_title": ["node", "setproperti", "set", "properti", "string", "calendar", "doesnt", "take", "time", "zone", "accountnod", "account", "node", "setproperti", "set", "properti", "string", "calendar", "doesnt", "take", "time", "zone", "account", "it", "look", "calendar", "valu", "straightli", "store", "as", "long", "without", "take", "consider", "time", "zone", "unit", "test", "follow"], "B_title": "Node#setProperty(String Calendar) doesnt take time zone in account", "B_clean_title": ["node", "setproperti", "set", "properti", "string", "calendar", "doesnt", "take", "time", "zone", "account"]},
{"A_title": "NumberTextField doesnt accept values <=0 for Double and FloatThe org.apache.wicket.util.lang.Numbers class defines the method : public static Number getMinValue(Class<? extends Number> numberType)  This method return the MatchingNumberTypeClass.MIN_VALUE. But for Double.MIN_VALUE and Float.MIN_VALUE return the smallest positive number not the smallest negative number like for the other number classes.  One side effect is that by default you cant enter a negative value or a 0 in a NumberTextField<Double> or NumberTextField<Float>.", "A_clean_title": ["numbertextfield", "number", "text", "field", "doesnt", "accept", "valu", "=0", "doubl", "floatth", "float", "org", "apach", "wicket", "util", "lang", "number", "class", "defin", "method", "public", "static", "number", "getminvalu", "get", "min", "valu", "class", "extend", "number", "numbertyp", "number", "type", "thi", "method", "return", "matchingnumbertypeclass", "match", "number", "type", "class", "min", "valu", "but", "doubl", "min", "valu", "float", "min", "valu", "return", "smallest", "posit", "number", "not", "smallest", "neg", "number", "like", "other", "number", "class", "one", "side", "effect", "that", "by", "default", "you", "cant", "enter", "neg", "valu", "or", "numbertextfield", "number", "text", "field", "doubl", "or", "numbertextfield", "number", "text", "field", "float"], "B_title": "MIN_VALUE of Double and Float is not minimum number", "B_clean_title": ["min", "valu", "doubl", "float", "not", "minimum", "number"]},
{"A_title": "Unsigned Shift Right (>>>) bug operating on negative numbersNone", "A_clean_title": ["unsign", "shift", "right", "bug", "oper", "neg", "numbersnon", "number", "none"], "B_title": "Propertly fold >>> 0 on negative values. Fixes issue 200", "B_clean_title": ["propertli", "fold", "neg", "valu", "fix", "issu", "200"]},
{"A_title": "MathUtils.gcd(Integer.MIN_VALUE 0) should throw an Exception instead of returning Integer.MIN_VALUEThe gcd method should throw an Exception for gcd(Integer.MIN_VALUE 0) like for gcd(Integer.MIN_VALUE Integer.MIN_VALUE). The method should only return nonnegative results.", "A_clean_title": ["mathutil", "gcd", "math", "util", "integ", "min", "valu", "throw", "except", "instead", "return", "integ", "min", "valueth", "valu", "gcd", "method", "throw", "except", "gcd", "integ", "min", "valu", "like", "gcd", "integ", "min", "valu", "integ", "min", "valu", "method", "onli", "return", "nonneg", "result"], "B_title": "Fixed an error in computing gcd and lcm for some extreme values at integer range boundaries. JIRA: MATH-243", "B_clean_title": ["fix", "error", "comput", "gcd", "lcm", "some", "extrem", "valu", "at", "integ", "rang", "boundari", "jira", "math", "243"]},
{"A_title": "getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries.  The current implementation in ArrayRealVector has a typo:  code     public double getLInfNorm()          double max = 0;         for (double a : data)              max += Math.max(max Math.abs(a));                  return max;      code  the += should just be an =.  There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test not a test for correctness).  Worse the implementation in OpenMapRealVector is not even positive semi-definite:  code        public double getLInfNorm()          double max = 0;         Iterator iter = entries.iterator();         while (iter.hasNext())              iter.advance();             max += iter.value();                  return max;      code  I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():  code   public double getLInfNorm()      double norm = 0;     Iterator<Entry> it = sparseIterator();     Entry e;     while(it.hasNext() && (e = it.next()) != null)        norm = Math.max(norm Math.abs(e.getValue()));          return norm;    code  Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.", "A_clean_title": ["getlinfnorm", "get", "inf", "norm", "use", "wrong", "formula", "both", "arrayrealvector", "array", "real", "vector", "openmaprealvector", "open", "map", "real", "vector", "differ", "way", "infin", "norm", "finit", "dimension", "vector", "just", "max", "absolut", "valu", "it", "entri", "current", "implement", "arrayrealvector", "array", "real", "vector", "ha", "typo", "code", "public", "doubl", "getlinfnorm", "get", "inf", "norm", "doubl", "max", "doubl", "data", "max", "math", "max", "max", "math", "ab", "return", "max", "code", "just", "there", "sadli", "unit", "test", "assur", "us", "that", "thi", "correct", "behavior", "effect", "regress", "onli", "test", "not", "test", "correct", "wors", "implement", "openmaprealvector", "open", "map", "real", "vector", "not", "even", "posit", "semi", "definit", "code", "public", "doubl", "getlinfnorm", "get", "inf", "norm", "doubl", "max", "iter", "iter", "entri", "iter", "while", "iter", "hasnext", "ha", "next", "iter", "advanc", "max", "iter", "valu", "return", "max", "code", "would", "suggest", "that", "thi", "method", "move", "up", "abstractrealvector", "abstract", "real", "vector", "superclass", "implement", "sparseiter", "spars", "iter", "code", "public", "doubl", "getlinfnorm", "get", "inf", "norm", "doubl", "norm", "iter", "entri", "it", "sparseiter", "spars", "iter", "entri", "while", "it", "hasnext", "ha", "next", "it", "next", "null", "norm", "math", "max", "norm", "math", "ab", "getvalu", "get", "valu", "return", "norm", "code", "unit", "test", "neg", "valu", "vector", "would", "help", "check", "thi", "kind", "thing", "futur"], "B_title": "fixed a wrong implementation of the Linf vector norm JIRA: MATH-326", "B_clean_title": ["fix", "wrong", "implement", "linf", "vector", "norm", "jira", "math", "326"]},
{"A_title": "In ADVANCED mode Compiler fails to warn about overridden methods with different signatures.None", "A_clean_title": ["advanc", "mode", "compil", "fail", "warn", "about", "overridden", "method", "differ", "signatur", "none"], "B_title": "When inferring a function type there may be formal parameters that do not appear in the function literal Fixes issue 368", "B_clean_title": ["when", "infer", "function", "type", "there", "may", "formal", "paramet", "that", "not", "appear", "function", "liter", "fix", "issu", "368"]},
{"A_title": "Null Pointer when invoking Whitebox.invokeMethod() with null one of the params null.None", "A_clean_title": ["null", "pointer", "when", "invok", "whitebox", "invokemethod", "invok", "method", "null", "one", "param", "null", "none"], "B_title": "Merge branch issue230", "B_clean_title": ["merg", "branch", "issue230"]},
{"A_title": "Problems with cookies disabled when using 301/302 and also 303 (even with cookies)As mentioned in the mailing list by Martin i open this as a bug...  Its not possible to use 303 as redirect (SC_SEE_OTHER) because thats not supported only 302 and 301 are supported but this is defined in RFC HTTP 1.1 from 1997.   301 will add the Location header - which works as expected when disabling cookies. But a 302 (which is what i prefer) will redirect to the same page because the Location header is missing. When i enable cookies its working.  Example can be found here: https://github.com/olze/WicketRedirect", "A_clean_title": ["problem", "cooki", "disabl", "when", "301", "302", "also", "303", "even", "cooki", "as", "mention", "mail", "list", "by", "martin", "open", "thi", "as", "bug", "it", "not", "possibl", "use", "303", "as", "redirect", "sc", "see", "other", "becaus", "that", "not", "support", "onli", "302", "301", "are", "support", "but", "thi", "defin", "rfc", "http", "1997", "301", "will", "add", "locat", "header", "which", "work", "as", "expect", "when", "disabl", "cooki", "but", "302", "which", "what", "prefer", "will", "redirect", "same", "page", "becaus", "locat", "header", "miss", "when", "enabl", "cooki", "it", "work", "exampl", "found", "here", "http", "github", "com", "olz", "wicketredirect", "wicket", "redirect"], "B_title": "Problems with cookies disabled when using 301/302 and also 303 (even with cookies)", "B_clean_title": ["problem", "cooki", "disabl", "when", "301", "302", "also", "303", "even", "cooki"]},
{"A_title": "Wrong parameter for first step size guess for Embedded Runge Kutta methodsIn a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator. Here DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...) The problem comes from the array scale that is used as a parameter in the call off initializeStep(..) Following the theory described by Hairer in his book Solving Ordinary Differential Equations 1 : Nonstiff Problems the scaling should be : sci = Atol i + |y0i| * Rtoli Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation sci = Atol i + |y0i| * Rtoli   when he performs the call to the same method initializeStep(..) In the method initializeStep the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user. But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...) To fix the bug one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator For exemple :  final double scale= new doubley0.length;;           if (vecAbsoluteTolerance == null)                for (int i = 0; i < scale.length; ++i)                   final double yi = Math.max(Math.abs(y0i) Math.abs(y0i));                 scalei = scalAbsoluteTolerance + scalRelativeTolerance * yi;                             else                for (int i = 0; i < scale.length; ++i)                   final double yi = Math.max(Math.abs(y0i) Math.abs(y0i));                 scalei = vecAbsoluteTolerancei + vecRelativeTolerancei * yi;                                       hNew = initializeStep(equations forward getOrder() scale                            stepStart y yDotK0 yTmp yDotK1); Sorry for the length of this message looking forward to hearing from you soon Vincent Morand", "A_clean_title": ["wrong", "paramet", "first", "step", "size", "guess", "embed", "rung", "kutta", "methodsin", "method", "space", "applic", "dop853", "detect", "what", "seem", "bad", "paramet", "call", "method", "initializestep", "initi", "step", "class", "adaptivestepsizeintegr", "adapt", "stepsiz", "integr", "here", "dormandprince853integr", "dormand", "prince853integr", "subclass", "embeddedrungekuttaintegr", "embed", "rung", "kutta", "integr", "which", "perform", "call", "initializestep", "initi", "step", "at", "begin", "it", "method", "integr", "problem", "come", "array", "scale", "that", "use", "as", "paramet", "call", "off", "initializestep", "initi", "step", "follow", "theori", "describ", "by", "hairer", "hi", "book", "solv", "ordinari", "differenti", "equat", "nonstiff", "problem", "scale", "sci", "atol", "|y0i|", "rtoli", "wherea", "embeddedrungekuttaintegr", "embed", "rung", "kutta", "integr", "use", "sci", "atoli", "note", "that", "gragg", "bulirsch", "stoer", "integr", "use", "good", "implement", "sci", "atol", "|y0i|", "rtoli", "when", "he", "perform", "call", "same", "method", "initializestep", "initi", "step", "method", "initializestep", "initi", "step", "error", "lead", "wrong", "step", "size", "use", "perform", "euler", "step", "most", "time", "it", "unvis", "user", "but", "my", "space", "applic", "euler", "step", "thi", "wrong", "step", "size", "much", "bigger", "than", "it", "make", "except", "occur", "my", "satellit", "hit", "ground", "fix", "bug", "one", "use", "same", "algorithm", "as", "rescal", "method", "graggbulirschstoerintegr", "gragg", "bulirsch", "stoer", "integr", "exempl", "final", "doubl", "scale=", "new", "doubley0", "length", "vecabsolutetoler", "vec", "absolut", "toler", "null", "int", "scale", "length", "++i", "final", "doubl", "yi", "math", "max", "math", "ab", "y0i", "math", "ab", "y0i", "scalei", "scalabsolutetoler", "scal", "absolut", "toler", "scalrelativetoler", "scal", "rel", "toler", "yi", "int", "scale", "length", "++i", "final", "doubl", "yi", "math", "max", "math", "ab", "y0i", "math", "ab", "y0i", "scalei", "vecabsolutetolerancei", "vec", "absolut", "tolerancei", "vecrelativetolerancei", "vec", "rel", "tolerancei", "yi", "hnew", "new", "initializestep", "initi", "step", "equat", "forward", "getord", "get", "order", "scale", "stepstart", "step", "start", "ydotk0", "dot", "k0", "ytmp", "tmp", "ydotk1", "dot", "k1", "sorri", "length", "thi", "messag", "look", "forward", "hear", "you", "soon", "vincent", "morand"], "B_title": "Fixed automatic step initialization in embedded Runge-Kutta integrators. The relative tolerance setting was never used only the absolute tolerance was used. JIRA: MATH-338", "B_clean_title": ["fix", "automat", "step", "initi", "embed", "rung", "kutta", "integr", "rel", "toler", "set", "wa", "never", "use", "onli", "absolut", "toler", "wa", "use", "jira", "math", "338"]},
{"A_title": "Large HTML File conversion to PDF hangs.Hi  I am trying to convert large HTML File approximately 600 pages which is not passing the conversion and hangs.  Following is my observation after debugging the core.  PdfRendererBuilder.class file has following method call.   renderer.layout(); // This action takes significant time but completes the process.  when I looked into it renderer.createPDF()  is trying to create entire PDF in memory (document) and after completion it starts writing to OutputStream.  Can we write it directly to OutputStream page by page? I think this might solve the problem.  Following is my code snippet please check the same if I am doing anything wrong here.   In  above code snippet it is not completing  builder.run(); process and hangs. Please help me with the solution.  Thanks in advance.", "A_clean_title": ["larg", "html", "file", "convers", "pdf", "hang", "hi", "am", "tri", "convert", "larg", "html", "file", "approxim", "600", "page", "which", "not", "pass", "convers", "hang", "follow", "my", "observ", "after", "debug", "core", "pdfrendererbuild", "class", "pdf", "render", "builder", "file", "ha", "follow", "method", "call", "render", "layout", "thi", "action", "take", "signific", "time", "but", "complet", "process", "when", "look", "into", "it", "render", "createpdf", "creat", "pdf", "tri", "creat", "entir", "pdf", "memori", "document", "after", "complet", "it", "start", "write", "outputstream", "output", "stream", "we", "write", "it", "directli", "outputstream", "output", "stream", "page", "by", "page", "think", "thi", "might", "solv", "problem", "follow", "my", "code", "snippet", "pleas", "check", "same", "am", "do", "anyth", "wrong", "here", "abov", "code", "snippet", "it", "not", "complet", "builder", "run", "process", "hang", "pleas", "help", "me", "solut", "thank", "advanc"], "B_title": "#180 - Tests and fixes around rtl overflow pages.", "B_clean_title": ["180", "test", "fix", "around", "rtl", "overflow", "page"]},
{"A_title": "Provide Simple Exception Name in Credentials Attribute for PW Expirycurrently upon encountering a pw history exception while changing the password of a user the credential attribute is set with the FQ class name instead of the simple name. this requires consumers (e.g. sling) to use oak package names instead of a simple class name to react to the situation.", "A_clean_title": ["provid", "simpl", "except", "name", "credenti", "attribut", "pw", "expirycurr", "upon", "encount", "pw", "histori", "except", "while", "chang", "password", "user", "credenti", "attribut", "set", "fq", "class", "name", "instead", "simpl", "name", "thi", "requir", "consum", "sling", "use", "oak", "packag", "name", "instead", "simpl", "class", "name", "react", "situat"], "B_title": ": Provide Simple Exception Name in Credentials Attribute for PW Expiry", "B_clean_title": ["provid", "simpl", "except", "name", "credenti", "attribut", "pw", "expiri"]},
{"A_title": "Allow convenient spying on abstract classes.Mockito is easy to use when the test needs to provide canned values for a certain method. But it gets harder when a canned value isnt sufficient.", "A_clean_title": ["allow", "conveni", "spi", "abstract", "class", "mockito", "easi", "use", "when", "test", "need", "provid", "can", "valu", "certain", "method", "but", "it", "get", "harder", "when", "can", "valu", "isnt", "suffici"], "B_title": "Adds support for issue #92 with ByteBuddy", "B_clean_title": ["add", "support", "issu", "92", "bytebuddi", "byte", "buddi"]},
{"A_title": "ACCEPT_CASE_INSENSITIVE_PROPERTIES fails with @JsonUnwrapped(note: moved from  FasterXML/jackson-dataformat-csv#133 ) When trying to deserialize type like:   with case-insensitive mapper (  mapper.enable(MapperFeature.ACCEPT_CASE_INSENSITIVE_PROPERTIES); ) I get exception:", "A_clean_title": ["accept", "case", "insensit", "properti", "fail", "jsonunwrap", "json", "unwrap", "note", "move", "dataformat", "csv", "fasterxml", "jackson", "faster", "xml", "133", "when", "tri", "deseri", "type", "like", "case", "insensit", "mapper", "mapper", "enabl", "mapperfeatur", "mapper", "featur", "accept", "case", "insensit", "properti", "get", "except"], "B_title": "Fix #1493", "B_clean_title": ["fix", "1493"]},
{"A_title": "SegmentWriter doesnt properly check the length of external blob IDsTo store the length field of an external binary ID the following encoding is used:  noformat 1110 + 4bit + 8bit noformat  which allows to store numbers between 0 and 2^12^ - 1.   The current implementation of SegmentWriter allows the length of binary IDs to range between 0 and 2^13^ - 1 writing incorrect data when the length of the binary ID ranges from 2^12^ to 2^13^ - 1.  When reading this incorrect data back an IllegalStateException is thrown complaining that the first byte of the length fields has an unexpected value record type. See OAK-1842 for an example.", "A_clean_title": ["segmentwrit", "segment", "writer", "doesnt", "properli", "check", "length", "extern", "blob", "idsto", "ds", "store", "length", "field", "extern", "binari", "id", "follow", "encod", "use", "noformat", "1110", "4bit", "8bit", "noformat", "which", "allow", "store", "number", "between", "2^12^", "current", "implement", "segmentwrit", "segment", "writer", "allow", "length", "binari", "id", "ds", "rang", "between", "2^13^", "write", "incorrect", "data", "when", "length", "binari", "id", "rang", "2^12^", "2^13^", "when", "read", "thi", "incorrect", "data", "back", "illegalstateexcept", "illeg", "state", "except", "thrown", "complain", "that", "first", "byte", "length", "field", "ha", "unexpect", "valu", "record", "type", "see", "oak", "1842", "exampl"], "B_title": "SegmentWriter doesnt properly check the length of external blob IDs Proper length check and unit test. Credits to Francesco Mari for the patch", "B_clean_title": ["segmentwrit", "segment", "writer", "doesnt", "properli", "check", "length", "extern", "blob", "id", "ds", "proper", "length", "check", "unit", "test", "credit", "francesco", "mari", "patch"]},
{"A_title": "Set state checkpointer before default state for PartitionedStreamOperatorStateCurrently the default state is set before the passed StateCheckpointer instance for operator states.  What currently happens because of this is that the default value is serialized with Java serialization and then deserialized on the opstate.value() call using the StateCheckpointer most likely causing a failure.  This can be trivially fixed by swaping the order of the 2 calls.", "A_clean_title": ["set", "state", "checkpoint", "befor", "default", "state", "partitionedstreamoperatorstatecurr", "partit", "stream", "oper", "state", "current", "default", "state", "set", "befor", "pass", "statecheckpoint", "state", "checkpoint", "instanc", "oper", "state", "what", "current", "happen", "becaus", "thi", "that", "default", "valu", "serial", "java", "serial", "then", "deseri", "opstat", "valu", "call", "statecheckpoint", "state", "checkpoint", "most", "like", "caus", "failur", "thi", "trivial", "fix", "by", "swape", "order", "call"], "B_title": "streaming Set StateCheckpointer before default state value", "B_clean_title": ["stream", "set", "statecheckpoint", "state", "checkpoint", "befor", "default", "state", "valu"]},
{"A_title": "ProcessCommonJSModules module exports failures when checkTypes enabledNone", "A_clean_title": ["processcommonjsmodul", "process", "common", "js", "modul", "modul", "export", "failur", "when", "checktyp", "check", "type", "enablednon", "enabl", "none"], "B_title": "dont bother with module exports if theres nothing to export Fixes issue 732", "B_clean_title": ["dont", "bother", "modul", "export", "there", "noth", "export", "fix", "issu", "732"]},
{"A_title": "Slow event listeners do not scale as expectedorg.apache.jackrabbit.oak.jcr.LargeOperationIT#slowListener does not scale to O n log n on the document node store.", "A_clean_title": ["slow", "event", "listen", "not", "scale", "as", "expectedorg", "apach", "jackrabbit", "oak", "jcr", "largeoperationit", "larg", "oper", "it", "slowlisten", "slow", "listen", "not", "scale", "log", "document", "node", "store"], "B_title": "Slow event listeners do not scale as expected", "B_clean_title": ["slow", "event", "listen", "not", "scale", "as", "expect"]},
{"A_title": "Try/catch blocks incorporate code not inside original blocksNone", "A_clean_title": ["tri", "catch", "block", "incorpor", "code", "not", "insid", "origin", "blocksnon", "block", "none"], "B_title": "Fix may-use data flow analysis in the presence of ON_EX edges. Fixes issue 794", "B_clean_title": ["fix", "may", "use", "data", "flow", "analysi", "presenc", "ex", "edg", "fix", "issu", "794"]},
{"A_title": "Page not recognized as stateless although stateful component is hidden in #onConfigure()Page#stateless gets cached. If Page#isStateless() is called before rendering a page might not be considered stateless although in #onConfigure() all stateful components are hidden.", "A_clean_title": ["page", "not", "recogn", "as", "stateless", "although", "state", "compon", "hidden", "onconfigur", "configur", "page", "stateless", "get", "cach", "page", "isstateless", "stateless", "call", "befor", "render", "page", "might", "not", "consid", "stateless", "although", "onconfigur", "configur", "all", "state", "compon", "are", "hidden"], "B_title": "reset stateless flag on render since stateful components might be added *and* removed", "B_clean_title": ["reset", "stateless", "flag", "render", "sinc", "state", "compon", "might", "ad", "remov"]},
{"A_title": "WordUtils.abbreviate bug when lower is greater than str.lengthIn WordUtils.abbreviate upper is adjusted to the length of the string then to lower. But lower is never adjusted to the length of the string so if lower is greater than str.lengt() upper will be too... Then str.substring(0 upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string", "A_clean_title": ["wordutil", "abbrevi", "word", "util", "bug", "when", "lower", "greater", "than", "str", "lengthin", "length", "wordutil", "abbrevi", "word", "util", "upper", "adjust", "length", "string", "then", "lower", "but", "lower", "never", "adjust", "length", "string", "so", "lower", "greater", "than", "str", "lengt", "upper", "will", "too", "then", "str", "substr", "upper", "throw", "stringindexoutofboundsexcept", "string", "index", "out", "bound", "except", "fix", "adjust", "lower", "length", "string"], "B_title": "Applying Vincent Behars second patch for LANG-419 - fixing a bug in abbreviate such that lower limits greater than the length of the string werent working correctly", "B_clean_title": ["appli", "vincent", "behar", "second", "patch", "lang", "419", "fix", "bug", "abbrevi", "such", "that", "lower", "limit", "greater", "than", "length", "string", "werent", "work", "correctli"]},
{"A_title": "Incorrect recovery of _lastRev for branch commitThe recovery process for _lastRevs is incorrect for branch commits. It propagates the revision of the commit to the branch up to the root node instead of the revision of the merge for the changes.", "A_clean_title": ["incorrect", "recoveri", "lastrev", "last", "rev", "branch", "committh", "commit", "recoveri", "process", "lastrev", "last", "rev", "incorrect", "branch", "commit", "it", "propag", "revis", "commit", "branch", "up", "root", "node", "instead", "revis", "merg", "chang"], "B_title": "Incorrect recovery of _lastRev for branch commit", "B_clean_title": ["incorrect", "recoveri", "lastrev", "last", "rev", "branch", "commit"]},
{"A_title": "Components onAfterRender() is called so many times as it is depth in the component tree + 1org.apache.wicket.Component.afterRender() calls org.apache.wicket.Component.onAfterRenderChildren() which for MarkupContainers calls afterRender() for its children.  So for code like:   WebMarkupContainer comp1 = new WebMarkupContainer(c1);         add(comp1);                  WebMarkupContainer comp2 = new WebMarkupContainer(c2);         comp1.add(comp2);                  WebMarkupContainer comp3 = new WebMarkupContainer(c3)               @Override             protected void onAfterRender()                  super.onAfterRender();                 System.err.println(called);                                   ;         comp2.add(comp3);  youll see called printed 4 times in a single request.  Additionally I think onAfterRenderChildren() should be called before onAfterRender() in Component.afterRender(). The flow should be first-in last-out: onBeforeRender > onBeforeRenderChildren > onAfterRenderChildren > onAfterRender", "A_clean_title": ["compon", "onafterrend", "after", "render", "call", "so", "mani", "time", "as", "it", "depth", "compon", "tree", "1org", "apach", "wicket", "compon", "afterrend", "after", "render", "call", "org", "apach", "wicket", "compon", "onafterrenderchildren", "after", "render", "children", "which", "markupcontain", "markup", "contain", "call", "afterrend", "after", "render", "it", "children", "so", "code", "like", "webmarkupcontain", "web", "markup", "contain", "comp1", "new", "webmarkupcontain", "web", "markup", "contain", "c1", "add", "comp1", "webmarkupcontain", "web", "markup", "contain", "comp2", "new", "webmarkupcontain", "web", "markup", "contain", "c2", "comp1", "add", "comp2", "webmarkupcontain", "web", "markup", "contain", "comp3", "new", "webmarkupcontain", "web", "markup", "contain", "c3", "overrid", "protect", "void", "onafterrend", "after", "render", "super", "onafterrend", "after", "render", "system", "err", "println", "call", "comp2", "add", "comp3", "youll", "see", "call", "print", "time", "singl", "request", "addit", "think", "onafterrenderchildren", "after", "render", "children", "call", "befor", "onafterrend", "after", "render", "compon", "afterrend", "after", "render", "flow", "first", "last", "out", "onbeforerend", "befor", "render", "onbeforerenderchildren", "befor", "render", "children", "onafterrenderchildren", "after", "render", "children", "onafterrend", "after", "render"], "B_title": "Components onAfterRender() is called so many times as it is depth in the component tree + 1", "B_clean_title": ["compon", "onafterrend", "after", "render", "call", "so", "mani", "time", "as", "it", "depth", "compon", "tree"]},
{"A_title": "BarrierBuffer does not properly clean up temp filesNone", "A_clean_title": ["barrierbuff", "barrier", "buffer", "not", "properli", "clean", "up", "temp", "filesnon", "file", "none"], "B_title": "streaming BarrierBuffer releases temp files properly.", "B_clean_title": ["stream", "barrierbuff", "barrier", "buffer", "releas", "temp", "file", "properli"]},
{"A_title": "Query constraints marked as invalid in the case of an mvpIt seems that in the case of a query that has more constraints on the same property like bq. //*(@prop = aaa and @prop = bbb and @prop = ccc)  the filter is marked as invalid (_#isAlwaysFalse_) and the query returns no results.  This is incorrect and affects queries that search for multi-valued properties on nodes.  This comes from/affects OAK-1075.", "A_clean_title": ["queri", "constraint", "mark", "as", "invalid", "case", "mvpit", "mvp", "it", "seem", "that", "case", "queri", "that", "ha", "more", "constraint", "same", "properti", "like", "bq", "prop", "aaa", "prop", "bbb", "prop", "ccc", "filter", "mark", "as", "invalid", "isalwaysfals", "alway", "fals", "queri", "return", "no", "result", "thi", "incorrect", "affect", "queri", "that", "search", "multi", "valu", "properti", "node", "thi", "come", "affect", "oak", "1075"], "B_title": "Query constraints marked as invalid in the case of an mvp", "B_clean_title": ["queri", "constraint", "mark", "as", "invalid", "case", "mvp"]},
{"A_title": "Template types on methods incorrectly trigger inference of a template on the class if that template type is unknownNone", "A_clean_title": ["templat", "type", "method", "incorrectli", "trigger", "infer", "templat", "class", "that", "templat", "type", "unknownnon", "unknown", "none"], "B_title": "Dont infer class template keys when inferring method template keys. Fixes issue 1058. ------------- Created by MOE: http://code.google.com/p/moe-java MOE_MIGRATED_REVID=53874505", "B_clean_title": ["dont", "infer", "class", "templat", "key", "when", "infer", "method", "templat", "key", "fix", "issu", "1058", "creat", "by", "moe", "http", "java", "googl", "code", "com", "moe", "moe", "migrat", "revid=53874505"]},
{"A_title": "Missing privileges after repository upgradeAfter upgrading from Jackrabbit classic all Oak specific privileges are missing (rep:userManagement rep:readNodes rep:readProperties rep:addProperties rep:alterProperties rep:removeProperties rep:indexDefinitionManagement).  The reason seems to be that the PrivilegeInitializer is not run during upgrade.", "A_clean_title": ["miss", "privileg", "after", "repositori", "upgradeaft", "upgrad", "after", "upgrad", "jackrabbit", "classic", "all", "oak", "specif", "privileg", "are", "miss", "rep", "usermanag", "user", "manag", "rep", "readnod", "read", "node", "rep", "readproperti", "read", "properti", "rep", "addproperti", "add", "properti", "rep", "alterproperti", "alter", "properti", "rep", "removeproperti", "remov", "properti", "rep", "indexdefinitionmanag", "index", "definit", "manag", "reason", "seem", "that", "privilegeiniti", "privileg", "initi", "not", "run", "dure", "upgrad"], "B_title": "Missing privileges after repository upgrade Dont copy built in privileges", "B_clean_title": ["miss", "privileg", "after", "repositori", "upgrad", "dont", "copi", "built", "privileg"]},
{"A_title": "importdirectory failing on split tablebulk import for the wikisearch example isnt working properly: files are not being assigned to partitions if there are splits.", "A_clean_title": ["importdirectori", "fail", "split", "tablebulk", "import", "wikisearch", "exampl", "isnt", "work", "properli", "file", "are", "not", "be", "assign", "partit", "there", "are", "split"], "B_title": "merge to trunk", "B_clean_title": ["merg", "trunk"]},
{"A_title": "Token class option always requires token propertyIn testing out ACCUMULO-2815 I attempted to manually provide a KerberosToken to authenticate myself and then launch the shell but ran into an issue. The KerberosToken (in its current state) needs no options: its wholly functional on its own.  accumulo shell -tc org.apache.accumulo.core.client.security.tokens.KerberosToken  gives an error  noformat 2014-12-16 11:41:09712 shell.Shell ERROR: com.beust.jcommander.ParameterException: Must supply either both or neither of --tokenClass and --tokenProperty noformat  And providing an empty option just prints the help message accumulo shell -tc org.apache.accumulo.core.client.security.tokens.KerberosToken -l   Im guessing the latter is just how the JCommander DynamicParameter is implemented but I dont see a reason why every authentication *must* have some properties provided to it.", "A_clean_title": ["token", "class", "option", "alway", "requir", "token", "propertyin", "properti", "test", "out", "accumulo", "2815", "attempt", "manual", "provid", "kerberostoken", "kerbero", "token", "authent", "myself", "then", "launch", "shell", "but", "ran", "into", "issu", "kerberostoken", "kerbero", "token", "it", "current", "state", "need", "no", "option", "it", "wholli", "function", "it", "own", "accumulo", "shell", "tc", "org", "apach", "accumulo", "core", "client", "secur", "token", "kerberostoken", "kerbero", "token", "give", "error", "noformat", "2014", "12", "16", "11:41:09712", "shell", "shell", "error", "com", "beust", "jcommand", "parameterexcept", "paramet", "except", "must", "suppli", "either", "both", "or", "neither", "tokenclass", "token", "class", "tokenproperti", "token", "properti", "noformat", "provid", "empti", "option", "just", "print", "help", "messag", "accumulo", "shell", "tc", "org", "apach", "accumulo", "core", "client", "secur", "token", "kerberostoken", "kerbero", "token", "im", "guess", "latter", "just", "how", "jcommand", "command", "dynamicparamet", "dynam", "paramet", "implement", "but", "dont", "see", "reason", "whi", "everi", "authent", "must", "have", "some", "properti", "provid", "it"], "B_title": "Remove requirement for properties on provided token.", "B_clean_title": ["remov", "requir", "properti", "provid", "token"]},
{"A_title": "Converts string properties into numbers in literal object definitionsNone", "A_clean_title": ["convert", "string", "properti", "into", "number", "liter", "object", "definitionsnon", "definit", "none"], "B_title": "numbers are hard! fixes issue 569", "B_clean_title": ["number", "are", "hard", "fix", "issu", "569"]},
{"A_title": "ArrayIndexOutOfBoundsException in MathArrays.linearCombinationWhen MathArrays.linearCombination is passed arguments with length 1 it throws an ArrayOutOfBoundsException. This is caused by this line: double prodHighNext = prodHigh1; linearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.", "A_clean_title": ["arrayindexoutofboundsexcept", "array", "index", "out", "bound", "except", "matharray", "linearcombinationwhen", "math", "array", "linear", "combin", "when", "matharray", "linearcombin", "math", "array", "linear", "combin", "pass", "argument", "length", "it", "throw", "arrayoutofboundsexcept", "array", "out", "bound", "except", "thi", "caus", "by", "thi", "line", "doubl", "prodhighnext", "prod", "high", "next", "prodhigh1", "prod", "high1", "linearcombin", "linear", "combin", "check", "length", "argument", "fall", "back", "simpl", "multipl", "length"], "B_title": "Array of length 1 must be handled as a special case.", "B_clean_title": ["array", "length", "must", "handl", "as", "special", "case"]},
{"A_title": "BigFraction.doubleValue() returns Double.NaN for large numerators or denominatorsThe current implementation of doubleValue() divides numerator.doubleValue() / denominator.doubleValue().  BigInteger.doubleValue() fails for any number greater than Double.MAX_VALUE.  So if the user has 308-digit numerator or denominator the resulting quotient fails even in cases where the result would be well inside Doubles range.  I have a patch to fix it if I can figure out how to attach it here I will.", "A_clean_title": ["bigfract", "doublevalu", "big", "fraction", "doubl", "valu", "return", "doubl", "nan", "na", "larg", "numer", "or", "denominatorsth", "denomin", "current", "implement", "doublevalu", "doubl", "valu", "divid", "numer", "doublevalu", "doubl", "valu", "denomin", "doublevalu", "doubl", "valu", "biginteg", "doublevalu", "big", "integ", "doubl", "valu", "fail", "ani", "number", "greater", "than", "doubl", "max", "valu", "so", "user", "ha", "308", "digit", "numer", "or", "denomin", "result", "quotient", "fail", "even", "case", "where", "result", "would", "well", "insid", "doubl", "rang", "have", "patch", "fix", "it", "figur", "out", "how", "attach", "it", "here", "will"], "B_title": "Fixed doubleValue() and floatValue() when numerator and denominator are larger than the range of the corresponding primitive type.", "B_clean_title": ["fix", "doublevalu", "doubl", "valu", "floatvalu", "float", "valu", "when", "numer", "denomin", "are", "larger", "than", "rang", "correspond", "primit", "type"]},
{"A_title": "Add support for handling primitive/discrepancy problem with type refinements(note: derived from  FasterXML/jackson-module-jaxb-annotations#64 ) The problem is that although  int and java.lang.Integer are related logically they are not related by inheritance (or implementation). Since some legacy code may try refinements in this axis itd be nice to handle this somehow. Two basic approaches would be:   Just ignore primitive/wrapper override return original type as is  Allow wrapper to refine primitive return wrapper.  There is also related question of whether to allow int to long and similar refinements but start with basics.", "A_clean_title": ["add", "support", "handl", "primit", "discrep", "problem", "type", "refin", "note", "deriv", "modul", "jaxb", "annot", "fasterxml", "jackson", "faster", "xml", "64", "problem", "that", "although", "int", "java", "lang", "integ", "are", "relat", "logic", "they", "are", "not", "relat", "by", "inherit", "or", "implement", "sinc", "some", "legaci", "code", "may", "tri", "refin", "thi", "axi", "itd", "nice", "handl", "thi", "somehow", "two", "basic", "approach", "would", "just", "ignor", "primit", "wrapper", "overrid", "return", "origin", "type", "as", "allow", "wrapper", "refin", "primit", "return", "wrapper", "there", "also", "relat", "question", "whether", "allow", "int", "long", "similar", "refin", "but", "start", "basic"], "B_title": "Fix #1592", "B_clean_title": ["fix", "1592"]},
{"A_title": "Unable to find markup for children of deeply nested IComponentResolvers during Ajax responseComponent hierarchy: Page -> WebMarkupContainer -> IComponentResolver (that uses Page to resolve) and Page -> Panel.  Markup hierarchy: Page -> WebMarkupContainer -> IComponentResolver -> Panel.  When rendering whole page it works because it is markup driven. Wicket encounters ComponentTag for Panel and resolves the Panel using IComponentResolver which retrieves the Panel from the Page.  When you add the Panel to an AjaxRequestTarget the render is component driven. In order to render the Panel we must retrieve the markup for the Panel from its parent MarkupContainer which happens to be the Page.  Markup.java around line 230 skips to closing tags of ComponentTag so when Page gets to the opening tag of the WebMarkupContainer it skips to the closing tag of the WebMarkupContainer and so passes over the ComponentTag for Panel without noticing it. There is actually another check in DefaultMarkupSourcingStrategy that tries to fetch from all the transparent components in the markup container but this is not good enough because in our example the IComponentResolver is not actually a direct child of the Panels parent to it is never used to try find the markup.  One solution might be to traverse the tree and attempt to find the markup from all IComponentResolving MarkupContainers but we should be careful. Im a bit concerned at how various parts of Wicket just assume that an IComponentResolver is transparent and resolves from its direct parent only.  If we do go down the route of traversing the tree to find IComponentResolvers then try find the markup from each of them we really should add a check in AbstractMarkupSourcingStrategy#searchMarkupInTransparentResolvers() to check that the Component that the IComponentResolver resolves for the markup id is the same component for which we are looking for markup.  This is a difficult one. I am working around it for the mean time just recording the difficulty here. Will try make a patch when I can.", "A_clean_title": ["unabl", "find", "markup", "children", "deepli", "nest", "icomponentresolv", "compon", "resolv", "dure", "ajax", "responsecompon", "respons", "compon", "hierarchi", "page", "webmarkupcontain", "web", "markup", "contain", "icomponentresolv", "compon", "resolv", "that", "use", "page", "resolv", "page", "panel", "markup", "hierarchi", "page", "webmarkupcontain", "web", "markup", "contain", "icomponentresolv", "compon", "resolv", "panel", "when", "render", "whole", "page", "it", "work", "becaus", "it", "markup", "driven", "wicket", "encount", "componenttag", "compon", "tag", "panel", "resolv", "panel", "icomponentresolv", "compon", "resolv", "which", "retriev", "panel", "page", "when", "you", "add", "panel", "ajaxrequesttarget", "ajax", "request", "target", "render", "compon", "driven", "order", "render", "panel", "we", "must", "retriev", "markup", "panel", "it", "parent", "markupcontain", "markup", "contain", "which", "happen", "page", "markup", "java", "around", "line", "230", "skip", "close", "tag", "componenttag", "compon", "tag", "so", "when", "page", "get", "open", "tag", "webmarkupcontain", "web", "markup", "contain", "it", "skip", "close", "tag", "webmarkupcontain", "web", "markup", "contain", "so", "pass", "over", "componenttag", "compon", "tag", "panel", "without", "notic", "it", "there", "actual", "anoth", "check", "defaultmarkupsourcingstrategi", "default", "markup", "sourc", "strategi", "that", "tri", "fetch", "all", "transpar", "compon", "markup", "contain", "but", "thi", "not", "good", "enough", "becaus", "our", "exampl", "icomponentresolv", "compon", "resolv", "not", "actual", "direct", "child", "panel", "parent", "it", "never", "use", "tri", "find", "markup", "one", "solut", "might", "travers", "tree", "attempt", "find", "markup", "all", "icomponentresolv", "compon", "resolv", "markupcontain", "markup", "contain", "but", "we", "care", "im", "bit", "concern", "at", "how", "variou", "part", "wicket", "just", "assum", "that", "icomponentresolv", "compon", "resolv", "transpar", "resolv", "it", "direct", "parent", "onli", "we", "go", "down", "rout", "travers", "tree", "find", "icomponentresolv", "compon", "resolv", "then", "tri", "find", "markup", "each", "them", "we", "realli", "add", "check", "abstractmarkupsourcingstrategi", "abstract", "markup", "sourc", "strategi", "searchmarkupintransparentresolv", "search", "markup", "transpar", "resolv", "check", "that", "compon", "that", "icomponentresolv", "compon", "resolv", "resolv", "markup", "id", "same", "compon", "which", "we", "are", "look", "markup", "thi", "difficult", "one", "am", "work", "around", "it", "mean", "time", "just", "record", "difficulti", "here", "will", "tri", "make", "patch", "when"], "B_title": "Unable to find markup for children of deeply nested IComponentResolvers during Ajax response", "B_clean_title": ["unabl", "find", "markup", "children", "deepli", "nest", "icomponentresolv", "compon", "resolv", "dure", "ajax", "respons"]},
{"A_title": "Miscellaneous issues concerning the optimization packageRevision 990792 contains changes triggered the following issues:  MATH-394 MATH-397 MATH-404  This issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance):  BrentOptimizer: a specific convergence checker must be used. LevenbergMarquardtOptimizer also has specific convergence checks. Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical):  See BrentOptimizer and LevenbergMarquardtOptimizer the algorithm passes points to the convergence checker but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker). In PowellOptimizer the line search (BrentOptimizer) tolerances depend on the tolerances within the main algorithm. Since tolerances come with ConvergenceChecker and so can be changed at any time it is awkward to adapt the values within the line search optimizer without exposing its internals (BrentOptimizer field) to the enclosing class (PowellOptimizer).   Given the numerous changes some Javadoc comments might be out-of-sync although I did try to update them all. Class DirectSearchOptimizer (in package optimization.direct) inherits from class AbstractScalarOptimizer (in package optimization.general). Some interfaces are defined in package optimization but their base implementations (abstract class that contain the boiler-plate code) are in package optimization.general (e.g. DifferentiableMultivariateVectorialOptimizer and BaseAbstractVectorialOptimizer). No check is performed to ensure the the convergence checker has been set (see e.g. BrentOptimizer and PowellOptimizer); if it hasnt there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker. NonLinearConjugateGradientOptimizer: Ugly workaround for the checked ConvergenceException. Everywhere we trail the checked FunctionEvaluationException although it is never used. There remains some duplicate code (such as the multi-start loop in the various MultiStart... implementations). The ConvergenceChecker interface is very general (the converged method can take any number of ...PointValuePair). However there remains a semantic problem: One cannot be sure that the list of points means the same thing for the caller of converged and within the implementation of the ConvergenceChecker that was independently set. It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In LevenbergMarquartdOptimizer for example it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations. In AbstractLeastSquaresOptimizer and LevenbergMarquardtOptimizer occurences of OptimizationException were replaced by the unchecked ConvergenceException but in some cases it might not be the most appropriate one. MultiStartUnivariateRealOptimizer: in the other classes (MultiStartMultivariate...) similar to this one the randomization is on the firts-guess value while in this class it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval). The Javadoc utility raises warnings (see output of mvn site) which I couldnt figure out how to correct. Some previously existing classes and interfaces have become no more than a specialisation of new generics classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.", "A_clean_title": ["miscellan", "issu", "concern", "optim", "packagerevis", "packag", "revis", "990792", "contain", "chang", "trigger", "follow", "issu", "math", "394", "math", "397", "math", "404", "thi", "issu", "collect", "current", "still", "unsatisfactori", "code", "not", "necessarili", "sort", "order", "annoy", "brentoptim", "brent", "optim", "specif", "converg", "checker", "must", "use", "levenbergmarquardtoptim", "levenberg", "marquardt", "optim", "also", "ha", "specif", "converg", "check", "tri", "make", "converg", "check", "independ", "optim", "algorithm", "creat", "problem", "conceptu", "practic", "see", "brentoptim", "brent", "optim", "levenbergmarquardtoptim", "levenberg", "marquardt", "optim", "algorithm", "pass", "point", "converg", "checker", "but", "actual", "mean", "point", "veri", "well", "differ", "caller", "optim", "algorithm", "calle", "converg", "checker", "powelloptim", "powel", "optim", "line", "search", "brentoptim", "brent", "optim", "toler", "depend", "toler", "within", "main", "algorithm", "sinc", "toler", "come", "convergencecheck", "converg", "checker", "so", "chang", "at", "ani", "time", "it", "awkward", "adapt", "valu", "within", "line", "search", "optim", "without", "expos", "it", "intern", "brentoptim", "brent", "optim", "field", "enclos", "class", "powelloptim", "powel", "optim", "given", "numer", "chang", "some", "javadoc", "comment", "might", "out", "sync", "although", "did", "tri", "updat", "them", "all", "class", "directsearchoptim", "direct", "search", "optim", "packag", "optim", "direct", "inherit", "class", "abstractscalaroptim", "abstract", "scalar", "optim", "packag", "optim", "gener", "some", "interfac", "are", "defin", "packag", "optim", "but", "their", "base", "implement", "abstract", "class", "that", "contain", "boiler", "plate", "code", "are", "packag", "optim", "gener", "differentiablemultivariatevectorialoptim", "differenti", "multivari", "vectori", "optim", "baseabstractvectorialoptim", "base", "abstract", "vectori", "optim", "no", "check", "perform", "ensur", "converg", "checker", "ha", "been", "set", "see", "brentoptim", "brent", "optim", "powelloptim", "powel", "optim", "it", "hasnt", "there", "will", "npe", "altern", "initi", "default", "checker", "that", "will", "never", "use", "case", "user", "had", "intend", "explicitli", "set", "checker", "nonlinearconjugategradientoptim", "non", "linear", "conjug", "gradient", "optim", "ugli", "workaround", "check", "convergenceexcept", "converg", "except", "everywher", "we", "trail", "check", "functionevaluationexcept", "function", "evalu", "except", "although", "it", "never", "use", "there", "remain", "some", "duplic", "code", "such", "as", "multi", "start", "loop", "variou", "multistart", "multi", "start", "implement", "convergencecheck", "converg", "checker", "interfac", "veri", "gener", "converg", "method", "take", "ani", "number", "pointvaluepair", "point", "valu", "pair", "howev", "there", "remain", "semant", "problem", "one", "not", "sure", "that", "list", "point", "mean", "same", "thing", "caller", "converg", "within", "implement", "convergencecheck", "converg", "checker", "that", "wa", "independ", "set", "it", "not", "clear", "whether", "it", "wise", "aggreg", "counter", "gradient", "evalu", "function", "evalu", "counter", "levenbergmarquartdoptim", "levenberg", "marquartd", "optim", "exampl", "it", "would", "unfair", "so", "current", "had", "remov", "all", "test", "refer", "gradient", "jacobian", "evalu", "abstractleastsquaresoptim", "abstract", "least", "squar", "optim", "levenbergmarquardtoptim", "levenberg", "marquardt", "optim", "occur", "optimizationexcept", "optim", "except", "were", "replac", "by", "uncheck", "convergenceexcept", "converg", "except", "but", "some", "case", "it", "might", "not", "most", "appropri", "one", "multistartunivariaterealoptim", "multi", "start", "univari", "real", "optim", "other", "class", "multistartmultivari", "multi", "start", "multivari", "similar", "thi", "one", "random", "firt", "guess", "valu", "while", "thi", "class", "it", "search", "interv", "think", "that", "here", "also", "we", "randomli", "choos", "start", "valu", "within", "user", "select", "interv", "javadoc", "util", "rais", "warn", "see", "output", "mvn", "site", "which", "couldnt", "figur", "out", "how", "correct", "some", "previous", "exist", "class", "interfac", "have", "becom", "no", "more", "than", "specialis", "new", "gener", "class", "it", "might", "interest", "remov", "them", "order", "reduc", "number", "class", "thu", "limit", "potenti", "confus"], "B_title": "(point 13) Selecting a random start value (instead of interval bounds).", "B_clean_title": ["point", "13", "select", "random", "start", "valu", "instead", "interv", "bound"]},
{"A_title": "Unresolved conflicts in TokenProviderImpl#createToken()In certain situations (e.g. heavy load) TokenProviderImpl#createToken() might create some unresolved conflicts.  e.g.   code org.apache.jackrabbit.oak.api.CommitFailedException: OakState0001: Unresolved conflicts in /home/users/..../..../.tokens/2014-04-07T11.55.58.167+02.00 code  and  code 01.04.2014 17:52:41.216 *WARN* qtp218544742-286 org.apache.jackrabbit.oak.security.authentication.token.TokenProviderImpl Failed to create login token. 01.04.2014 17:52:41.218 *WARN* qtp218544742-300 org.eclipse.jetty.servlet.ServletHandler /projects.html java.lang.IllegalArgumentException: Invalid token      at org.apache.jackrabbit.api.security.authentication.token.TokenCredentials.<init>(TokenCredentials.java:42) code", "A_clean_title": ["unresolv", "conflict", "tokenproviderimpl", "token", "provid", "impl", "createtoken", "creat", "token", "certain", "situat", "heavi", "load", "tokenproviderimpl", "token", "provid", "impl", "createtoken", "creat", "token", "might", "creat", "some", "unresolv", "conflict", "code", "org", "apach", "jackrabbit", "oak", "api", "commitfailedexcept", "commit", "fail", "except", "oakstate0001", "oak", "state0001", "unresolv", "conflict", "home", "user", "04", "07t11", "55", "58", "167+02", "00", "token", "2014", "code", "code", "01", "04", "2014", "17:52:41", "216", "warn", "qtp218544742", "286", "org", "apach", "jackrabbit", "oak", "secur", "authent", "token", "tokenproviderimpl", "token", "provid", "impl", "fail", "creat", "login", "token", "01", "04", "2014", "17:52:41", "218", "warn", "qtp218544742", "300", "org", "eclips", "jetti", "servlet", "servlethandl", "servlet", "handler", "html", "project", "java", "lang", "illegalargumentexcept", "illeg", "argument", "except", "invalid", "token", "at", "org", "apach", "jackrabbit", "api", "secur", "authent", "token", "tokencredenti", "token", "credenti", "init", "tokencredenti", "java:42", "token", "credenti", "code"], "B_title": ": Unresolved conflicts in TokenProviderImpl#createToken()", "B_clean_title": ["unresolv", "conflict", "tokenproviderimpl", "token", "provid", "impl", "createtoken", "creat", "token"]},
{"A_title": "ArrayKeySelector returns wrong positions (or fails)The ArrayKeySelector is broken and returns wrong values in all cases except for 0 as a single only key position.", "A_clean_title": ["arraykeyselector", "array", "key", "selector", "return", "wrong", "posit", "or", "fail", "arraykeyselector", "array", "key", "selector", "broken", "return", "wrong", "valu", "all", "case", "except", "as", "singl", "onli", "key", "posit"], "B_title": "streaming Fix ArrayKeySelector", "B_clean_title": ["stream", "fix", "arraykeyselector", "array", "key", "selector"]},
{"A_title": "Missing type-checks for var_args notationNone", "A_clean_title": ["miss", "type", "check", "var", "arg", "notationnon", "notat", "none"], "B_title": "check var_args properly Fixes issue 229.", "B_clean_title": ["check", "var", "arg", "properli", "fix", "issu", "229"]},
{"A_title": "Negative value with restrictNonNegativeProblem: commons-math-2.2 SimplexSolver. A variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call: SimplexSolver.optimize(function constraints GoalType.MINIMIZE true); Function 1 * x + 1 * y + 0 Constraints: 1 * x + 0 * y = 1 Result: x = 1; y = -1; Probably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.", "A_clean_title": ["neg", "valu", "restrictnonnegativeproblem", "restrict", "non", "neg", "problem", "common", "math", "simplexsolv", "simplex", "solver", "variabl", "coeffici", "may", "assign", "neg", "valu", "nevertheless", "restricttononneg", "restrict", "nonneg", "flag", "call", "simplexsolv", "optim", "simplex", "solver", "function", "constraint", "goaltyp", "minim", "goal", "type", "true", "function", "constraint", "result", "probabl", "variabl", "coeffici", "are", "omit", "at", "some", "point", "comput", "becaus", "that", "restrict", "not", "affect", "their", "valu"], "B_title": "Fixed case of unconstrained variables that still occur in the objective function in simplex solver.", "B_clean_title": ["fix", "case", "unconstrain", "variabl", "that", "still", "occur", "object", "function", "simplex", "solver"]},
{"A_title": "Polygon difference produces erronious results in some casesThe 2D polygon difference method is returning incorrect results.  Below is a test case of subtracting two polygons (Sorry this is the simplest case that I could find that duplicates the problem).    There are three problems with the result. The first is that the first point of the first set of vertices is null (and the first point of the second set is also null).  The second is that even if the first null points are ignored  the returned polygon is not the correct result. The first and last points are way off and the remaining points do not match the original polygon boundaries.  Additionally there are two holes that are returned in the results.  This subtraction case should not have holes.  code:title=Complex Polygon Difference Test public void testComplexDifference()          Vector2D vertices1 = new Vector2D              new Vector2D                      new Vector2D( 90.08714908223715  38.370299337260235)                     new Vector2D( 90.08709517675004  38.3702895991413)                     new Vector2D( 90.08401538704919  38.368849330127944)                     new Vector2D( 90.08258210430711  38.367634558585564)                     new Vector2D( 90.08251455106665  38.36763409247078)                     new Vector2D( 90.08106599752608  38.36761621664249)                     new Vector2D( 90.08249585300035  38.36753627557965)                     new Vector2D( 90.09075743352184  38.35914647644972)                     new Vector2D( 90.09099945896571  38.35896264724079)                     new Vector2D( 90.09269383800086  38.34595756121246)                     new Vector2D( 90.09638631543191  38.3457988093121)                     new Vector2D( 90.09666417351019  38.34523360999418)                     new Vector2D( 90.1297082145872  38.337670454923625)                     new Vector2D( 90.12971687748956  38.337669827794684)                     new Vector2D( 90.1240820219179  38.34328502001131)                     new Vector2D( 90.13084259656404  38.34017811765017)                     new Vector2D( 90.13378567942857  38.33860579180606)                     new Vector2D( 90.13519557833206  38.33621054663689)                     new Vector2D( 90.13545616732307  38.33614965452864)                     new Vector2D( 90.13553111202748  38.33613962818305)                     new Vector2D( 90.1356903436448  38.33610227127048)                     new Vector2D( 90.13576283227428  38.33609255422783)                     new Vector2D( 90.13595870833188  38.33604606376991)                     new Vector2D( 90.1361556630693  38.3360024198866)                     new Vector2D( 90.13622408795709  38.335987048115726)                     new Vector2D( 90.13696189099994  38.33581914328681)                     new Vector2D( 90.13746655304897  38.33616706665265)                     new Vector2D( 90.13845973716064  38.33650776167099)                     new Vector2D( 90.13950901827667  38.3368469456463)                     new Vector2D( 90.14393814424852  38.337591835857495)                     new Vector2D( 90.14483839716831  38.337076122362475)                     new Vector2D( 90.14565474433601  38.33769000964429)                     new Vector2D( 90.14569421179482  38.3377117256905)                     new Vector2D( 90.14577067124333  38.33770883625908)                     new Vector2D( 90.14600350631684  38.337714326520995)                     new Vector2D( 90.14600355139731  38.33771435193319)                     new Vector2D( 90.14600369112401  38.33771443882085)                     new Vector2D( 90.14600382486884  38.33771453466096)                     new Vector2D( 90.14600395205912  38.33771463904344)                     new Vector2D( 90.14600407214999  38.337714751520764)                     new Vector2D( 90.14600418462749  38.337714871611695)                     new Vector2D( 90.14600422249327  38.337714915811034)                     new Vector2D( 90.14867838361471  38.34113888210675)                     new Vector2D( 90.14923750157374  38.341582537502575)                     new Vector2D( 90.14877083250991  38.34160685841391)                     new Vector2D( 90.14816667319519  38.34244232585684)                     new Vector2D( 90.14797696744586  38.34248455284745)                     new Vector2D( 90.14484318014337  38.34385573215269)                     new Vector2D( 90.14477919958296  38.3453797747614)                     new Vector2D( 90.14202393306448  38.34464324839456)                     new Vector2D( 90.14198920640195  38.344651155237216)                     new Vector2D( 90.14155207025175  38.34486424263724)                     new Vector2D( 90.1415196143314  38.344871730519)                     new Vector2D( 90.14128611910814  38.34500196593859)                     new Vector2D( 90.14047850603913  38.34600084496253)                     new Vector2D( 90.14045907000337  38.34601860032171)                     new Vector2D( 90.14039496493928  38.346223030432384)                     new Vector2D( 90.14037626063737  38.346240203360026)                     new Vector2D( 90.14030005823724  38.34646920000705)                     new Vector2D( 90.13799164754806  38.34903093011013)                     new Vector2D( 90.11045289492762  38.36801537312368)                     new Vector2D( 90.10871471476526  38.36878044144294)                     new Vector2D( 90.10424901707671  38.374300101757)                     new Vector2D( 90.10263482039932  38.37310041316073)                     new Vector2D( 90.09834601753448  38.373615053823414)                     new Vector2D( 90.0979455456843  38.373578376172475)                     new Vector2D( 90.09086514328669  38.37527884194668)                     new Vector2D( 90.09084931407364  38.37590801712463)                     new Vector2D( 90.09081227075944  38.37526295920463)                     new Vector2D( 90.09081378927135  38.375193883266434)                      ;         PolygonsSet set1 = buildSet(vertices1);          Vector2D vertices2 = new Vector2D              new Vector2D                      new Vector2D( 90.13067558880044  38.36977255037573)                     new Vector2D( 90.12907570488  38.36817308242706)                     new Vector2D( 90.1342774136516  38.356886880294724)                     new Vector2D( 90.13090330629757  38.34664392676211)                     new Vector2D( 90.13078571364593  38.344904617518466)                     new Vector2D( 90.1315602208914  38.3447185040846)                     new Vector2D( 90.1316336226821  38.34470643148342)                     new Vector2D( 90.134020944832  38.340936644972885)                     new Vector2D( 90.13912536387306  38.335497255122334)                     new Vector2D( 90.1396178806582  38.334878075552126)                     new Vector2D( 90.14083049696671  38.33316530644106)                     new Vector2D( 90.14145252901329  38.33152722916191)                     new Vector2D( 90.1404779335565  38.32863516047786)                     new Vector2D( 90.14282712131586  38.327504432532066)                     new Vector2D( 90.14616669875488  38.3237354115015)                     new Vector2D( 90.14860976050608  38.315714862457924)                     new Vector2D( 90.14999277782437  38.3164932507504)                     new Vector2D( 90.15005207194997  38.316534677663356)                     new Vector2D( 90.15508513859612  38.31878731691609)                     new Vector2D( 90.15919938519221  38.31852743183782)                     new Vector2D( 90.16093758658837  38.31880662005153)                     new Vector2D( 90.16099420184912  38.318825953291594)                     new Vector2D( 90.1665411125756  38.31859497874757)                     new Vector2D( 90.16999653861313  38.32505772048029)                     new Vector2D( 90.17475243391698  38.32594398441148)                     new Vector2D( 90.17940844844992  38.327427213761325)                     new Vector2D( 90.20951909541378  38.330616833491774)                     new Vector2D( 90.2155400467941  38.331746223670336)                     new Vector2D( 90.21559881391778  38.33175551425302)                     new Vector2D( 90.21916646426041  38.332584299620805)                     new Vector2D( 90.23863749852285  38.34778978875795)                     new Vector2D( 90.25459855175802  38.357790570608984)                     new Vector2D( 90.25964298227257  38.356918010203174)                     new Vector2D( 90.26024593994703  38.361692743151366)                     new Vector2D( 90.26146187570015  38.36311080550837)                     new Vector2D( 90.26614159359622  38.36510808579902)                     new Vector2D( 90.26621342936448  38.36507942500333)                     new Vector2D( 90.26652190211962  38.36494042196722)                     new Vector2D( 90.26621240678867  38.365113172030874)                     new Vector2D( 90.26614057102057  38.365141832826794)                     new Vector2D( 90.26380080055299  38.3660381760273)                     new Vector2D( 90.26315345241  38.36670658276421)                     new Vector2D( 90.26251574942881  38.367490323488084)                     new Vector2D( 90.26247873448426  38.36755266444749)                     new Vector2D( 90.26234628016698  38.36787989125406)                     new Vector2D( 90.26214559424784  38.36945909356126)                     new Vector2D( 90.25861728442555  38.37200753430875)                     new Vector2D( 90.23905557537864  38.375405314295904)                     new Vector2D( 90.22517251874075  38.38984691662256)                     new Vector2D( 90.22549955153215  38.3911564273979)                     new Vector2D( 90.22434386063355  38.391476432092134)                     new Vector2D( 90.22147729457276  38.39134652252034)                     new Vector2D( 90.22142070120117  38.391349167741964)                     new Vector2D( 90.20665060751588  38.39475580900313)                     new Vector2D( 90.20042268367109  38.39842558622888)                     new Vector2D( 90.17423771242085  38.402727751805344)                     new Vector2D( 90.16756796257476  38.40913898597597)                     new Vector2D( 90.16728283954308  38.411255399912875)                     new Vector2D( 90.16703538220418  38.41136059866693)                     new Vector2D( 90.16725865657685  38.41013618805954)                     new Vector2D( 90.16746107640665  38.40902614307544)                     new Vector2D( 90.16122795307462  38.39773101873203)                      ;         PolygonsSet set2 = buildSet(vertices2);         PolygonsSet set  = (PolygonsSet) new RegionFactory<Euclidean2D>().difference(set1.copySelf()                set2.copySelf());          Vector2D verticies = set.getVertices();         Assert.assertTrue(verticies00 != null);         Assert.assertEquals(1 verticies.length);      code", "A_clean_title": ["polygon", "differ", "produc", "erroni", "result", "some", "casesth", "case", "2d", "polygon", "differ", "method", "return", "incorrect", "result", "below", "test", "case", "subtract", "two", "polygon", "sorri", "thi", "simplest", "case", "that", "could", "find", "that", "duplic", "problem", "there", "are", "three", "problem", "result", "first", "that", "first", "point", "first", "set", "vertic", "null", "first", "point", "second", "set", "also", "null", "second", "that", "even", "first", "null", "point", "are", "ignor", "return", "polygon", "not", "correct", "result", "first", "last", "point", "are", "way", "off", "remain", "point", "not", "match", "origin", "polygon", "boundari", "addit", "there", "are", "two", "hole", "that", "are", "return", "result", "thi", "subtract", "case", "not", "have", "hole", "code", "title=complex", "polygon", "differ", "test", "public", "void", "testcomplexdiffer", "test", "complex", "differ", "vector2d", "vertices1", "new", "vector2d", "new", "vector2d", "new", "vector2d", "90", "08714908223715", "38", "370299337260235", "new", "vector2d", "90", "08709517675004", "38", "3702895991413", "new", "vector2d", "90", "08401538704919", "38", "368849330127944", "new", "vector2d", "90", "08258210430711", "38", "367634558585564", "new", "vector2d", "90", "08251455106665", "38", "36763409247078", "new", "vector2d", "90", "08106599752608", "38", "36761621664249", "new", "vector2d", "90", "08249585300035", "38", "36753627557965", "new", "vector2d", "90", "09075743352184", "38", "35914647644972", "new", "vector2d", "90", "09099945896571", "38", "35896264724079", "new", "vector2d", "90", "09269383800086", "38", "34595756121246", "new", "vector2d", "90", "09638631543191", "38", "3457988093121", "new", "vector2d", "90", "09666417351019", "38", "34523360999418", "new", "vector2d", "90", "1297082145872", "38", "337670454923625", "new", "vector2d", "90", "12971687748956", "38", "337669827794684", "new", "vector2d", "90", "1240820219179", "38", "34328502001131", "new", "vector2d", "90", "13084259656404", "38", "34017811765017", "new", "vector2d", "90", "13378567942857", "38", "33860579180606", "new", "vector2d", "90", "13519557833206", "38", "33621054663689", "new", "vector2d", "90", "13545616732307", "38", "33614965452864", "new", "vector2d", "90", "13553111202748", "38", "33613962818305", "new", "vector2d", "90", "1356903436448", "38", "33610227127048", "new", "vector2d", "90", "13576283227428", "38", "33609255422783", "new", "vector2d", "90", "13595870833188", "38", "33604606376991", "new", "vector2d", "90", "1361556630693", "38", "3360024198866", "new", "vector2d", "90", "13622408795709", "38", "335987048115726", "new", "vector2d", "90", "13696189099994", "38", "33581914328681", "new", "vector2d", "90", "13746655304897", "38", "33616706665265", "new", "vector2d", "90", "13845973716064", "38", "33650776167099", "new", "vector2d", "90", "13950901827667", "38", "3368469456463", "new", "vector2d", "90", "14393814424852", "38", "337591835857495", "new", "vector2d", "90", "14483839716831", "38", "337076122362475", "new", "vector2d", "90", "14565474433601", "38", "33769000964429", "new", "vector2d", "90", "14569421179482", "38", "3377117256905", "new", "vector2d", "90", "14577067124333", "38", "33770883625908", "new", "vector2d", "90", "14600350631684", "38", "337714326520995", "new", "vector2d", "90", "14600355139731", "38", "33771435193319", "new", "vector2d", "90", "14600369112401", "38", "33771443882085", "new", "vector2d", "90", "14600382486884", "38", "33771453466096", "new", "vector2d", "90", "14600395205912", "38", "33771463904344", "new", "vector2d", "90", "14600407214999", "38", "337714751520764", "new", "vector2d", "90", "14600418462749", "38", "337714871611695", "new", "vector2d", "90", "14600422249327", "38", "337714915811034", "new", "vector2d", "90", "14867838361471", "38", "34113888210675", "new", "vector2d", "90", "14923750157374", "38", "341582537502575", "new", "vector2d", "90", "14877083250991", "38", "34160685841391", "new", "vector2d", "90", "14816667319519", "38", "34244232585684", "new", "vector2d", "90", "14797696744586", "38", "34248455284745", "new", "vector2d", "90", "14484318014337", "38", "34385573215269", "new", "vector2d", "90", "14477919958296", "38", "3453797747614", "new", "vector2d", "90", "14202393306448", "38", "34464324839456", "new", "vector2d", "90", "14198920640195", "38", "344651155237216", "new", "vector2d", "90", "14155207025175", "38", "34486424263724", "new", "vector2d", "90", "1415196143314", "38", "344871730519", "new", "vector2d", "90", "14128611910814", "38", "34500196593859", "new", "vector2d", "90", "14047850603913", "38", "34600084496253", "new", "vector2d", "90", "14045907000337", "38", "34601860032171", "new", "vector2d", "90", "14039496493928", "38", "346223030432384", "new", "vector2d", "90", "14037626063737", "38", "346240203360026", "new", "vector2d", "90", "14030005823724", "38", "34646920000705", "new", "vector2d", "90", "13799164754806", "38", "34903093011013", "new", "vector2d", "90", "11045289492762", "38", "36801537312368", "new", "vector2d", "90", "10871471476526", "38", "36878044144294", "new", "vector2d", "90", "10424901707671", "38", "374300101757", "new", "vector2d", "90", "10263482039932", "38", "37310041316073", "new", "vector2d", "90", "09834601753448", "38", "373615053823414", "new", "vector2d", "90", "0979455456843", "38", "373578376172475", "new", "vector2d", "90", "09086514328669", "38", "37527884194668", "new", "vector2d", "90", "09084931407364", "38", "37590801712463", "new", "vector2d", "90", "09081227075944", "38", "37526295920463", "new", "vector2d", "90", "09081378927135", "38", "375193883266434", "polygonsset", "polygon", "set", "set1", "buildset", "build", "set", "vertices1", "vector2d", "vertices2", "new", "vector2d", "new", "vector2d", "new", "vector2d", "90", "13067558880044", "38", "36977255037573", "new", "vector2d", "90", "12907570488", "38", "36817308242706", "new", "vector2d", "90", "1342774136516", "38", "356886880294724", "new", "vector2d", "90", "13090330629757", "38", "34664392676211", "new", "vector2d", "90", "13078571364593", "38", "344904617518466", "new", "vector2d", "90", "1315602208914", "38", "3447185040846", "new", "vector2d", "90", "1316336226821", "38", "34470643148342", "new", "vector2d", "90", "134020944832", "38", "340936644972885", "new", "vector2d", "90", "13912536387306", "38", "335497255122334", "new", "vector2d", "90", "1396178806582", "38", "334878075552126", "new", "vector2d", "90", "14083049696671", "38", "33316530644106", "new", "vector2d", "90", "14145252901329", "38", "33152722916191", "new", "vector2d", "90", "1404779335565", "38", "32863516047786", "new", "vector2d", "90", "14282712131586", "38", "327504432532066", "new", "vector2d", "90", "14616669875488", "38", "3237354115015", "new", "vector2d", "90", "14860976050608", "38", "315714862457924", "new", "vector2d", "90", "14999277782437", "38", "3164932507504", "new", "vector2d", "90", "15005207194997", "38", "316534677663356", "new", "vector2d", "90", "15508513859612", "38", "31878731691609", "new", "vector2d", "90", "15919938519221", "38", "31852743183782", "new", "vector2d", "90", "16093758658837", "38", "31880662005153", "new", "vector2d", "90", "16099420184912", "38", "318825953291594", "new", "vector2d", "90", "1665411125756", "38", "31859497874757", "new", "vector2d", "90", "16999653861313", "38", "32505772048029", "new", "vector2d", "90", "17475243391698", "38", "32594398441148", "new", "vector2d", "90", "17940844844992", "38", "327427213761325", "new", "vector2d", "90", "20951909541378", "38", "330616833491774", "new", "vector2d", "90", "2155400467941", "38", "331746223670336", "new", "vector2d", "90", "21559881391778", "38", "33175551425302", "new", "vector2d", "90", "21916646426041", "38", "332584299620805", "new", "vector2d", "90", "23863749852285", "38", "34778978875795", "new", "vector2d", "90", "25459855175802", "38", "357790570608984", "new", "vector2d", "90", "25964298227257", "38", "356918010203174", "new", "vector2d", "90", "26024593994703", "38", "361692743151366", "new", "vector2d", "90", "26146187570015", "38", "36311080550837", "new", "vector2d", "90", "26614159359622", "38", "36510808579902", "new", "vector2d", "90", "26621342936448", "38", "36507942500333", "new", "vector2d", "90", "26652190211962", "38", "36494042196722", "new", "vector2d", "90", "26621240678867", "38", "365113172030874", "new", "vector2d", "90", "26614057102057", "38", "365141832826794", "new", "vector2d", "90", "26380080055299", "38", "3660381760273", "new", "vector2d", "90", "26315345241", "38", "36670658276421", "new", "vector2d", "90", "26251574942881", "38", "367490323488084", "new", "vector2d", "90", "26247873448426", "38", "36755266444749", "new", "vector2d", "90", "26234628016698", "38", "36787989125406", "new", "vector2d", "90", "26214559424784", "38", "36945909356126", "new", "vector2d", "90", "25861728442555", "38", "37200753430875", "new", "vector2d", "90", "23905557537864", "38", "375405314295904", "new", "vector2d", "90", "22517251874075", "38", "38984691662256", "new", "vector2d", "90", "22549955153215", "38", "3911564273979", "new", "vector2d", "90", "22434386063355", "38", "391476432092134", "new", "vector2d", "90", "22147729457276", "38", "39134652252034", "new", "vector2d", "90", "22142070120117", "38", "391349167741964", "new", "vector2d", "90", "20665060751588", "38", "39475580900313", "new", "vector2d", "90", "20042268367109", "38", "39842558622888", "new", "vector2d", "90", "17423771242085", "38", "402727751805344", "new", "vector2d", "90", "16756796257476", "38", "40913898597597", "new", "vector2d", "90", "16728283954308", "38", "411255399912875", "new", "vector2d", "90", "16703538220418", "38", "41136059866693", "new", "vector2d", "90", "16725865657685", "38", "41013618805954", "new", "vector2d", "90", "16746107640665", "38", "40902614307544", "new", "vector2d", "90", "16122795307462", "38", "39773101873203", "polygonsset", "polygon", "set", "set2", "buildset", "build", "set", "vertices2", "polygonsset", "polygon", "set", "set", "polygonsset", "polygon", "set", "new", "regionfactori", "region", "factori", "euclidean2d", "differ", "set1", "copyself", "copi", "self", "set2", "copyself", "copi", "self", "vector2d", "vertici", "set", "getvertic", "get", "vertic", "assert", "asserttru", "assert", "true", "verticies00", "null", "assert", "assertequ", "assert", "equal", "vertici", "length", "code"], "B_title": "Finalized fix for MATH-880.", "B_clean_title": ["final", "fix", "math", "880"]},
{"A_title": "Some version copy settings conflicts with the earlyShutdownThe RepositoryUpgrade#earlyShutdown property causes the source CRX2 repository to shutdown right after copying the content before the first commit hook is launched. However the VersionableEditor hook sometimes needs access to the source repository to read the version histories that hasnt been copied yet (as the version histories are copied in two stages). As a result the earlyShutdown may cause the upgrade process to fail.  earlyShutdown should be overriden for all cases in which the source repository is still needed in the commit hook phase. In particular it should be set to false if:  * orphaned version histories are not copied * orphaned version histories are copied but the copyOrphanedVersion date is set after the copyVersion date.", "A_clean_title": ["some", "version", "copi", "set", "conflict", "earlyshutdownth", "earli", "shutdown", "repositoryupgrad", "repositori", "upgrad", "earlyshutdown", "earli", "shutdown", "properti", "caus", "sourc", "crx2", "repositori", "shutdown", "right", "after", "copi", "content", "befor", "first", "commit", "hook", "launch", "howev", "versionableeditor", "version", "editor", "hook", "sometim", "need", "access", "sourc", "repositori", "read", "version", "histori", "that", "hasnt", "been", "copi", "yet", "as", "version", "histori", "are", "copi", "two", "stage", "as", "result", "earlyshutdown", "earli", "shutdown", "may", "caus", "upgrad", "process", "fail", "earlyshutdown", "earli", "shutdown", "overriden", "all", "case", "which", "sourc", "repositori", "still", "need", "commit", "hook", "phase", "particular", "it", "set", "fals", "orphan", "version", "histori", "are", "not", "copi", "orphan", "version", "histori", "are", "copi", "but", "copyorphanedvers", "copi", "orphan", "version", "date", "set", "after", "copyvers", "copi", "version", "date"], "B_title": "Some version copy settings conflicts with the earlyShutdown", "B_clean_title": ["some", "version", "copi", "set", "conflict", "earlyshutdown", "earli", "shutdown"]},
{"A_title": "Parsing of ChinUnionPay credit card should use the first 6 charactersUser report:  A China UnionPay number has to start with 622 (622126-622925) and has to have a length between 16 and 19. The source code of CreditCardValidator is:    220   private boolean isChinaUnionPay(String creditCardNumber)   221      222   cardId = CreditCardValidator.INVALID;   223   boolean returnValue = false;   224      225   if ((creditCardNumber.length() >= 16 && creditCardNumber.length() <= 19) &&   226   (creditCardNumber.startsWith(622)))   227      228   int firstDigits = Integer.parseInt(creditCardNumber.substring(0 5));   229   if (firstDigits >= 622126 && firstDigits <= 622925)   230      231   cardId = CreditCardValidator.CHINA_UNIONPAY;   232   returnValue = true;   233      234      235      236   return returnValue;   237    The problem is on the line 228 because the substring returns the first 5 digits and it is compared to 6 digits so firstDigits is always < than 622126. The fix is to do #substring(0 6).", "A_clean_title": ["pars", "chinunionpay", "chin", "union", "pay", "credit", "card", "use", "first", "charactersus", "charact", "user", "report", "china", "unionpay", "union", "pay", "number", "ha", "start", "622", "622126", "622925", "ha", "have", "length", "between", "16", "19", "sourc", "code", "creditcardvalid", "credit", "card", "valid", "220", "privat", "boolean", "ischinaunionpay", "china", "union", "pay", "string", "creditcardnumb", "credit", "card", "number", "221", "222", "cardid", "card", "id", "creditcardvalid", "invalid", "credit", "card", "valid", "223", "boolean", "returnvalu", "return", "valu", "fals", "224", "225", "creditcardnumb", "length", "credit", "card", "number", "16", "creditcardnumb", "length", "credit", "card", "number", "19", "226", "creditcardnumb", "startswith", "credit", "card", "number", "start", "622", "227", "228", "int", "firstdigit", "first", "digit", "integ", "parseint", "pars", "int", "creditcardnumb", "substr", "credit", "card", "number", "229", "firstdigit", "first", "digit", "622126", "firstdigit", "first", "digit", "622925", "230", "231", "cardid", "card", "id", "creditcardvalid", "credit", "card", "valid", "china", "unionpay", "232", "returnvalu", "return", "valu", "true", "233", "234", "235", "236", "return", "returnvalu", "return", "valu", "237", "problem", "line", "228", "becaus", "substr", "return", "first", "digit", "it", "compar", "digit", "so", "firstdigit", "first", "digit", "alway", "than", "622126", "fix", "substr"], "B_title": "Parsing of ChinUnionPay credit card should use the first 6 characters", "B_clean_title": ["pars", "chinunionpay", "chin", "union", "pay", "credit", "card", "use", "first", "charact"]},
{"A_title": "TableTrees NodeBorder does not properly close divsNodeBorder fails to properly close generated <div>s.", "A_clean_title": ["tabletre", "tabl", "tree", "nodebord", "node", "border", "not", "properli", "close", "divsnodebord", "div", "node", "border", "fail", "properli", "close", "gener", "div"], "B_title": "close generated divs", "B_clean_title": ["close", "gener", "div"]},
{"A_title": "RandomDataImpl.nextInt does not distribute uniformly for negative lower boundWhen using the RandomDataImpl.nextInt function to get a uniform sample in a lower upper interval when the lower value is less than zero the output is not uniformly distributed as the lowest value is practically never returned.  See the attached NextIntUniformTest.java file. It uses a -3 5 interval. For several values between 0 and 1 testNextIntUniform1 prints the return value of RandomDataImpl.nextInt (as double and as int). We see that -2 through 5 are returned several times. The -3 value however is only returned for 0.0 and is thus under-respresented in the integer samples. The output of test method testNextIntUniform2 also clearly shows that value -3 is never sampled.", "A_clean_title": ["randomdataimpl", "nextint", "random", "data", "impl", "next", "int", "not", "distribut", "uniformli", "neg", "lower", "boundwhen", "bound", "when", "randomdataimpl", "nextint", "random", "data", "impl", "next", "int", "function", "get", "uniform", "sampl", "lower", "upper", "interv", "when", "lower", "valu", "less", "than", "zero", "output", "not", "uniformli", "distribut", "as", "lowest", "valu", "practic", "never", "return", "see", "attach", "nextintuniformtest", "java", "next", "int", "uniform", "test", "file", "it", "use", "interv", "sever", "valu", "between", "testnextintuniform1", "test", "next", "int", "uniform1", "print", "return", "valu", "randomdataimpl", "nextint", "random", "data", "impl", "next", "int", "as", "doubl", "as", "int", "we", "see", "that", "through", "are", "return", "sever", "time", "valu", "howev", "onli", "return", "thu", "under", "respres", "integ", "sampl", "output", "test", "method", "testnextintuniform2", "test", "next", "int", "uniform2", "also", "clearli", "show", "that", "valu", "never", "sampl"], "B_title": "Fixed rounding error in RandomDataImpl nextInt nextLong methods causing lower endpoints to be excluded when negative. Also improved robustness of nextUniform for extreme values and changed its contract to throw IAE when provided bounds are infinite or NaN.", "B_clean_title": ["fix", "round", "error", "randomdataimpl", "random", "data", "impl", "nextint", "next", "int", "nextlong", "next", "long", "method", "caus", "lower", "endpoint", "exclud", "when", "neg", "also", "improv", "robust", "nextuniform", "next", "uniform", "extrem", "valu", "chang", "it", "contract", "throw", "iae", "when", "provid", "bound", "are", "infinit", "or", "nan", "na"]},
{"A_title": "missing support for relative path consisting of parent-elementcould not reopen OAK-95 -> cloning. during testing of user-mgt api found that relpath consisting of a single parent element doesnt work (but used to):  code @Test     public void getNode3() throws RepositoryException          Node node = getNode(/foo);         Node root = node.getNode(..);         assertNotNull(root);         assertEquals( root.getName());         assertTrue(/.equals(root.getPath()));      :  code", "A_clean_title": ["miss", "support", "rel", "path", "consist", "parent", "elementcould", "not", "reopen", "oak", "95", "clone", "dure", "test", "user", "mgt", "api", "found", "that", "relpath", "consist", "singl", "parent", "element", "doesnt", "work", "but", "use", "code", "test", "public", "void", "getnode3", "get", "node3", "throw", "repositoryexcept", "repositori", "except", "node", "node", "getnod", "get", "node", "foo", "node", "root", "node", "getnod", "get", "node", "assertnotnul", "assert", "not", "null", "root", "assertequ", "assert", "equal", "root", "getnam", "get", "name", "asserttru", "assert", "true", "equal", "root", "getpath", "get", "path", "code"], "B_title": "missing support for relative path consisting of parent-element", "B_clean_title": ["miss", "support", "rel", "path", "consist", "parent", "element"]},
{"A_title": "Invalid generics resolution for locally declared wildcard and fully resolved target type DATACMNS-1138opened and commented Given the following context:    An entity declares a field which type is a class with a wildcard type  An implementation of the fields class is typed with a custom object  A custom converter has been declared for the custom object  We persist an entity with the custom class in Mongo  Then when we retrieve the entity from the database the field with the custom type is not deserialized by the custom converter.  This problem does not happen if:   We remove the wildcard from the declared field  We use a type that does not require a custom converter (e.g. Integer)  Im not sure if this description is clear please take a look at the project on GitHub:  https://github.com/mclem/spring-data-mongodb-generics to reproduce the problem by running mvn test   Affects: 1.12.11 (Hopper SR11) 1.13.6 (Ingalls SR6) 2.0 RC2 (Kay)  Reference URL:  https://github.com/mclem/spring-data-mongodb-generics  Issue Links:     Backported to:  1.13.7 (Ingalls SR7)  1.12.12 (Hopper SR12)", "A_clean_title": ["invalid", "gener", "resolut", "local", "declar", "wildcard", "fulli", "resolv", "target", "type", "datacmn", "1138open", "comment", "given", "follow", "context", "entiti", "declar", "field", "which", "type", "class", "wildcard", "type", "implement", "field", "class", "type", "custom", "object", "custom", "convert", "ha", "been", "declar", "custom", "object", "we", "persist", "entiti", "custom", "class", "mongo", "then", "when", "we", "retriev", "entiti", "databas", "field", "custom", "type", "not", "deseri", "by", "custom", "convert", "thi", "problem", "not", "happen", "we", "remov", "wildcard", "declar", "field", "we", "use", "type", "that", "not", "requir", "custom", "convert", "integ", "im", "not", "sure", "thi", "descript", "clear", "pleas", "take", "look", "at", "project", "github", "git", "hub", "http", "data", "mongodb", "gener", "github", "com", "mclem", "spring", "reproduc", "problem", "by", "run", "mvn", "test", "affect", "12", "11", "hopper", "sr11", "13", "ingal", "sr6", "rc2", "kay", "refer", "url", "http", "data", "mongodb", "gener", "github", "com", "mclem", "spring", "issu", "link", "backport", "13", "ingal", "sr7", "12", "12", "hopper", "sr12"], "B_title": "DATACMNS-1138 - TypeInformation.specialize(…) now only specializes unresolved parameterized types.  Type specialization - i.e. enrichment of a raw type with a current generic context - is now only done if the current type is not yet resolved completely. This allows wildcarded target references to just fall back to the type to specialize which will then by definition carry more generics information than the one to be specialized.", "B_clean_title": ["datacmn", "1138", "typeinform", "special", "type", "inform", "now", "onli", "special", "unresolv", "parameter", "type", "type", "special", "enrich", "raw", "type", "current", "gener", "context", "now", "onli", "done", "current", "type", "not", "yet", "resolv", "complet", "thi", "allow", "wildcard", "target", "refer", "just", "fall", "back", "type", "special", "which", "will", "then", "by", "definit", "carri", "more", "gener", "inform", "than", "one", "special"]},
{"A_title": "OakIndexInput cloned instances are not closedRelated to the inspections I was doing for OAK-2798 I also noticed that we dont fully comply with the IndexInput javadoc 1 as the cloned instances should throw the given exception if original is closed but I also think that the original instance should close the cloned instances see also ByteBufferIndexInput#close|https://github.com/apache/lucene-solr/blob/lucene_solr_4_7_1/lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java#L271.  1 : code /** Abstract base class for input from a file in a @link Directory.  A  * random-access input stream.  Used for all Lucene index input operations.  *  * <p>@code IndexInput may only be used from one thread because it is not  * thread safe (it keeps internal state like file position). To allow  * multithreaded use every @code IndexInput instance must be cloned before  * used in another thread. Subclasses must therefore implement @link #clone()  * returning a new @code IndexInput which operates on the same underlying  * resource but positioned independently. Lucene never closes cloned  * @code IndexInputs it will only do this on the original one.  * The original instance must take care that cloned instances throw  * @link AlreadyClosedException when the original one is closed. code", "A_clean_title": ["oakindexinput", "oak", "index", "input", "clone", "instanc", "are", "not", "closedrel", "close", "relat", "inspect", "wa", "do", "oak", "2798", "also", "notic", "that", "we", "dont", "fulli", "compli", "indexinput", "index", "input", "javadoc", "as", "clone", "instanc", "throw", "given", "except", "origin", "close", "but", "also", "think", "that", "origin", "instanc", "close", "clone", "instanc", "see", "also", "bytebufferindexinput", "byte", "buffer", "index", "input", "close|http", "java", "github", "com", "apach", "lucen", "solr", "solr", "blob", "lucen", "lucen", "core", "src", "java", "org", "apach", "lucen", "store", "bytebufferindexinput", "byte", "buffer", "index", "input", "l271", "code", "abstract", "base", "class", "input", "file", "link", "directori", "random", "access", "input", "stream", "use", "all", "lucen", "index", "input", "oper", "code", "indexinput", "index", "input", "may", "onli", "use", "one", "thread", "becaus", "it", "not", "thread", "safe", "it", "keep", "intern", "state", "like", "file", "posit", "allow", "multithread", "use", "everi", "code", "indexinput", "index", "input", "instanc", "must", "clone", "befor", "use", "anoth", "thread", "subclass", "must", "therefor", "implement", "link", "clone", "return", "new", "code", "indexinput", "index", "input", "which", "oper", "same", "underli", "resourc", "but", "posit", "independ", "lucen", "never", "close", "clone", "code", "indexinput", "index", "input", "it", "will", "onli", "thi", "origin", "one", "origin", "instanc", "must", "take", "care", "that", "clone", "instanc", "throw", "link", "alreadyclosedexcept", "alreadi", "close", "except", "when", "origin", "one", "close", "code"], "B_title": "- close cloned OakIndexInput instances on close of main one", "B_clean_title": ["close", "clone", "oakindexinput", "oak", "index", "input", "instanc", "close", "main", "one"]},
{"A_title": "Spaces in path cause ModifcationWatcher to failThe ModificationWatcher isnt able to reload resource files if theres a space in the path.  The problem is that Files#getLocalFileFromUrl(String) receives an URL encoded String in which spaces are encoded to %20. They are never decoded and passed to File(). The fix is not to use the external representation of an URL but the file representation.", "A_clean_title": ["space", "path", "caus", "modifcationwatch", "modifc", "watcher", "failth", "fail", "modificationwatch", "modif", "watcher", "isnt", "abl", "reload", "resourc", "file", "there", "space", "path", "problem", "that", "file", "getlocalfilefromurl", "get", "local", "file", "url", "string", "receiv", "url", "encod", "string", "which", "space", "are", "encod", "20", "they", "are", "never", "decod", "pass", "file", "fix", "not", "use", "extern", "represent", "url", "but", "file", "represent"], "B_title": "url must be decoded for local files", "B_clean_title": ["url", "must", "decod", "local", "file"]},
{"A_title": "Parent of unseen children must not be removableWith OAK-2673 its now possible to have hidden intermediate nodes created concurrently. So a scenario like: noformat start -> /:hidden N1 creates /:hiddent/parent/node1 N2 creates /:hidden/parent/node2 noformat is allowed.  But if N2s creation of parent got persisted later than that on N1 then N2 is currently able to delete parent even though theres node1.", "A_clean_title": ["parent", "unseen", "children", "must", "not", "removablewith", "remov", "oak", "2673", "it", "now", "possibl", "have", "hidden", "intermedi", "node", "creat", "concurr", "so", "scenario", "like", "noformat", "start", "hidden", "n1", "creat", "hiddent", "parent", "node1", "n2", "creat", "hidden", "parent", "node2", "noformat", "allow", "but", "n2", "creation", "parent", "got", "persist", "later", "than", "that", "n1", "then", "n2", "current", "abl", "delet", "parent", "even", "though", "there", "node1"], "B_title": "Parent of unseen children must not be removable", "B_clean_title": ["parent", "unseen", "children", "must", "not", "remov"]},
{"A_title": "Adding a node with the name of a removed node can lead to an inconsistent hierarchy of node buildersNone", "A_clean_title": ["ad", "node", "name", "remov", "node", "lead", "inconsist", "hierarchi", "node", "buildersnon", "builder", "none"], "B_title": "Adding a node with the name of a removed node can lead to an inconsistent hierarchy of node builders", "B_clean_title": ["ad", "node", "name", "remov", "node", "lead", "inconsist", "hierarchi", "node", "builder"]},
{"A_title": "Async index update persists conflict markersA long running test I performed yesterday failed with a FileNotFoundException in the lucene index. After analyzing the issue it turned  out the async index update persisted a conflict markers introduced by a rebase call. So far Im not able to reproduce it with a more simple test setup and after a shorter time (the initial test failed after 10 hours). Given the way the async index update work there shouldnt be any conflicts because its the only component writing into this location of the repository.   As an immediate workaround Id like to add the AnnotatingConflictHandler & ConflictValidator combo to the merge call to make sure a commit with conflict markers does not get persisted.", "A_clean_title": ["async", "index", "updat", "persist", "conflict", "markersa", "marker", "long", "run", "test", "perform", "yesterday", "fail", "filenotfoundexcept", "file", "not", "found", "except", "lucen", "index", "after", "analyz", "issu", "it", "turn", "out", "async", "index", "updat", "persist", "conflict", "marker", "introduc", "by", "rebas", "call", "so", "far", "im", "not", "abl", "reproduc", "it", "more", "simpl", "test", "setup", "after", "shorter", "time", "initi", "test", "fail", "after", "10", "hour", "given", "way", "async", "index", "updat", "work", "there", "shouldnt", "ani", "conflict", "becaus", "it", "onli", "compon", "write", "into", "thi", "locat", "repositori", "as", "immedi", "workaround", "id", "like", "add", "annotatingconflicthandl", "annot", "conflict", "handler", "conflictvalid", "conflict", "valid", "combo", "merg", "call", "make", "sure", "commit", "conflict", "marker", "not", "get", "persist"], "B_title": "Async index update persists conflict markers", "B_clean_title": ["async", "index", "updat", "persist", "conflict", "marker"]}]