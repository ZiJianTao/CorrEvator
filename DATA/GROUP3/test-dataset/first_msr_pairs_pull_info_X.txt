[{"A_title": "weird object literal invalid property error on unrelated object prototypeNone", "A_clean_title": ["weird", "object", "liter", "invalid", "properti", "error", "unrel", "object", "prototypenon", "prototyp", "none"], "B_title": "Avoid matching against an unneeded property .. ", "B_clean_title": ["avoid", "match", "against", "unneed", "properti"]},
{"A_title": "Classify non-rightmost expressions as problematicNone", "A_clean_title": ["classifi", "non", "rightmost", "express", "as", "problematicnon", "problemat", "none"], "B_title": "Remove one more for - loop. ", "B_clean_title": ["remov", "one", "more", "loop"]},
{"A_title": "Classify non-rightmost expressions as problematicNone", "A_clean_title": ["classifi", "non", "rightmost", "express", "as", "problematicnon", "problemat", "none"], "B_title": "Updated patch. ", "B_clean_title": ["updat", "patch"]},
{"A_title": "getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries. The current implementation in ArrayRealVector has a typo:      public double getLInfNorm()          double max = 0;         for (double a : data)              max += Math.max(max Math.abs(a));                  return max;        the += should just be an =. There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test not a test for correctness). Worse the implementation in OpenMapRealVector is not even positive semi-definite:          public double getLInfNorm()          double max = 0;         Iterator iter = entries.iterator();         while (iter.hasNext())              iter.advance();             max += iter.value();                  return max;        I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():    public double getLInfNorm()      double norm = 0;     Iterator<Entry> it = sparseIterator();     Entry e;     while(it.hasNext() && (e = it.next()) != null)        norm = Math.max(norm Math.abs(e.getValue()));          return norm;      Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.", "A_clean_title": ["getlinfnorm", "get", "inf", "norm", "use", "wrong", "formula", "both", "arrayrealvector", "array", "real", "vector", "openmaprealvector", "open", "map", "real", "vector", "differ", "way", "infin", "norm", "finit", "dimension", "vector", "just", "max", "absolut", "valu", "it", "entri", "current", "implement", "arrayrealvector", "array", "real", "vector", "ha", "typo", "public", "doubl", "getlinfnorm", "get", "inf", "norm", "doubl", "max", "doubl", "data", "max", "math", "max", "max", "math", "ab", "return", "max", "just", "there", "sadli", "unit", "test", "assur", "us", "that", "thi", "correct", "behavior", "effect", "regress", "onli", "test", "not", "test", "correct", "wors", "implement", "openmaprealvector", "open", "map", "real", "vector", "not", "even", "posit", "semi", "definit", "public", "doubl", "getlinfnorm", "get", "inf", "norm", "doubl", "max", "iter", "iter", "entri", "iter", "while", "iter", "hasnext", "ha", "next", "iter", "advanc", "max", "iter", "valu", "return", "max", "would", "suggest", "that", "thi", "method", "move", "up", "abstractrealvector", "abstract", "real", "vector", "superclass", "implement", "sparseiter", "spars", "iter", "public", "doubl", "getlinfnorm", "get", "inf", "norm", "doubl", "norm", "iter", "entri", "it", "sparseiter", "spars", "iter", "entri", "while", "it", "hasnext", "ha", "next", "it", "next", "null", "norm", "math", "max", "norm", "math", "ab", "getvalu", "get", "valu", "return", "norm", "unit", "test", "neg", "valu", "vector", "would", "help", "check", "thi", "kind", "thing", "futur"], "B_title": "Fix ArrayRealVector . getLInfNorm ( ). Added get ( ) method to OpenMapRealVector. ", "B_clean_title": ["fix", "arrayrealvector", "array", "real", "vector", "getlinfnorm", "get", "inf", "norm", "ad", "get", "method", "openmaprealvector", "open", "map", "real", "vector"]},
{"A_title": "Constructing invalid PartialsPartials can be constructed by invoking a constructor Partial(DateTimeFieldType int) or by merging together a set of partials using with each constructed by calling Partial(DateTimeFieldType int). However the above doesnt work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  Theres also a related issue (probably stems from the fact that the Partial is invalid):", "A_clean_title": ["construct", "invalid", "partialsparti", "partial", "partial", "construct", "by", "invok", "constructor", "partial", "datetimefieldtyp", "date", "time", "field", "type", "int", "or", "by", "merg", "togeth", "set", "partial", "each", "construct", "by", "call", "partial", "datetimefieldtyp", "date", "time", "field", "type", "int", "howev", "abov", "doesnt", "work", "all", "case", "suppos", "partial", "not", "allow", "construct", "either", "case", "that", "right", "there", "also", "relat", "issu", "probabl", "stem", "fact", "that", "partial", "invalid"], "B_title": "Fix partial constructor to not use iChronology directly in the partial constructor. ", "B_clean_title": ["fix", "partial", "constructor", "not", "use", "ichronolog", "chronolog", "directli", "partial", "constructor"]},
{"A_title": "Constructing invalid PartialsPartials can be constructed by invoking a constructor Partial(DateTimeFieldType int) or by merging together a set of partials using with each constructed by calling Partial(DateTimeFieldType int). However the above doesnt work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  Theres also a related issue (probably stems from the fact that the Partial is invalid):", "A_clean_title": ["construct", "invalid", "partialsparti", "partial", "partial", "construct", "by", "invok", "constructor", "partial", "datetimefieldtyp", "date", "time", "field", "type", "int", "or", "by", "merg", "togeth", "set", "partial", "each", "construct", "by", "call", "partial", "datetimefieldtyp", "date", "time", "field", "type", "int", "howev", "abov", "doesnt", "work", "all", "case", "suppos", "partial", "not", "allow", "construct", "either", "case", "that", "right", "there", "also", "relat", "issu", "probabl", "stem", "fact", "that", "partial", "invalid"], "B_title": "Fix partial constructor to work with joda - time. ", "B_clean_title": ["fix", "partial", "constructor", "work", "joda", "time"]},
{"A_title": "ClassCastException during TypeCheck passNone", "A_clean_title": ["classcastexcept", "class", "cast", "except", "dure", "typecheck", "type", "check", "passnon", "pass", "none"], "B_title": "Improved performance for record types .. ", "B_clean_title": ["improv", "perform", "record", "type"]},
{"A_title": "ClassCastException during TypeCheck passNone", "A_clean_title": ["classcastexcept", "class", "cast", "except", "dure", "typecheck", "type", "check", "passnon", "pass", "none"], "B_title": "Improved method return type for JSType . getLeastSupertype ( JSType ). ", "B_clean_title": ["improv", "method", "return", "type", "jstype", "js", "type", "getleastsupertyp", "get", "least", "supertyp", "jstype", "js", "type"]},
{"A_title": "ClassCastException during TypeCheck passNone", "A_clean_title": ["classcastexcept", "class", "cast", "except", "dure", "typecheck", "type", "check", "passnon", "pass", "none"], "B_title": "Remove recording type properties where they are not equivalent. ", "B_clean_title": ["remov", "record", "type", "properti", "where", "they", "are", "not", "equival"]},
{"A_title": "LocaleUtils.toLocale() rejects strings with only language+variantLocaleUtils.toLocale() throws an exception on strings containing a language and a variant but no country code. For example : fr__POSIX This string can be produced with the JDK by instanciating a Locale with an empty string for the country : new Locale(fr  POSIX).toString(). According to the javadoc for the Locale class a variant is allowed with just a language code or just a country code. Commons Configuration handles this case in its PropertyConverter.toLocale() method. Id like to replace our implementation by the one provided by LocaleUtils but our tests fail due to this case.", "A_clean_title": ["localeutil", "tolocal", "local", "util", "local", "reject", "string", "onli", "language+variantlocaleutil", "tolocal", "language+vari", "local", "util", "local", "throw", "except", "string", "contain", "languag", "variant", "but", "no", "countri", "code", "exampl", "fr", "posix", "thi", "string", "produc", "jdk", "by", "instanci", "local", "empti", "string", "countri", "new", "local", "fr", "posix", "tostr", "string", "accord", "javadoc", "local", "class", "variant", "allow", "just", "languag", "code", "or", "just", "countri", "code", "common", "configur", "handl", "thi", "case", "it", "propertyconvert", "tolocal", "properti", "convert", "local", "method", "id", "like", "replac", "our", "implement", "by", "one", "provid", "by", "localeutil", "local", "util", "but", "our", "test", "fail", "due", "thi", "case"], "B_title": "Remove spaces. Missing _ sign in LocaleUtils. Added missing copy of LocaleUtils. ", "B_clean_title": ["remov", "space", "miss", "sign", "localeutil", "local", "util", "ad", "miss", "copi", "localeutil", "local", "util"]},
{"A_title": "Bugs in Frequency APII think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object but one cannot add anything further to the instance as the second add fails with IllegalArgumentException. In fact the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods dont fail when given a non-Comparable object (because the class cast exception is caught) however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail but doesnt causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable it seems to me that the API would be much improved by using Comparable instead of Object. Also it should make it easier to implement generics. However this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly as the first attempt to add a non-Comparable object would fail rather than the second add of a possibly valid object. But is that a viable program? It can only add one object and any attempt to get statistics will either return 0 or an Exception and applying the instanceof fix would also cause it to fail.", "A_clean_title": ["bug", "frequenc", "apii", "think", "exist", "frequenc", "api", "ha", "some", "bug", "it", "addvalu", "add", "valu", "object", "method", "allow", "one", "add", "plain", "object", "but", "one", "not", "add", "anyth", "further", "instanc", "as", "second", "add", "fail", "illegalargumentexcept", "illeg", "argument", "except", "fact", "problem", "first", "call", "addvalu", "add", "valu", "object", "which", "not", "allow", "plain", "object", "ad", "it", "onli", "allow", "compar", "object", "thi", "could", "fix", "by", "check", "that", "object", "compar", "similar", "consider", "appli", "getcumfreq", "get", "cum", "freq", "object", "getcumpct", "get", "cum", "pct", "object", "method", "they", "will", "onli", "work", "object", "that", "implement", "compar", "getcount", "get", "count", "object", "getpct", "get", "pct", "object", "method", "dont", "fail", "when", "given", "non", "compar", "object", "becaus", "class", "cast", "except", "caught", "howev", "they", "just", "return", "as", "object", "wa", "not", "present", "final", "object", "obj", "new", "object", "addvalu", "add", "valu", "obj", "thi", "ought", "fail", "but", "doesnt", "caus", "unexpect", "behaviour", "below", "system", "out", "println", "getcount", "get", "count", "obj", "system", "out", "println", "getpct", "get", "pct", "obj", "rather", "than", "ad", "extra", "check", "compar", "it", "seem", "me", "that", "api", "would", "much", "improv", "by", "compar", "instead", "object", "also", "it", "make", "it", "easier", "implement", "gener", "howev", "thi", "would", "caus", "compil", "failur", "some", "program", "that", "pass", "object", "rather", "than", "compar", "class", "these", "would", "need", "recod", "but", "think", "they", "would", "continu", "run", "ok", "against", "new", "api", "it", "would", "also", "affect", "run", "time", "behaviour", "slightli", "as", "first", "attempt", "add", "non", "compar", "object", "would", "fail", "rather", "than", "second", "add", "possibl", "valid", "object", "but", "that", "viabl", "program", "it", "onli", "add", "one", "object", "ani", "attempt", "get", "statist", "will", "either", "return", "or", "except", "appli", "instanceof", "fix", "would", "also", "caus", "it", "fail"], "B_title": "AddValue ( Object ) to Frequency  throw exception if it is not a Comparable. ", "B_clean_title": ["addvalu", "add", "valu", "object", "frequenc", "throw", "except", "it", "not", "compar"]},
{"A_title": "Bugs in Frequency APII think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object but one cannot add anything further to the instance as the second add fails with IllegalArgumentException. In fact the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods dont fail when given a non-Comparable object (because the class cast exception is caught) however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail but doesnt causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable it seems to me that the API would be much improved by using Comparable instead of Object. Also it should make it easier to implement generics. However this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly as the first attempt to add a non-Comparable object would fail rather than the second add of a possibly valid object. But is that a viable program? It can only add one object and any attempt to get statistics will either return 0 or an Exception and applying the instanceof fix would also cause it to fail.", "A_clean_title": ["bug", "frequenc", "apii", "think", "exist", "frequenc", "api", "ha", "some", "bug", "it", "addvalu", "add", "valu", "object", "method", "allow", "one", "add", "plain", "object", "but", "one", "not", "add", "anyth", "further", "instanc", "as", "second", "add", "fail", "illegalargumentexcept", "illeg", "argument", "except", "fact", "problem", "first", "call", "addvalu", "add", "valu", "object", "which", "not", "allow", "plain", "object", "ad", "it", "onli", "allow", "compar", "object", "thi", "could", "fix", "by", "check", "that", "object", "compar", "similar", "consider", "appli", "getcumfreq", "get", "cum", "freq", "object", "getcumpct", "get", "cum", "pct", "object", "method", "they", "will", "onli", "work", "object", "that", "implement", "compar", "getcount", "get", "count", "object", "getpct", "get", "pct", "object", "method", "dont", "fail", "when", "given", "non", "compar", "object", "becaus", "class", "cast", "except", "caught", "howev", "they", "just", "return", "as", "object", "wa", "not", "present", "final", "object", "obj", "new", "object", "addvalu", "add", "valu", "obj", "thi", "ought", "fail", "but", "doesnt", "caus", "unexpect", "behaviour", "below", "system", "out", "println", "getcount", "get", "count", "obj", "system", "out", "println", "getpct", "get", "pct", "obj", "rather", "than", "ad", "extra", "check", "compar", "it", "seem", "me", "that", "api", "would", "much", "improv", "by", "compar", "instead", "object", "also", "it", "make", "it", "easier", "implement", "gener", "howev", "thi", "would", "caus", "compil", "failur", "some", "program", "that", "pass", "object", "rather", "than", "compar", "class", "these", "would", "need", "recod", "but", "think", "they", "would", "continu", "run", "ok", "against", "new", "api", "it", "would", "also", "affect", "run", "time", "behaviour", "slightli", "as", "first", "attempt", "add", "non", "compar", "object", "would", "fail", "rather", "than", "second", "add", "possibl", "valid", "object", "but", "that", "viabl", "program", "it", "onli", "add", "one", "object", "ani", "attempt", "get", "statist", "will", "either", "return", "or", "except", "appli", "instanceof", "fix", "would", "also", "caus", "it", "fail"], "B_title": "Adding missing throw in Frequency . addValue ( ). ", "B_clean_title": ["ad", "miss", "throw", "frequenc", "addvalu", "add", "valu"]},
{"A_title": "bogus missing return warningNone", "A_clean_title": ["bogu", "miss", "return", "warningnon", "warn", "none"], "B_title": "Fixing finally map .. ", "B_clean_title": ["fix", "final", "map"]},
{"A_title": "StringValidator.exactLength has wrong variable in ErrorMessageIn error message for StringValidator.exactLength is variable  exact  but in StringValidator.decorate is added variable length to map and not exact.   Exception when is error message interpolate for show in feedback.  Caused by: java.lang.IllegalArgumentException: Value of variable exact could not be resolved while interpolating  label is not exactly  exact characters long.  property from application. StringValidator.exact= label is not exactly  exact characters long.  When I added same property in my own properties and change exact to length it works.", "A_clean_title": ["stringvalid", "exactlength", "string", "valid", "exact", "length", "ha", "wrong", "variabl", "errormessagein", "error", "messag", "error", "messag", "stringvalid", "exactlength", "string", "valid", "exact", "length", "variabl", "exact", "but", "stringvalid", "decor", "string", "valid", "ad", "variabl", "length", "map", "not", "exact", "except", "when", "error", "messag", "interpol", "show", "feedback", "caus", "by", "java", "lang", "illegalargumentexcept", "illeg", "argument", "except", "valu", "variabl", "exact", "could", "not", "resolv", "while", "interpol", "label", "not", "exactli", "exact", "charact", "long", "properti", "applic", "stringvalid", "exact=", "string", "valid", "label", "not", "exactli", "exact", "charact", "long", "when", "ad", "same", "properti", "my", "own", "properti", "chang", "exact", "length", "it", "work"], "B_title": "exact variable is missing", "B_clean_title": ["exact", "variabl", "miss"]},
{"A_title": "DerivativeStructure.atan2(yx) does not handle special cases properlyThe four special cases +/-0 for both x and y should give the same values as Math.atan2 and FastMath.atan2. However they give NaN for the value in all cases.", "A_clean_title": ["derivativestructur", "atan2", "deriv", "structur", "yx", "not", "handl", "special", "case", "properlyth", "properli", "four", "special", "case", "both", "give", "same", "valu", "as", "math", "atan2", "fastmath", "atan2", "fast", "math", "howev", "they", "give", "nan", "na", "valu", "all", "case"], "B_title": "Fixed DerivativeStructure.atan2 for special cases when both arguments are +/-0.", "B_clean_title": ["fix", "derivativestructur", "atan2", "deriv", "structur", "special", "case", "when", "both", "argument", "are"]},
{"A_title": "RangeInputSplit Writable methods dont serialize IteratorSettingsWas trying to figure out why some information was getting lost on a RangeInputSplit after serialization and found out it was because the serialization and deserialization of the class didnt include the configured IteratorSettings.  This likely isnt a big problem for normal users as when no IteratorSettings are configured on the RangeInputSplit it falls back to pulling from the Configuration but its possible with non-standard uses of mapreduce that information could be missing in the Configuration that the mappers receive and would subsequently error.", "A_clean_title": ["rangeinputsplit", "rang", "input", "split", "writabl", "method", "dont", "serial", "iteratorsettingswa", "iter", "set", "wa", "tri", "figur", "out", "whi", "some", "inform", "wa", "get", "lost", "rangeinputsplit", "rang", "input", "split", "after", "serial", "found", "out", "it", "wa", "becaus", "serial", "deseri", "class", "didnt", "includ", "configur", "iteratorset", "iter", "set", "thi", "like", "isnt", "big", "problem", "normal", "user", "as", "when", "no", "iteratorset", "iter", "set", "are", "configur", "rangeinputsplit", "rang", "input", "split", "it", "fall", "back", "pull", "configur", "but", "it", "possibl", "non", "standard", "use", "mapreduc", "that", "inform", "could", "miss", "configur", "that", "mapper", "receiv", "would", "subsequ", "error"], "B_title": "(De)serialize IteratorSettings on RangeInputSplit(s).", "B_clean_title": ["de", "serial", "iteratorset", "iter", "set", "rangeinputsplit", "rang", "input", "split"]},
{"A_title": "DocumentNodeStore does not make use of References while serializing BlobThe BlobSerializer in DocumentNodeStore does not make use of Blob references which results in copying the blobs by value hence significantly slowing down any migration", "A_clean_title": ["documentnodestor", "document", "node", "store", "not", "make", "use", "refer", "while", "serial", "blobth", "blob", "blobseri", "blob", "serial", "documentnodestor", "document", "node", "store", "not", "make", "use", "blob", "refer", "which", "result", "copi", "blob", "by", "valu", "henc", "significantli", "slow", "down", "ani", "migrat"], "B_title": "- DocumentNodeStore does not make use of References while serializing Blob", "B_clean_title": ["documentnodestor", "document", "node", "store", "not", "make", "use", "refer", "while", "serial", "blob"]},
{"A_title": "INullAcceptingValidator behavior seems broken in 1.5-RC4.2As discussed in this forum thread: http://apache-wicket.1842946.n4.nabble.com/INullAcceptingValidator-behavior-tp3570352p3570352.html  It appears that Wicket no longer calls INullAcceptingValidator intances when the validatable value is null.  Wicket wraps validators  as behaviors using the adapter pattern. The adapter class (org.apache.wicket.validation.ValidatorAdapter) implements  the interface IValidator<T>. This hides the case where the actual validator is an INullAcceptingValidator. Therefore when going through a components attached validators the code of org.apache.wicket.markup.html.form.FormComponent will never call INullAcceptingValidators when the value is null.", "A_clean_title": ["inullacceptingvalid", "null", "accept", "valid", "behavior", "seem", "broken", "rc4", "2a", "discuss", "thi", "forum", "thread", "http", "behavior", "apach", "wicket", "1842946", "n4", "nabbl", "tp3570352p3570352", "html", "com", "inullacceptingvalid", "null", "accept", "valid", "it", "appear", "that", "wicket", "no", "longer", "call", "inullacceptingvalid", "null", "accept", "valid", "intanc", "when", "validat", "valu", "null", "wicket", "wrap", "valid", "as", "behavior", "adapt", "pattern", "adapt", "class", "org", "apach", "wicket", "valid", "validatoradapt", "valid", "adapt", "implement", "interfac", "ivalid", "valid", "thi", "hide", "case", "where", "actual", "valid", "inullacceptingvalid", "null", "accept", "valid", "therefor", "when", "go", "through", "compon", "attach", "valid", "code", "org", "apach", "wicket", "markup", "html", "form", "formcompon", "form", "compon", "will", "never", "call", "inullacceptingvalid", "null", "accept", "valid", "when", "valu", "null"], "B_title": "Issue: WICKET-3767", "B_clean_title": ["issu", "wicket", "3767"]},
{"A_title": "Wrong serializer causing JsonMappingExceptionIm using spring-data-rest (3.0.0) which uses jackson-databind 2.9.0.pr2.  Im not sure what have changed since not so long ago I had an functional application. But now Im getting:  Could not write JSON document: java.lang.Double cannot be cast to java.lang.Integer (through reference chain: org.springframework.data.rest.webmvc.json.PersistentEntityJackson2Module$PersistentEntityResourceSerializer$1content->**.ContratostorageUtilizado); nested exception is com.fasterxml.jackson.databind.JsonMappingException: java.lang.Double cannot be cast to java.lang.Integer (through reference chain: org.springframework.data.rest.webmvc.json.PersistentEntityJackson2Module$PersistentEntityResourceSerializer$1content->**.ContratostorageUtilizado) Ive been stucked with this problem for a while and Im just assuming that there is something wrong when de Serializer for this specific field is defined   I need at least some directions...", "A_clean_title": ["wrong", "serial", "caus", "jsonmappingexceptionim", "json", "map", "except", "im", "spring", "data", "rest", "which", "use", "jackson", "databind", "pr2", "im", "not", "sure", "what", "have", "chang", "sinc", "not", "so", "long", "ago", "had", "function", "applic", "but", "now", "im", "get", "could", "not", "write", "json", "document", "java", "lang", "doubl", "not", "cast", "java", "lang", "integ", "through", "refer", "chain", "org", "springframework", "data", "rest", "webmvc", "json", "persistententityjackson2modul", "persist", "entiti", "jackson2modul", "persistententityresourceseri", "persist", "entiti", "resourc", "serial", "1content", "contratostorageutilizado", "contratostorag", "utilizado", "nest", "except", "com", "fasterxml", "jackson", "databind", "jsonmappingexcept", "json", "map", "except", "java", "lang", "doubl", "not", "cast", "java", "lang", "integ", "through", "refer", "chain", "org", "springframework", "data", "rest", "webmvc", "json", "persistententityjackson2modul", "persist", "entiti", "jackson2modul", "persistententityresourceseri", "persist", "entiti", "resourc", "serial", "1content", "contratostorageutilizado", "contratostorag", "utilizado", "ive", "been", "stuck", "thi", "problem", "while", "im", "just", "assum", "that", "there", "someth", "wrong", "when", "de", "serial", "thi", "specif", "field", "defin", "need", "at", "least", "some", "direct"], "B_title": "Improve error handling wrt #1612; add a test to verify exception being thrown", "B_clean_title": ["improv", "error", "handl", "wrt", "1612", "add", "test", "verifi", "except", "be", "thrown"]},
{"A_title": "NodeBuilder.reset might lead to inconsistent builderThe following test fails: code NodeBuilder root = new MemoryNodeBuilder(BASE); NodeBuilder x = root.child(x); NodeBuilder y = x.child(y);  root.reset(BASE); assertTrue(root.hasChildNode(x)); assertFalse(x.hasChildNode(y));  // fails code", "A_clean_title": ["nodebuild", "reset", "node", "builder", "might", "lead", "inconsist", "builderth", "builder", "follow", "test", "fail", "code", "nodebuild", "node", "builder", "root", "new", "memorynodebuild", "memori", "node", "builder", "base", "nodebuild", "node", "builder", "root", "child", "nodebuild", "node", "builder", "child", "root", "reset", "base", "asserttru", "assert", "true", "root", "haschildnod", "ha", "child", "node", "assertfals", "assert", "fals", "haschildnod", "ha", "child", "node", "fail", "code"], "B_title": "NodeBuilder.reset might lead to inconsistent builder", "B_clean_title": ["nodebuild", "reset", "node", "builder", "might", "lead", "inconsist", "builder"]},
{"A_title": "Method getResult() in MultiStartUnivariateRealOptimizerIn MultiStartUnivariateRealOptimizer (package optimization) the method getResult returns the result of the last run of the underlying optimizer; this last result might not be the best one in which case it will not correspond to the value returned by the optimize method. This is confusing and does not seem very useful. I think that getResult should be defined as code  public double getResult()      return optima0;  code and similarly code public double getFunctionValue()      return optimaValues0;  code", "A_clean_title": ["method", "getresult", "get", "result", "multistartunivariaterealoptimizerin", "multi", "start", "univari", "real", "optim", "multistartunivariaterealoptim", "multi", "start", "univari", "real", "optim", "packag", "optim", "method", "getresult", "get", "result", "return", "result", "last", "run", "underli", "optim", "thi", "last", "result", "might", "not", "best", "one", "which", "case", "it", "will", "not", "correspond", "valu", "return", "by", "optim", "method", "thi", "confus", "not", "seem", "veri", "use", "think", "that", "getresult", "get", "result", "defin", "as", "code", "public", "doubl", "getresult", "get", "result", "return", "optima0", "code", "similarli", "code", "public", "doubl", "getfunctionvalu", "get", "function", "valu", "return", "optimavalues0", "optima", "values0", "code"], "B_title": "Fixed inconsistent definition of getResult. Modified associated test accordingly.", "B_clean_title": ["fix", "inconsist", "definit", "getresult", "get", "result", "modifi", "associ", "test", "accordingli"]},
{"A_title": "AssertionError thrown for Lucene index with empty suggest disctionaryCreate an index where one field is enabled for suggestion but no content is indexed for that index i.e. no matching content. Then while performing any query following exception is thrown  noformat java.lang.AssertionError at org.apache.lucene.search.suggest.analyzing.AnalyzingInfixSuggester.<init>(AnalyzingInfixSuggester.java:167) at org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper 2.<init>(SuggestHelper.java:127) at org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper.getLookup(SuggestHelper.java:127) at org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper.getLookup(SuggestHelper.java:123) at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.<init>(IndexNode.java:109) at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.open(IndexNode.java:69) at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.findIndexNode(IndexTracker.java:162) at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.acquireIndexNode(IndexTracker.java:137) at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getPlans(LucenePropertyIndex.java:249) at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:1016) at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:949) at org.apache.jackrabbit.oak.query.ast.SelectorImpl.prepare(SelectorImpl.java:288) noformat  This happens with -ea flag i.e. java assertions enabled. It caused here|https://github.com/apache/lucene-solr/blob/releases/lucene-solr/4.7.1/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L167", "A_clean_title": ["assertionerror", "assert", "error", "thrown", "lucen", "index", "empti", "suggest", "disctionarycr", "disctionari", "creat", "index", "where", "one", "field", "enabl", "suggest", "but", "no", "content", "index", "that", "index", "no", "match", "content", "then", "while", "perform", "ani", "queri", "follow", "except", "thrown", "noformat", "java", "lang", "assertionerror", "assert", "error", "at", "org", "apach", "lucen", "search", "suggest", "analyz", "analyzinginfixsuggest", "analyz", "infix", "suggest", "init", "analyzinginfixsuggest", "java:167", "analyz", "infix", "suggest", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "util", "suggesthelp", "suggest", "helper", "init", "suggesthelp", "java:127", "suggest", "helper", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "util", "suggesthelp", "getlookup", "suggest", "helper", "get", "lookup", "suggesthelp", "java:127", "suggest", "helper", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "util", "suggesthelp", "getlookup", "suggest", "helper", "get", "lookup", "suggesthelp", "java:123", "suggest", "helper", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "indexnod", "index", "node", "init", "indexnod", "java:109", "index", "node", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "indexnod", "open", "index", "node", "indexnod", "java:69", "index", "node", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "indextrack", "findindexnod", "index", "tracker", "find", "index", "node", "indextrack", "java:162", "index", "tracker", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "indextrack", "acquireindexnod", "index", "tracker", "acquir", "index", "node", "indextrack", "java:137", "index", "tracker", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "lucenepropertyindex", "getplan", "lucen", "properti", "index", "get", "plan", "lucenepropertyindex", "java:249", "lucen", "properti", "index", "at", "org", "apach", "jackrabbit", "oak", "queri", "queryimpl", "getbestselectorexecutionplan", "queri", "impl", "get", "best", "selector", "execut", "plan", "queryimpl", "java:1016", "queri", "impl", "at", "org", "apach", "jackrabbit", "oak", "queri", "queryimpl", "getbestselectorexecutionplan", "queri", "impl", "get", "best", "selector", "execut", "plan", "queryimpl", "java:949", "queri", "impl", "at", "org", "apach", "jackrabbit", "oak", "queri", "ast", "selectorimpl", "prepar", "selector", "impl", "selectorimpl", "java:288", "selector", "impl", "noformat", "thi", "happen", "ea", "flag", "java", "assert", "enabl", "it", "caus", "here|http", "solr", "blob", "releas", "lucen", "java", "github", "com", "apach", "lucen", "solr", "lucen", "suggest", "src", "java", "org", "apach", "lucen", "search", "suggest", "analyz", "analyzinginfixsuggest", "analyz", "infix", "suggest", "l167"], "B_title": "- avoid building suggester with existing directory when no content is indexed", "B_clean_title": ["avoid", "build", "suggest", "exist", "directori", "when", "no", "content", "index"]},
{"A_title": "Bugs in BrentOptimizerI apologize for having provided a buggy implementation of Brents optimization algorithm (class BrentOptimizer in package optimization.univariate). The unit tests didnt show that there was something wrong although (from the changes.xml file) I discovered that at the time Luc had noticed something weird in the implementations behaviour. Comparing with an implementation in Python I could figure out the fixes. Ill modify BrentOptimizer and add a test. I also propose to change the name of the unit test class from BrentMinimizerTest to BrentOptimizerTest.", "A_clean_title": ["bug", "brentoptimizeri", "brent", "optim", "apolog", "have", "provid", "buggi", "implement", "brent", "optim", "algorithm", "class", "brentoptim", "brent", "optim", "packag", "optim", "univari", "unit", "test", "didnt", "show", "that", "there", "wa", "someth", "wrong", "although", "chang", "xml", "file", "discov", "that", "at", "time", "luc", "had", "notic", "someth", "weird", "implement", "behaviour", "compar", "implement", "python", "could", "figur", "out", "fix", "ill", "modifi", "brentoptim", "brent", "optim", "add", "test", "also", "propos", "chang", "name", "unit", "test", "class", "brentminimizertest", "brent", "minim", "test", "brentoptimizertest", "brent", "optim", "test"], "B_title": "Another bug uncovered; all things being equal the code now behaves like the Puthon implementation. MATH-397: Modified BrentOptimizer following the changes in AbstractUnivariateRealOptimizer.", "B_clean_title": ["anoth", "bug", "uncov", "all", "thing", "be", "equal", "code", "now", "behav", "like", "puthon", "implement", "math", "397", "modifi", "brentoptim", "brent", "optim", "follow", "chang", "abstractunivariaterealoptim", "abstract", "univari", "real", "optim"]},
{"A_title": "Prevent partitioning pushdown unless partitions fields match exactlyConsider an operation grouped on fields (A B) followed by an operation grouped on field (A).  Right now the optimizer can push down the partitioning on (A) which serves both operations (the first step locally still groups by A and B). This may however by a bad idea for the cases where the field A has a low cardinality or the value distribution is skewed.  Since we cannot determine that robustly yet I suggest to disable this optimization for now.", "A_clean_title": ["prevent", "partit", "pushdown", "unless", "partit", "field", "match", "exactlyconsid", "exactli", "consid", "oper", "group", "field", "follow", "by", "oper", "group", "field", "right", "now", "optim", "push", "down", "partit", "which", "serv", "both", "oper", "first", "step", "local", "still", "group", "by", "thi", "may", "howev", "by", "bad", "idea", "case", "where", "field", "ha", "low", "cardin", "or", "valu", "distribut", "skew", "sinc", "we", "not", "determin", "that", "robustli", "yet", "suggest", "disabl", "thi", "optim", "now"], "B_title": "Prevent partitionings on subsets of fields from being pushed down", "B_clean_title": ["prevent", "partit", "subset", "field", "be", "push", "down"]},
{"A_title": "Always create new UUID on ImportBehavior.IMPORT_UUID_CREATE_NEWThe implementation should create a new UUID for each referenceable node even if there is no existing node with that UUID. This spec says:  bq.  Incoming nodes are assigned newly created identifiers upon addition to the workspace. As a result identifier collisions never occur.  This will break backward compatibility but is IMO the correct behavior and the only way to guarantee import of referenceable nodes does not fail in a concurrent import scenario. See OAK-1186 for more details.", "A_clean_title": ["alway", "creat", "new", "uuid", "importbehavior", "import", "behavior", "import", "uuid", "creat", "newth", "new", "implement", "creat", "new", "uuid", "each", "referenc", "node", "even", "there", "no", "exist", "node", "that", "uuid", "thi", "spec", "say", "bq", "incom", "node", "are", "assign", "newli", "creat", "identifi", "upon", "addit", "workspac", "as", "result", "identifi", "collis", "never", "occur", "thi", "will", "break", "backward", "compat", "but", "imo", "correct", "behavior", "onli", "way", "guarante", "import", "referenc", "node", "not", "fail", "concurr", "import", "scenario", "see", "oak", "1186", "more", "detail"], "B_title": "Always create new UUID on ImportBehavior.IMPORT_UUID_CREATE_NEW", "B_clean_title": ["alway", "creat", "new", "uuid", "importbehavior", "import", "behavior", "import", "uuid", "creat", "new"]},
{"A_title": "Consolidate ZK code WRT retriesA couple of general ZK things that should be fixed up:  # Multiple means of automatic retrying of recoverable ZooKeeper errors through use of an InvocationHandler and a Proxy around IZooReader(Writer) # Encapsulate retry logic # Switch over callers to use the retrying instance instead of the non-retrying instance", "A_clean_title": ["consolid", "zk", "code", "wrt", "retriesa", "retri", "coupl", "gener", "zk", "thing", "that", "fix", "up", "multipl", "mean", "automat", "retri", "recover", "zookeep", "zoo", "keeper", "error", "through", "use", "invocationhandl", "invoc", "handler", "proxi", "around", "izooread", "zoo", "reader", "writer", "encapsul", "retri", "logic", "switch", "over", "caller", "use", "retri", "instanc", "instead", "non", "retri", "instanc"], "B_title": "Spelling error missing hashCode/toString/equals on ZKConnectionInfo and some extra braces.", "B_clean_title": ["spell", "error", "miss", "hashcod", "tostr", "equal", "hash", "code", "string", "zkconnectioninfo", "zk", "connect", "info", "some", "extra", "brace"]},
{"A_title": "LongConverter converts some values greater than Long.MAX_VALUECurrently its possible to submit some values via Long Textfield<Long> that are greater than Long.MAX_VALUE. This will produce converted input and model update with value of Long.MAX_VALUE  Im not sure what the behavior should be - imho throwing ConversionException seems fair as the input isnt a valid Long.  The reason seems to be precision loss during Double.valueOf(input) execution while converting and then comparing to Long.MAX_VALUE  using Long.doubleValue() in *AbstractNumberConverter* which by casting leads to to the same precision loss and the numbers are seemingly equal during comparison of ranges.  Maybe using BigDecimals for parsing could help here.  The quickstart is available at https://github.com/zeratul021/wicket-number-conversion.  For the fastest demonstration I extended Wickets _longConversion()_ test-case in *ConvertersTest*: https://github.com/zeratul021/wicket-number-conversion/blob/master/src/test/java/com/github/zeratul021/wicketnumberconversion/ConvertersTest.java#L300", "A_clean_title": ["longconvert", "long", "convert", "convert", "some", "valu", "greater", "than", "long", "max", "valuecurr", "valu", "current", "it", "possibl", "submit", "some", "valu", "via", "long", "textfield", "long", "that", "are", "greater", "than", "long", "max", "valu", "thi", "will", "produc", "convert", "input", "model", "updat", "valu", "long", "max", "valu", "im", "not", "sure", "what", "behavior", "imho", "throw", "conversionexcept", "convers", "except", "seem", "fair", "as", "input", "isnt", "valid", "long", "reason", "seem", "precis", "loss", "dure", "doubl", "valueof", "valu", "input", "execut", "while", "convert", "then", "compar", "long", "max", "valu", "long", "doublevalu", "doubl", "valu", "abstractnumberconvert", "abstract", "number", "convert", "which", "by", "cast", "lead", "same", "precis", "loss", "number", "are", "seemingli", "equal", "dure", "comparison", "rang", "mayb", "bigdecim", "big", "decim", "pars", "could", "help", "here", "quickstart", "avail", "at", "http", "number", "convers", "github", "com", "zeratul021", "wicket", "fastest", "demonstr", "extend", "wicket", "longconvers", "long", "convers", "test", "case", "converterstest", "convert", "test", "http", "number", "java", "github", "com", "zeratul021", "wicket", "convers", "blob", "master", "src", "test", "java", "com", "github", "zeratul021", "wicketnumberconvers", "converterstest", "convert", "test", "l300"], "B_title": "and WICKET-5861: number converters are based on BigDecimals now", "B_clean_title": ["wicket", "5861", "number", "convert", "are", "base", "bigdecim", "big", "decim", "now"]},
{"A_title": "Deep stubbing with generic responses in the call chain is not workingDeep stubbing will throw an Exception if multiple generics occur in the call chain. For instance consider having a mock myMock1 that provides a function that returns a generic T. If T also has a function that returns a generic an Exception with the message Raw extraction not supported for : null will be thrown. I think the issue is that further generics are not possible to be mocked by ReturnsDeepStubsSerializationFallback since the GenericMetadataSupport is closed at this point.", "A_clean_title": ["deep", "stub", "gener", "respons", "call", "chain", "not", "workingdeep", "work", "deep", "stub", "will", "throw", "except", "multipl", "gener", "occur", "call", "chain", "instanc", "consid", "have", "mock", "mymock1", "my", "mock1", "that", "provid", "function", "that", "return", "gener", "also", "ha", "function", "that", "return", "gener", "except", "messag", "raw", "extract", "not", "support", "null", "will", "thrown", "think", "issu", "that", "further", "gener", "are", "not", "possibl", "mock", "by", "returnsdeepstubsserializationfallback", "return", "deep", "stub", "serial", "fallback", "sinc", "genericmetadatasupport", "gener", "metadata", "support", "close", "at", "thi", "point"], "B_title": "Merge branch pbielicki-bug#128", "B_clean_title": ["merg", "branch", "pbielicki", "bug", "128"]},
{"A_title": "weird object literal invalid property error on unrelated object prototypeNone", "A_clean_title": ["weird", "object", "liter", "invalid", "properti", "error", "unrel", "object", "prototypenon", "prototyp", "none"], "B_title": "fix a bug in constraint-matching fixes issue 700", "B_clean_title": ["fix", "bug", "constraint", "match", "fix", "issu", "700"]},
{"A_title": "StringUtils equals() relies on undefined behaviorSince the java.lang.CharSequence class was first introduced in 1.4 the JavaDoc block has contained the following note:  This interface does not refine the general contracts of the equals and hashCode methods. The result of comparing two objects that implement CharSequence is therefore in general undefined. Each object may be implemented by a different class and there is no guarantee that each class will be capable of testing its instances for equality with those of the other. When the signature of the StringUtils equals() method was changed from equals(String String) to equals(CharSequence CharSequence) in R920543 the implementation still relied on calling CharSequence#equals(Object) even though in general the result is undefined. One example where equals(Object) returns false even though as CharSequences two objects represent equal sequences is when one object is an instance of javax.lang.model.element.Name and the other object is a String.", "A_clean_title": ["stringutil", "string", "util", "equal", "reli", "undefin", "behaviorsinc", "behavior", "sinc", "java", "lang", "charsequ", "char", "sequenc", "class", "wa", "first", "introduc", "javadoc", "java", "doc", "block", "ha", "contain", "follow", "note", "thi", "interfac", "not", "refin", "gener", "contract", "equal", "hashcod", "hash", "code", "method", "result", "compar", "two", "object", "that", "implement", "charsequ", "char", "sequenc", "therefor", "gener", "undefin", "each", "object", "may", "implement", "by", "differ", "class", "there", "no", "guarante", "that", "each", "class", "will", "capabl", "test", "it", "instanc", "equal", "those", "other", "when", "signatur", "stringutil", "string", "util", "equal", "method", "wa", "chang", "equal", "string", "string", "equal", "charsequ", "char", "sequenc", "charsequ", "char", "sequenc", "r920543", "implement", "still", "reli", "call", "charsequ", "char", "sequenc", "equal", "object", "even", "though", "gener", "result", "undefin", "one", "exampl", "where", "equal", "object", "return", "fals", "even", "though", "as", "charsequ", "char", "sequenc", "two", "object", "repres", "equal", "sequenc", "when", "one", "object", "instanc", "javax", "lang", "model", "element", "name", "other", "object", "string"], "B_title": "StringUtils equals() relies on undefined behavior; thanks to Daniel Trebbien", "B_clean_title": ["stringutil", "string", "util", "equal", "reli", "undefin", "behavior", "thank", "daniel", "trebbien"]},
{"A_title": "Moving or deleting tree instances with status NEW doesnt change its status to DISCONNECTEDFurther fall out from OAK-606:  code         Tree t = tree.addChild(new);          root.move(/x /y/x);         assertEquals(Status.DISCONNECTED t.getStatus()); code  The assertion fails.", "A_clean_title": ["move", "or", "delet", "tree", "instanc", "statu", "new", "doesnt", "chang", "it", "statu", "disconnectedfurth", "disconnect", "further", "fall", "out", "oak", "606", "code", "tree", "tree", "addchild", "add", "child", "new", "root", "move", "assertequ", "assert", "equal", "statu", "disconnect", "getstatu", "get", "statu", "code", "assert", "fail"], "B_title": "Moving or deleting tree instances with status NEW doesnt change its status to DISCONNECTED", "B_clean_title": ["move", "or", "delet", "tree", "instanc", "statu", "new", "doesnt", "chang", "it", "statu", "disconnect"]},
{"A_title": "ZooKeeperInstance only uses first ZooKeeper in list of quorumHad tests running which had a quorum of 3 ZooKeeper servers. One appears to have died and the test was then unable to connect to the Accumulo shell hanging on trying to connect to ZooKeeper.  There was no client.conf file present so a ClientConfiguration was constructed from accumulo-site.xml.  code this.zooKeepers = clientConf.get(ClientProperty.INSTANCE_ZK_HOST); code  When the commons configuration AbstractConfiguration class is used with the get() method only the first element in the value is returned as the implementation treats the other items as a list because of the default separator of a comma.  Its easily reproduced with the following:  code     ZooKeeperInstance inst = new ZooKeeperInstance(accumulo localhost127.0.0.1);     System.out.println(inst.getZooKeepers()); code  The above will print  noformat localhost noformat  instead of the expected  noformat localhost127.0.0.1 noformat", "A_clean_title": ["zookeeperinst", "zoo", "keeper", "instanc", "onli", "use", "first", "zookeep", "zoo", "keeper", "list", "quorumhad", "quorum", "had", "test", "run", "which", "had", "quorum", "zookeep", "zoo", "keeper", "server", "one", "appear", "have", "die", "test", "wa", "then", "unabl", "connect", "accumulo", "shell", "hang", "tri", "connect", "zookeep", "zoo", "keeper", "there", "wa", "no", "client", "conf", "file", "present", "so", "clientconfigur", "client", "configur", "wa", "construct", "accumulo", "site", "xml", "code", "thi", "zookeep", "zoo", "keeper", "clientconf", "get", "client", "conf", "clientproperti", "client", "properti", "instanc", "zk", "host", "code", "when", "common", "configur", "abstractconfigur", "abstract", "configur", "class", "use", "get", "method", "onli", "first", "element", "valu", "return", "as", "implement", "treat", "other", "item", "as", "list", "becaus", "default", "separ", "comma", "it", "easili", "reproduc", "follow", "code", "zookeeperinst", "zoo", "keeper", "instanc", "inst", "new", "zookeeperinst", "zoo", "keeper", "instanc", "accumulo", "localhost127", "system", "out", "println", "inst", "getzookeep", "get", "zoo", "keeper", "code", "abov", "will", "print", "noformat", "localhost", "noformat", "instead", "expect", "noformat", "localhost127", "noformat"], "B_title": "Set the list separator to 0 to disable list interpretation", "B_clean_title": ["set", "list", "separ", "disabl", "list", "interpret"]},
{"A_title": "Stack overflow in Beta.regularizedBetaIn org.apache.commons.math3.special.Beta.regularizedBeta(doubledoubledoubledoubleint) the case    else if (x > (a + 1.0) / (a + b + 2.0))        ret = 1.0 - regularizedBeta(1.0 - x b a epsilon maxIterations);    is prone to infinite recursion: If x is approximately the tested value then 1-x is approximately the tested value in the recursion. Thus due to loss of precision after the subtraction this condition can be true for the recursive call as well.  Example: double x= Double.longBitsToDouble(4597303555101269224L); double a= Double.longBitsToDouble(4634227472812299606L); double b = Double.longBitsToDouble(4642050131540049920L); System.out.println(x > (a + 1.0) / (a + b + 2.0)); System.out.println(1-x>(b + 1.0) / (b + a + 2.0)); System.out.println(1-(1-x)>(a + 1.0) / (a + b + 2.0));  Possible solution: change the condition to x > (a + 1.0) / (a + b + 2.0) && 1-x<=(b + 1.0) / (b + a + 2.0)", "A_clean_title": ["stack", "overflow", "beta", "regularizedbetain", "regular", "beta", "org", "apach", "common", "math3", "special", "beta", "regularizedbeta", "regular", "beta", "doubledoubledoubledoubleint", "case", "ret", "regularizedbeta", "regular", "beta", "epsilon", "maxiter", "max", "iter", "prone", "infinit", "recurs", "approxim", "test", "valu", "then", "approxim", "test", "valu", "recurs", "thu", "due", "loss", "precis", "after", "subtract", "thi", "condit", "true", "recurs", "call", "as", "well", "exampl", "doubl", "x=", "doubl", "longbitstodoubl", "long", "bit", "doubl", "4597303555101269224l", "doubl", "a=", "doubl", "longbitstodoubl", "long", "bit", "doubl", "4634227472812299606l", "doubl", "doubl", "longbitstodoubl", "long", "bit", "doubl", "4642050131540049920l", "system", "out", "println", "system", "out", "println", "system", "out", "println", "possibl", "solut", "chang", "condit"], "B_title": "Avoid infinite recursion. Thanks to Florian Erhard.", "B_clean_title": ["avoid", "infinit", "recurs", "thank", "florian", "erhard"]},
{"A_title": "NPE on DefaultJavaPrettyPrinter#printCtFieldAccessMore intel about bug reproduction can be found on the following issue:  HabchiSarra/SmellDetector#8 The full stack trace using spoon 5.9.0-SNAPSHOT below", "A_clean_title": ["npe", "defaultjavaprettyprint", "default", "java", "pretti", "printer", "printctfieldaccessmor", "print", "ct", "field", "access", "more", "intel", "about", "bug", "reproduct", "found", "follow", "issu", "habchisarra", "smelldetector", "habchi", "sarra", "smell", "detector", "full", "stack", "trace", "spoon", "snapshot", "below"], "B_title": "fix: fix NPE in noclasspath mode (#1502)  Fix #1501", "B_clean_title": ["fix", "fix", "npe", "noclasspath", "mode", "1502", "fix", "1501"]},
{"A_title": "Line.revert() is impreciseLine.revert() only maintains ~10 digits for the direction. This becomes an issue when the lines position is evaluated far from the origin. A simple fix would be to use Vector3D.negate() for the direction.  Also is there a reason why Line is not immutable? It is just comprised of two vectors.", "A_clean_title": ["line", "revert", "impreciselin", "revert", "imprecis", "line", "onli", "maintain", "~10", "digit", "direct", "thi", "becom", "issu", "when", "line", "posit", "evalu", "far", "origin", "simpl", "fix", "would", "use", "vector3d", "negat", "direct", "also", "there", "reason", "whi", "line", "not", "immut", "it", "just", "compris", "two", "vector"], "B_title": "Fixed accuracy of 3D Line.revert().", "B_clean_title": ["fix", "accuraci", "3d", "line", "revert"]},
{"A_title": "Write operations on Property do not check checked-out state of NodeWrite operations on Property do not check the checked-out state. The same is true for Node.setProperty(... null).", "A_clean_title": ["write", "oper", "properti", "not", "check", "check", "out", "state", "nodewrit", "node", "write", "oper", "properti", "not", "check", "check", "out", "state", "same", "true", "node", "setproperti", "set", "properti", "null"], "B_title": "consistently check the checked-out state of a Node for property modifications", "B_clean_title": ["consist", "check", "check", "out", "state", "node", "properti", "modif"]},
{"A_title": "Source files should not be put in binary JARSource files (*.java) should not be put into binary mockito-core.jar. It stupefies Idea to show decompiled file even when source jar is available.", "A_clean_title": ["sourc", "file", "not", "put", "binari", "jarsourc", "jar", "sourc", "file", "java", "not", "put", "into", "binari", "mockito", "core", "jar", "it", "stupefi", "idea", "show", "decompil", "file", "even", "when", "sourc", "jar", "avail"], "B_title": "Fixed issue 157 In order to avoid ArrayIndexOutOfBoundsException when anyvarag() matcher in use", "B_clean_title": ["fix", "issu", "157", "order", "avoid", "arrayindexoutofboundsexcept", "array", "index", "out", "bound", "except", "when", "anyvarag", "matcher", "use"]},
{"A_title": "Trying to remove a missing property throws PathNotFoundExceptionThe following code snippet throws a PathNotFoundException if the missing property is not present.  code:java node.setProperty(missing (String) null); code  A better way to handle such a case would be for the above statement to simply do nothing.", "A_clean_title": ["tri", "remov", "miss", "properti", "throw", "pathnotfoundexceptionth", "path", "not", "found", "except", "follow", "code", "snippet", "throw", "pathnotfoundexcept", "path", "not", "found", "except", "miss", "properti", "not", "present", "code", "java", "node", "setproperti", "set", "properti", "miss", "string", "null", "code", "better", "way", "handl", "such", "case", "would", "abov", "statement", "simpli", "noth"], "B_title": "Trying to remove a missing property throws PathNotFoundException", "B_clean_title": ["tri", "remov", "miss", "properti", "throw", "pathnotfoundexcept", "path", "not", "found", "except"]},
{"A_title": "Resource bundles are not resolved on PriorityHeaderItemsIf a bundle X provides resource A and resource A is rendered as priority header item the resource A is rendered not bundle X.", "A_clean_title": ["resourc", "bundl", "are", "not", "resolv", "priorityheaderitemsif", "prioriti", "header", "item", "bundl", "provid", "resourc", "resourc", "render", "as", "prioriti", "header", "item", "resourc", "render", "not", "bundl"], "B_title": "unwrap wrapped header items when resolving bundles", "B_clean_title": ["unwrap", "wrap", "header", "item", "when", "resolv", "bundl"]},
{"A_title": "CachingResourceLocator lookup key doesnt take strict into accountCachingResourceLocator uses a CacheKey to store lookups for resources.   With e.g.  - b_nl.js - b.js  When e.g. a strict resource lookup for b.js with locale us is performed it will store the not-found for locale us under the cache key. The cache key consists of resource name locale style and variation. However when you search non-strict for locale us the resource locator should find the non-localized resource b.js but since a matching key for the lookup was stored for this particular resource it will fail.", "A_clean_title": ["cachingresourceloc", "cach", "resourc", "locat", "lookup", "key", "doesnt", "take", "strict", "into", "accountcachingresourceloc", "account", "cach", "resourc", "locat", "use", "cachekey", "cach", "key", "store", "lookup", "resourc", "js", "nl", "js", "when", "strict", "resourc", "lookup", "js", "local", "us", "perform", "it", "will", "store", "not", "found", "local", "us", "under", "cach", "key", "cach", "key", "consist", "resourc", "name", "local", "style", "variat", "howev", "when", "you", "search", "non", "strict", "local", "us", "resourc", "locat", "find", "non", "local", "resourc", "js", "but", "sinc", "match", "key", "lookup", "wa", "store", "thi", "particular", "resourc", "it", "will", "fail"], "B_title": "CachingResourceLocator lookup key doesnt take strict into account", "B_clean_title": ["cachingresourceloc", "cach", "resourc", "locat", "lookup", "key", "doesnt", "take", "strict", "into", "account"]},
{"A_title": "Precision.round() returns different results when provided negative zero as double or floatPrecision.round(-0.0d x) = 0.0 Precision.round(-0.0f x) = -0.0  After discussion on the mailinglist the result should always be -0.0.", "A_clean_title": ["precis", "round", "return", "differ", "result", "when", "provid", "neg", "zero", "as", "doubl", "or", "floatprecis", "round", "float", "precis", "0d", "precis", "round", "0f", "after", "discuss", "mailinglist", "result", "alway"], "B_title": "Precision.round(double ...) will return negative zero for negative values rounded to zero.", "B_clean_title": ["precis", "round", "doubl", "will", "return", "neg", "zero", "neg", "valu", "round", "zero"]},
{"A_title": "Diff reads too many nodesDocumentNodeStore.diffManyChildren() may read too many nodes when there is an inactive cluster node with an old _lastRev on the root document. This is a regression introduced with the fix for OAK-2232.  The fix assumes an inactive cluster node does not have a revision range with an old revision seen at a current timestamp. The DocumentNodeStore will in fact purge revisions from the range in the RevisionComparator after an hour. But on startup the first background read may populate the RevisionComparator with a revision which is potentially very old (e.g. if the clusterId is not used anymore).", "A_clean_title": ["diff", "read", "too", "mani", "nodesdocumentnodestor", "diffmanychildren", "node", "document", "node", "store", "diff", "mani", "children", "may", "read", "too", "mani", "node", "when", "there", "inact", "cluster", "node", "old", "lastrev", "last", "rev", "root", "document", "thi", "regress", "introduc", "fix", "oak", "2232", "fix", "assum", "inact", "cluster", "node", "not", "have", "revis", "rang", "old", "revis", "seen", "at", "current", "timestamp", "documentnodestor", "document", "node", "store", "will", "fact", "purg", "revis", "rang", "revisioncompar", "revis", "compar", "after", "hour", "but", "startup", "first", "background", "read", "may", "popul", "revisioncompar", "revis", "compar", "revis", "which", "potenti", "veri", "old", "clusterid", "cluster", "id", "not", "use", "anymor"], "B_title": "Diff reads too many nodes", "B_clean_title": ["diff", "read", "too", "mani", "node"]},
{"A_title": "Bulk random walk test failedThe bulk random walk test failed while running on a 10 node cluster w/ the following error message.  noformat 18 23:36:05167 bulk.Setup INFO : Starting bulk test on 459a04a0   19 00:24:33950 randomwalk.Framework ERROR: Error during random walk java.lang.Exception: Error running node Bulk.xml         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:253)         at org.apache.accumulo.server.test.randomwalk.Framework.run(Framework.java:61)         at org.apache.accumulo.server.test.randomwalk.Framework.main(Framework.java:114)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at org.apache.accumulo.start.Main 1.run(Main.java:89)         at java.lang.Thread.run(Thread.java:662) Caused by: java.lang.Exception: Error running node bulk.Verify         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:253)         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:249)         ... 8 more Caused by: java.lang.Exception: Bad key at r0d646 cf:000  1326932285943 false -1         at org.apache.accumulo.server.test.randomwalk.bulk.Verify.visit(Verify.java:51)         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:249)         ... 9 more noformat  Looking at the table the rows r0d646 r0edd9 and r0f056 r10467 all had -1 values.  There was a tablet that overlapped the first range of -1 rows exactly 268;r0edd9;r0d645.  This tablet had only the following activity on a tablet server and was then merged out of existence.  The merge operation was 268;r10eff;r093b1.  noformat 19 00:05:10966 tabletserver.Tablet DEBUG: Files for low split 268;r0edd9;r0d645  /b-0001azp/I0001azt.rf /b-0001azp/I0001azu.rf /t-0001ale/A0001an3.rf 19 00:05:10974 tabletserver.Tablet TABLET_HIST: 268;r0f055;r0d645 split 268;r0edd9;r0d645 268;r0f055;r0edd9 19 00:05:10975 tabletserver.Tablet TABLET_HIST: 268;r0edd9;r0d645 opened  19 00:05:15029 tabletserver.Tablet TABLET_HIST: 268;r0edd9;r0d645 import /b-0001azi/I0001azm.rf 17138 0 19 00:05:15103 tabletserver.Tablet DEBUG: Starting MajC 268;r0edd9;r0d645 /b-0001azi/I0001azm.rf /b-0001azp/I0001azt.rf /b-0001azp/I0001azu.rf /t-0001ale/A0001an3.rf --> /t-0001apj/A0001bri.rf_tmp 19 00:05:15339 tabletserver.Tablet TABLET_HIST: 268;r0edd9;r0d645 import /b-0001azx/I0001azy.rf 16620 0 19 00:05:15651 tabletserver.Compactor DEBUG: Compaction 268;r0edd9;r0d645 181080 read | 60360 written | 553761 entries/sec |  0.327 secs 19 00:05:15661 tabletserver.Tablet TABLET_HIST: 268;r0edd9;r0d645 MajC /b-0001azi/I0001azm.rf /b-0001azp/I0001azt.rf /b-0001azp/I0001azu.rf /t-0001ale/A0001an3.rf --> /t-0001apj/A0001bri.rf 19 00:05:30672 tabletserver.Tablet DEBUG: Starting MajC 268;r0edd9;r0d645 /b-0001azx/I0001azy.rf --> /t-0001apj/C0001brn.rf_tmp 19 00:05:30810 tabletserver.Compactor DEBUG: Compaction 268;r0edd9;r0d645 60360 read | 60360 written | 534159 entries/sec |  0.113 secs 19 00:05:30824 tabletserver.Tablet TABLET_HIST: 268;r0edd9;r0d645 MajC /b-0001azx/I0001azy.rf --> /t-0001apj/C0001brn.rf 19 00:05:30943 tabletserver.Tablet DEBUG: initiateClose(saveState=true queueMinC=false disableWrites=false) 268;r0edd9;r0d645 19 00:05:30943 tabletserver.Tablet DEBUG: completeClose(saveState=true completeClose=true) 268;r0edd9;r0d645 19 00:05:30947 tabletserver.Tablet TABLET_HIST: 268;r0edd9;r0d645 closed 19 00:05:30947 tabletserver.TabletServer DEBUG: Unassigning 268;r0edd9;r0d645@(nullxxx.xxx.xxx.xxx:9997134d7425fc59413null) 19 00:05:30949 tabletserver.TabletServer INFO : unloaded 268;r0edd9;r0d645 19 00:05:30949 tabletserver.TabletServer INFO : unloaded 268;r0edd9;r0d645  noformat   For the second range of -1 values r0f056 r10467 r0f056 corresponds to the split point r0f055.  Howerver there is no split point corresponding to r10467. All of the tablets w/ a split of r0f055 lived on one tablet server.    noformat 19 00:02:21262 tabletserver.Tablet TABLET_HIST: 268<;r0d645 split 268;r0f055;r0d645 268<;r0f055 19 00:02:21263 tabletserver.Tablet TABLET_HIST: 268;r0f055;r0d645 opened  19 00:02:21264 tabletserver.Tablet TABLET_HIST: 268<;r0f055 opened  19 00:02:44504 tabletserver.Tablet TABLET_HIST: 268<;r0f055 split 268;r11da6;r0f055 268<;r11da6 19 00:02:44505 tabletserver.Tablet TABLET_HIST: 268;r11da6;r0f055 opened  19 00:05:10974 tabletserver.Tablet TABLET_HIST: 268;r0f055;r0d645 split 268;r0edd9;r0d645 268;r0f055;r0edd9 19 00:05:10975 tabletserver.Tablet TABLET_HIST: 268;r0f055;r0edd9 opened  19 00:05:15023 tabletserver.Tablet TABLET_HIST: 268;r11da6;r0f055 split 268;r0f622;r0f055 268;r11da6;r0f622 19 00:05:15024 tabletserver.Tablet TABLET_HIST: 268;r0f622;r0f055 opened  noformat  All of the tablets mentioned so far were all merged away in the same merge operation making this operation a possible place were data loss occurred.  However I can not pinpoint the issue at this point in time.  Below is a little info about the merge from the master logs showing which tablets were involved in the merge.  noformat 19 00:05:30616 master.EventCoordinator INFO : Merge state of 268;r10eff;r093b1 set to WAITING_FOR_CHOPPED 19 00:05:30677 master.Master INFO : Asking xxx.xxx.xxx.xxx:9997134d7425fc5940c to chop 268;r09927;r0903a 19 00:05:30678 master.Master INFO : Asking xxx.xxx.xxx.xxx:9997134d7425fc5940c to chop 268;r0ca9e;r09927 19 00:05:30678 master.Master INFO : Asking xxx.xxx.xxx.xxx:9997134d7425fc5940a to chop 268;r0d2b5;r0ca9e 19 00:05:30678 master.Master INFO : Asking xxx.xxx.xxx.xxx:9997134d7425fc59412 to chop 268;r0d645;r0d2b5 19 00:05:30678 master.Master INFO : Asking xxx.xxx.xxx.xxx:9997134d7425fc59413 to chop 268;r0edd9;r0d645 19 00:05:30678 master.Master INFO : Asking xxx.xxx.xxx.xxx:9997134d7425fc59413 to chop 268;r0f055;r0edd9 19 00:05:30678 master.Master INFO : Asking xxx.xxx.xxx.xxx:9997134d7425fc59413 to chop 268;r0f622;r0f055 19 00:05:30678 master.Master INFO : Asking xxx.xxx.xxx.xxx:9997134d7425fc59413 to chop 268;r0f68b;r0f622 19 00:05:30678 master.Master INFO : Asking xxx.xxx.xxx.xxx:9997134d7425fc59413 to chop 268;r10c14;r0f68b 19 00:05:30678 master.Master INFO : Asking xxx.xxx.xxx.xxx:9997134d7425fc59413 to chop 268;r110f7;r10c14 noformat  When this test verifies its data and detects data loss there is no easy way to determine at what time the data loss occurred.  It might be useful to modify the data in the bulk test such that it is easier to determine the time when data was lost.  For example the continuous ingest test creates linked list and it is possible to determine tight time bounds when a node was ingested.  However that may change the nature of this test and the bugs that it might find.", "A_clean_title": ["bulk", "random", "walk", "test", "failedth", "fail", "bulk", "random", "walk", "test", "fail", "while", "run", "10", "node", "cluster", "follow", "error", "messag", "noformat", "18", "23:36:05167", "bulk", "setup", "info", "start", "bulk", "test", "459a04a0", "19", "00:24:33950", "randomwalk", "framework", "error", "error", "dure", "random", "walk", "java", "lang", "except", "error", "run", "node", "bulk", "xml", "at", "org", "apach", "accumulo", "server", "test", "randomwalk", "modul", "visit", "modul", "java:253", "at", "org", "apach", "accumulo", "server", "test", "randomwalk", "framework", "run", "framework", "java:61", "at", "org", "apach", "accumulo", "server", "test", "randomwalk", "framework", "main", "framework", "java:114", "at", "sun", "reflect", "nativemethodaccessorimpl", "invoke0", "nativ", "method", "accessor", "impl", "nativ", "method", "at", "sun", "reflect", "nativemethodaccessorimpl", "invok", "nativ", "method", "accessor", "impl", "nativemethodaccessorimpl", "java:39", "nativ", "method", "accessor", "impl", "at", "sun", "reflect", "delegatingmethodaccessorimpl", "invok", "deleg", "method", "accessor", "impl", "delegatingmethodaccessorimpl", "java:25", "deleg", "method", "accessor", "impl", "at", "java", "lang", "reflect", "method", "invok", "method", "java:597", "at", "org", "apach", "accumulo", "start", "main", "run", "main", "java:89", "at", "java", "lang", "thread", "run", "thread", "java:662", "caus", "by", "java", "lang", "except", "error", "run", "node", "bulk", "verifi", "at", "org", "apach", "accumulo", "server", "test", "randomwalk", "modul", "visit", "modul", "java:253", "at", "org", "apach", "accumulo", "server", "test", "randomwalk", "modul", "visit", "modul", "java:249", "more", "caus", "by", "java", "lang", "except", "bad", "key", "at", "r0d646", "cf:000", "1326932285943", "fals", "at", "org", "apach", "accumulo", "server", "test", "randomwalk", "bulk", "verifi", "visit", "verifi", "java:51", "at", "org", "apach", "accumulo", "server", "test", "randomwalk", "modul", "visit", "modul", "java:249", "more", "noformat", "look", "at", "tabl", "row", "r0d646", "r0edd9", "r0f056", "r10467", "all", "had", "valu", "there", "wa", "tablet", "that", "overlap", "first", "rang", "row", "exactli", "268", "r0edd9", "r0d645", "thi", "tablet", "had", "onli", "follow", "activ", "tablet", "server", "wa", "then", "merg", "out", "exist", "merg", "oper", "wa", "268", "r10eff", "r093b1", "noformat", "19", "00:05:10966", "tabletserv", "tablet", "debug", "file", "low", "split", "268", "r0edd9", "r0d645", "rf", "0001azp", "i0001azt", "rf", "0001azp", "i0001azu", "rf", "0001ale", "a0001an3", "19", "00:05:10974", "tabletserv", "tablet", "tablet", "hist", "268", "r0f055", "r0d645", "split", "268", "r0edd9", "r0d645", "268", "r0f055", "r0edd9", "19", "00:05:10975", "tabletserv", "tablet", "tablet", "hist", "268", "r0edd9", "r0d645", "open", "19", "00:05:15029", "tabletserv", "tablet", "tablet", "hist", "268", "r0edd9", "r0d645", "import", "rf", "0001azi", "i0001azm", "17138", "19", "00:05:15103", "tabletserv", "tablet", "debug", "start", "majc", "maj", "268", "r0edd9", "r0d645", "rf", "0001azi", "i0001azm", "rf", "0001azp", "i0001azt", "rf", "0001azp", "i0001azu", "rf", "0001ale", "a0001an3", "0001apj", "a0001bri", "rf", "tmp", "19", "00:05:15339", "tabletserv", "tablet", "tablet", "hist", "268", "r0edd9", "r0d645", "import", "rf", "0001azx", "i0001azi", "16620", "19", "00:05:15651", "tabletserv", "compactor", "debug", "compact", "268", "r0edd9", "r0d645", "181080", "read", "60360", "written", "553761", "entri", "sec", "327", "sec", "19", "00:05:15661", "tabletserv", "tablet", "tablet", "hist", "268", "r0edd9", "r0d645", "majc", "maj", "rf", "0001azi", "i0001azm", "rf", "0001azp", "i0001azt", "rf", "0001azp", "i0001azu", "rf", "0001ale", "a0001an3", "rf", "0001apj", "a0001bri", "19", "00:05:30672", "tabletserv", "tablet", "debug", "start", "majc", "maj", "268", "r0edd9", "r0d645", "rf", "0001azx", "i0001azi", "0001apj", "c0001brn", "rf", "tmp", "19", "00:05:30810", "tabletserv", "compactor", "debug", "compact", "268", "r0edd9", "r0d645", "60360", "read", "60360", "written", "534159", "entri", "sec", "113", "sec", "19", "00:05:30824", "tabletserv", "tablet", "tablet", "hist", "268", "r0edd9", "r0d645", "majc", "maj", "rf", "0001azx", "i0001azi", "rf", "0001apj", "c0001brn", "19", "00:05:30943", "tabletserv", "tablet", "debug", "initiateclos", "initi", "close", "savestate=tru", "save", "state=tru", "queueminc=fals", "queue", "min", "c=fals", "disablewrites=fals", "disabl", "writes=fals", "268", "r0edd9", "r0d645", "19", "00:05:30943", "tabletserv", "tablet", "debug", "completeclos", "complet", "close", "savestate=tru", "save", "state=tru", "completeclose=tru", "complet", "close=tru", "268", "r0edd9", "r0d645", "19", "00:05:30947", "tabletserv", "tablet", "tablet", "hist", "268", "r0edd9", "r0d645", "close", "19", "00:05:30947", "tabletserv", "tabletserv", "tablet", "server", "debug", "unassign", "268", "r0edd9", "r0d645", "nullxxx", "xxx", "xxx", "xxx:9997134d7425fc59413null", "19", "00:05:30949", "tabletserv", "tabletserv", "tablet", "server", "info", "unload", "268", "r0edd9", "r0d645", "19", "00:05:30949", "tabletserv", "tabletserv", "tablet", "server", "info", "unload", "268", "r0edd9", "r0d645", "noformat", "second", "rang", "valu", "r0f056", "r10467", "r0f056", "correspond", "split", "point", "r0f055", "howerv", "there", "no", "split", "point", "correspond", "r10467", "all", "tablet", "split", "r0f055", "live", "one", "tablet", "server", "noformat", "19", "00:02:21262", "tabletserv", "tablet", "tablet", "hist", "268", "r0d645", "split", "268", "r0f055", "r0d645", "268", "r0f055", "19", "00:02:21263", "tabletserv", "tablet", "tablet", "hist", "268", "r0f055", "r0d645", "open", "19", "00:02:21264", "tabletserv", "tablet", "tablet", "hist", "268", "r0f055", "open", "19", "00:02:44504", "tabletserv", "tablet", "tablet", "hist", "268", "r0f055", "split", "268", "r11da6", "r0f055", "268", "r11da6", "19", "00:02:44505", "tabletserv", "tablet", "tablet", "hist", "268", "r11da6", "r0f055", "open", "19", "00:05:10974", "tabletserv", "tablet", "tablet", "hist", "268", "r0f055", "r0d645", "split", "268", "r0edd9", "r0d645", "268", "r0f055", "r0edd9", "19", "00:05:10975", "tabletserv", "tablet", "tablet", "hist", "268", "r0f055", "r0edd9", "open", "19", "00:05:15023", "tabletserv", "tablet", "tablet", "hist", "268", "r11da6", "r0f055", "split", "268", "r0f622", "r0f055", "268", "r11da6", "r0f622", "19", "00:05:15024", "tabletserv", "tablet", "tablet", "hist", "268", "r0f622", "r0f055", "open", "noformat", "all", "tablet", "mention", "so", "far", "were", "all", "merg", "away", "same", "merg", "oper", "make", "thi", "oper", "possibl", "place", "were", "data", "loss", "occur", "howev", "not", "pinpoint", "issu", "at", "thi", "point", "time", "below", "littl", "info", "about", "merg", "master", "log", "show", "which", "tablet", "were", "involv", "merg", "noformat", "19", "00:05:30616", "master", "eventcoordin", "event", "coordin", "info", "merg", "state", "268", "r10eff", "r093b1", "set", "wait", "chop", "19", "00:05:30677", "master", "master", "info", "ask", "xxx", "xxx", "xxx", "xxx:9997134d7425fc5940c", "chop", "268", "r09927", "r0903a", "19", "00:05:30678", "master", "master", "info", "ask", "xxx", "xxx", "xxx", "xxx:9997134d7425fc5940c", "chop", "268", "r0ca9e", "r09927", "19", "00:05:30678", "master", "master", "info", "ask", "xxx", "xxx", "xxx", "xxx:9997134d7425fc5940a", "chop", "268", "r0d2b5", "r0ca9e", "19", "00:05:30678", "master", "master", "info", "ask", "xxx", "xxx", "xxx", "xxx:9997134d7425fc59412", "chop", "268", "r0d645", "r0d2b5", "19", "00:05:30678", "master", "master", "info", "ask", "xxx", "xxx", "xxx", "xxx:9997134d7425fc59413", "chop", "268", "r0edd9", "r0d645", "19", "00:05:30678", "master", "master", "info", "ask", "xxx", "xxx", "xxx", "xxx:9997134d7425fc59413", "chop", "268", "r0f055", "r0edd9", "19", "00:05:30678", "master", "master", "info", "ask", "xxx", "xxx", "xxx", "xxx:9997134d7425fc59413", "chop", "268", "r0f622", "r0f055", "19", "00:05:30678", "master", "master", "info", "ask", "xxx", "xxx", "xxx", "xxx:9997134d7425fc59413", "chop", "268", "r0f68b", "r0f622", "19", "00:05:30678", "master", "master", "info", "ask", "xxx", "xxx", "xxx", "xxx:9997134d7425fc59413", "chop", "268", "r10c14", "r0f68b", "19", "00:05:30678", "master", "master", "info", "ask", "xxx", "xxx", "xxx", "xxx:9997134d7425fc59413", "chop", "268", "r110f7", "r10c14", "noformat", "when", "thi", "test", "verifi", "it", "data", "detect", "data", "loss", "there", "no", "easi", "way", "determin", "at", "what", "time", "data", "loss", "occur", "it", "might", "use", "modifi", "data", "bulk", "test", "such", "that", "it", "easier", "determin", "time", "when", "data", "wa", "lost", "exampl", "continu", "ingest", "test", "creat", "link", "list", "it", "possibl", "determin", "tight", "time", "bound", "when", "node", "wa", "ingest", "howev", "that", "may", "chang", "natur", "thi", "test", "bug", "that", "it", "might", "find"], "B_title": "ACCUMULO-307 merged from 1.4", "B_clean_title": ["accumulo", "307", "merg"]},
{"A_title": "DateTimeField improperly converts time causing wrong dates when the servers current date is different from the clients date.The bug is in DateTimeField#convertInput(). <code> // Get year month and day ignoring any timezone of the Date object Calendar cal = Calendar.getInstance(); cal.setTime(dateFieldInput); int year = cal.get(Calendar.YEAR); int month = cal.get(Calendar.MONTH) + 1; int day = cal.get(Calendar.DAY_OF_MONTH); int hours = (hoursInput == null ? 0 : hoursInput % 24); int minutes = (minutesInput == null ? 0 : minutesInput);  // Use the input to create a date object with proper timezone MutableDateTime date = new MutableDateTime(year month day hours minutes 0 0 DateTimeZone.forTimeZone(getClientTimeZone())); </code> If the servers current date is different from the clients this produces wrong output. I attached a patch with a test case that simulates this condition.  I dont know why this casting of day month year is done.", "A_clean_title": ["datetimefield", "date", "time", "field", "improperli", "convert", "time", "caus", "wrong", "date", "when", "server", "current", "date", "differ", "client", "date", "bug", "datetimefield", "date", "time", "field", "convertinput", "convert", "input", "code", "get", "year", "month", "day", "ignor", "ani", "timezon", "date", "object", "calendar", "cal", "calendar", "getinst", "get", "instanc", "cal", "settim", "set", "time", "datefieldinput", "date", "field", "input", "int", "year", "cal", "get", "calendar", "year", "int", "month", "cal", "get", "calendar", "month", "int", "day", "cal", "get", "calendar", "day", "month", "int", "hour", "hoursinput", "hour", "input", "null", "hoursinput", "hour", "input", "24", "int", "minut", "minutesinput", "minut", "input", "null", "minutesinput", "minut", "input", "use", "input", "creat", "date", "object", "proper", "timezon", "mutabledatetim", "mutabl", "date", "time", "date", "new", "mutabledatetim", "mutabl", "date", "time", "year", "month", "day", "hour", "minut", "datetimezon", "fortimezon", "date", "time", "zone", "time", "zone", "getclienttimezon", "get", "client", "time", "zone", "code", "server", "current", "date", "differ", "client", "thi", "produc", "wrong", "output", "attach", "patch", "test", "case", "that", "simul", "thi", "condit", "dont", "know", "whi", "thi", "cast", "day", "month", "year", "done"], "B_title": "", "B_clean_title": []},
{"A_title": "Cancelling a running job can lead to restart instead of stoppingI just tried cancelling a regularly running job. Instead of the job stopping it restarted.   code 2016-02-29 10:39:28415 INFO  org.apache.flink.yarn.YarnJobManager                          - Trying to cancel job with ID 5c0604694c8469cfbb89daaa990068df. 2016-02-29 10:39:28416 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map Timestamps/Watermarks) (1/1) (e3b05555ab0e373defb925898de9f200) switched from RUNNING to CANCELING .... 2016-02-29 10:39:28488 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000) FoldingStateDescriptorname=window-contents defaultValue=(092233720368547758070) serializer=null EventTimeTrigger() WindowedStream.apply(WindowedStream.java:397)) (19/24) (c1be31b0be596d2521073b2d78ffa60a) switched from CANCELING to CANCELED 2016-02-29 10:40:08468 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map Timestamps/Watermarks) (1/1) (e3b05555ab0e373defb925898de9f200) switched from CANCELING to FAILED 2016-02-29 10:40:08468 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000) FoldingStateDescriptorname=window-contents defaultValue=(092233720368547758070) serializer=null EventTimeTrigger() WindowedStream.apply(WindowedStream.java:397)) (1/24) (5ad172ec9932b24d5a98377a2c82b0b3) switched from CANCELING to FAILED 2016-02-29 10:40:08472 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000) FoldingStateDescriptorname=window-contents defaultValue=(092233720368547758070) serializer=null EventTimeTrigger() WindowedStream.apply(WindowedStream.java:397)) (2/24) (5404ca28ac7cf23b67dff30ef2309078) switched from CANCELING to FAILED 2016-02-29 10:40:08473 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: auto.offset.reset=earliest rocksdb=hdfs:///user/robert/rocksdb generateInPlace=soTrue parallelism=24 bootstrap.servers=cdh544-worker-0:9092 topic=eventsGenerator eventsPerKeyPerGenerator=2 numKeys=1000000000 zookeeper.connect=cdh544-worker-0:2181 timeSliceSize=60000 eventsKerPey=1 genPar=1) changed to FAILING. java.lang.Exception: Task could not be canceled. at org.apache.flink.runtime.executiongraph.Execution 5.onComplete(Execution.java:902) at akka.dispatch.OnComplete.internal(Future.scala:246) at akka.dispatch.OnComplete.internal(Future.scala:244) at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on Actorakka.tcp://flink@10.240.242.143:50119/user/taskmanager#640539146 after 10000 ms at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) at java.lang.Thread.run(Thread.java:745) 2016-02-29 10:40:08477 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000) FoldingStateDescriptorname=window-contents defaultValue=(092233720368547758070) serializer=null EventTimeTrigger() WindowedStream.apply(WindowedStream.java:397)) (3/24) (fc527d65ec8df3ccf68f882d968e776e) switched from CANCELING to FAILED 2016-02-29 10:40:08487 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000) FoldingStateDescriptorname=window-contents defaultValue=(092233720368547758070) serializer=null EventTimeTrigger() WindowedStream.apply(WindowedStream.java:397)) (4/24) (afb1aa3c2d8acdee0f138cf344238e4e) switched from CANCELING to FAILED 2016-02-29 10:40:08488 INFO  org.apache.flink.runtime.executiongraph.restart.FixedDelayRestartStrategy  - Delaying retry of job execution for 3000 ms ... 2016-02-29 10:40:08488 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: auto.offset.reset=earliest rocksdb=hdfs:///user/robert/rocksdb generateInPlace=soTrue parallelism=24 bootstrap.servers=cdh544-worker-0:9092 topic=eventsGenerator eventsPerKeyPerGenerator=2 numKeys=1000000000 zookeeper.connect=cdh544-worker-0:2181 timeSliceSize=60000 eventsKerPey=1 genPar=1) changed to RESTARTING. 2016-02-29 10:40:11490 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: auto.offset.reset=earliest rocksdb=hdfs:///user/robert/rocksdb generateInPlace=soTrue parallelism=24 bootstrap.servers=cdh544-worker-0:9092 topic=eventsGenerator eventsPerKeyPerGenerator=2 numKeys=1000000000 zookeeper.connect=cdh544-worker-0:2181 timeSliceSize=60000 eventsKerPey=1 genPar=1) changed to CREATED. 2016-02-29 10:40:11490 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map Timestamps/Watermarks) (1/1) (1319b2f44d78d99948ffde4350c052d9) switched from CREATED to SCHEDULED 2016-02-29 10:40:11490 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: auto.offset.reset=earliest rocksdb=hdfs:///user/robert/rocksdb generateInPlace=soTrue parallelism=24 bootstrap.servers=cdh544-worker-0:9092 topic=eventsGenerator eventsPerKeyPerGenerator=2 numKeys=1000000000 zookeeper.connect=cdh544-worker-0:2181 timeSliceSize=60000 eventsKerPey=1 genPar=1) changed to RUNNING. code", "A_clean_title": ["cancel", "run", "job", "lead", "restart", "instead", "stoppingi", "stop", "just", "tri", "cancel", "regularli", "run", "job", "instead", "job", "stop", "it", "restart", "code", "2016", "02", "29", "10:39:28415", "info", "org", "apach", "flink", "yarn", "yarnjobmanag", "yarn", "job", "manag", "tri", "cancel", "job", "id", "5c0604694c8469cfbb89daaa990068df", "2016", "02", "29", "10:39:28416", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "sourc", "out", "order", "data", "gener", "flat", "map", "timestamp", "watermark", "e3b05555ab0e373defb925898de9f200", "switch", "run", "cancel", "2016", "02", "29", "10:39:28488", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "triggerwindow", "trigger", "window", "tumblingtimewindow", "tumbl", "time", "window", "60000", "foldingstatedescriptorname=window", "content", "fold", "state", "descriptorname=window", "defaultvalue=", "default", "value=", "092233720368547758070", "serializer=nul", "eventtimetrigg", "event", "time", "trigger", "windowedstream", "appli", "window", "stream", "windowedstream", "java:397", "window", "stream", "19", "24", "c1be31b0be596d2521073b2d78ffa60a", "switch", "cancel", "cancel", "2016", "02", "29", "10:40:08468", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "sourc", "out", "order", "data", "gener", "flat", "map", "timestamp", "watermark", "e3b05555ab0e373defb925898de9f200", "switch", "cancel", "fail", "2016", "02", "29", "10:40:08468", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "triggerwindow", "trigger", "window", "tumblingtimewindow", "tumbl", "time", "window", "60000", "foldingstatedescriptorname=window", "content", "fold", "state", "descriptorname=window", "defaultvalue=", "default", "value=", "092233720368547758070", "serializer=nul", "eventtimetrigg", "event", "time", "trigger", "windowedstream", "appli", "window", "stream", "windowedstream", "java:397", "window", "stream", "24", "5ad172ec9932b24d5a98377a2c82b0b3", "switch", "cancel", "fail", "2016", "02", "29", "10:40:08472", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "triggerwindow", "trigger", "window", "tumblingtimewindow", "tumbl", "time", "window", "60000", "foldingstatedescriptorname=window", "content", "fold", "state", "descriptorname=window", "defaultvalue=", "default", "value=", "092233720368547758070", "serializer=nul", "eventtimetrigg", "event", "time", "trigger", "windowedstream", "appli", "window", "stream", "windowedstream", "java:397", "window", "stream", "24", "5404ca28ac7cf23b67dff30ef2309078", "switch", "cancel", "fail", "2016", "02", "29", "10:40:08473", "info", "org", "apach", "flink", "yarn", "yarnjobmanag", "yarn", "job", "manag", "statu", "job", "5c0604694c8469cfbb89daaa990068df", "event", "counter", "auto", "offset", "reset=earliest", "rocksdb=hdf", "user", "robert", "rocksdb", "generateinplace=sotru", "gener", "place=so", "true", "parallelism=24", "worker", "0:9092", "bootstrap", "servers=cdh544", "topic=eventsgener", "topic=ev", "gener", "eventsperkeypergenerator=2", "event", "per", "key", "per", "generator=2", "numkeys=1000000000", "num", "keys=1000000000", "worker", "0:2181", "zookeep", "connect=cdh544", "timeslicesize=60000", "time", "slice", "size=60000", "eventskerpey=1", "event", "ker", "pey=1", "genpar=1", "gen", "par=1", "chang", "fail", "java", "lang", "except", "task", "could", "not", "cancel", "at", "org", "apach", "flink", "runtim", "executiongraph", "execut", "oncomplet", "complet", "execut", "java:902", "at", "akka", "dispatch", "oncomplet", "intern", "complet", "futur", "scala:246", "at", "akka", "dispatch", "oncomplet", "intern", "complet", "futur", "scala:244", "at", "akka", "dispatch", "japi", "callbackbridg", "appli", "callback", "bridg", "futur", "scala:174", "at", "akka", "dispatch", "japi", "callbackbridg", "appli", "callback", "bridg", "futur", "scala:171", "at", "scala", "concurr", "impl", "callbackrunn", "run", "callback", "runnabl", "promis", "scala:32", "at", "scala", "concurr", "impl", "executioncontextimpl", "execut", "context", "impl", "anon", "exec", "executioncontextimpl", "scala:107", "execut", "context", "impl", "at", "scala", "concurr", "forkjoin", "forkjointask", "doexec", "fork", "join", "task", "exec", "forkjointask", "java:260", "fork", "join", "task", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "fork", "join", "pool", "workqueu", "runtask", "work", "queue", "run", "task", "forkjoinpool", "java:1339", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinpool", "runwork", "fork", "join", "pool", "run", "worker", "forkjoinpool", "java:1979", "fork", "join", "pool", "at", "scala", "concurr", "forkjoin", "forkjoinworkerthread", "run", "fork", "join", "worker", "thread", "forkjoinworkerthread", "java:107", "fork", "join", "worker", "thread", "caus", "by", "akka", "pattern", "asktimeoutexcept", "ask", "timeout", "except", "ask", "time", "out", "actorakka", "tcp", "flink", "10", "240", "242", "143:50119", "user", "taskmanag", "640539146", "after", "10000", "ms", "at", "akka", "pattern", "promiseactorref", "promis", "actor", "ref", "anonfun", "appli", "mcv", "mc", "sp", "asksupport", "scala:333", "ask", "support", "at", "akka", "actor", "schedul", "anon", "run", "schedul", "scala:117", "at", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "unbatchedexecut", "unbatch", "execut", "futur", "scala:694", "at", "scala", "concurr", "futur", "internalcallbackexecutor", "intern", "callback", "executor", "execut", "futur", "scala:691", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "taskhold", "executetask", "task", "holder", "execut", "task", "schedul", "scala:467", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "executebucket", "execut", "bucket", "schedul", "scala:419", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "nexttick", "next", "tick", "schedul", "scala:423", "at", "akka", "actor", "lightarrayrevolverschedul", "light", "array", "revolv", "schedul", "anon", "run", "schedul", "scala:375", "at", "java", "lang", "thread", "run", "thread", "java:745", "2016", "02", "29", "10:40:08477", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "triggerwindow", "trigger", "window", "tumblingtimewindow", "tumbl", "time", "window", "60000", "foldingstatedescriptorname=window", "content", "fold", "state", "descriptorname=window", "defaultvalue=", "default", "value=", "092233720368547758070", "serializer=nul", "eventtimetrigg", "event", "time", "trigger", "windowedstream", "appli", "window", "stream", "windowedstream", "java:397", "window", "stream", "24", "fc527d65ec8df3ccf68f882d968e776", "switch", "cancel", "fail", "2016", "02", "29", "10:40:08487", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "triggerwindow", "trigger", "window", "tumblingtimewindow", "tumbl", "time", "window", "60000", "foldingstatedescriptorname=window", "content", "fold", "state", "descriptorname=window", "defaultvalue=", "default", "value=", "092233720368547758070", "serializer=nul", "eventtimetrigg", "event", "time", "trigger", "windowedstream", "appli", "window", "stream", "windowedstream", "java:397", "window", "stream", "24", "afb1aa3c2d8acdee0f138cf344238e4", "switch", "cancel", "fail", "2016", "02", "29", "10:40:08488", "info", "org", "apach", "flink", "runtim", "executiongraph", "restart", "fixeddelayrestartstrategi", "fix", "delay", "restart", "strategi", "delay", "retri", "job", "execut", "3000", "ms", "2016", "02", "29", "10:40:08488", "info", "org", "apach", "flink", "yarn", "yarnjobmanag", "yarn", "job", "manag", "statu", "job", "5c0604694c8469cfbb89daaa990068df", "event", "counter", "auto", "offset", "reset=earliest", "rocksdb=hdf", "user", "robert", "rocksdb", "generateinplace=sotru", "gener", "place=so", "true", "parallelism=24", "worker", "0:9092", "bootstrap", "servers=cdh544", "topic=eventsgener", "topic=ev", "gener", "eventsperkeypergenerator=2", "event", "per", "key", "per", "generator=2", "numkeys=1000000000", "num", "keys=1000000000", "worker", "0:2181", "zookeep", "connect=cdh544", "timeslicesize=60000", "time", "slice", "size=60000", "eventskerpey=1", "event", "ker", "pey=1", "genpar=1", "gen", "par=1", "chang", "restart", "2016", "02", "29", "10:40:11490", "info", "org", "apach", "flink", "yarn", "yarnjobmanag", "yarn", "job", "manag", "statu", "job", "5c0604694c8469cfbb89daaa990068df", "event", "counter", "auto", "offset", "reset=earliest", "rocksdb=hdf", "user", "robert", "rocksdb", "generateinplace=sotru", "gener", "place=so", "true", "parallelism=24", "worker", "0:9092", "bootstrap", "servers=cdh544", "topic=eventsgener", "topic=ev", "gener", "eventsperkeypergenerator=2", "event", "per", "key", "per", "generator=2", "numkeys=1000000000", "num", "keys=1000000000", "worker", "0:2181", "zookeep", "connect=cdh544", "timeslicesize=60000", "time", "slice", "size=60000", "eventskerpey=1", "event", "ker", "pey=1", "genpar=1", "gen", "par=1", "chang", "creat", "2016", "02", "29", "10:40:11490", "info", "org", "apach", "flink", "runtim", "executiongraph", "executiongraph", "execut", "graph", "sourc", "out", "order", "data", "gener", "flat", "map", "timestamp", "watermark", "1319b2f44d78d99948ffde4350c052d9", "switch", "creat", "schedul", "2016", "02", "29", "10:40:11490", "info", "org", "apach", "flink", "yarn", "yarnjobmanag", "yarn", "job", "manag", "statu", "job", "5c0604694c8469cfbb89daaa990068df", "event", "counter", "auto", "offset", "reset=earliest", "rocksdb=hdf", "user", "robert", "rocksdb", "generateinplace=sotru", "gener", "place=so", "true", "parallelism=24", "worker", "0:9092", "bootstrap", "servers=cdh544", "topic=eventsgener", "topic=ev", "gener", "eventsperkeypergenerator=2", "event", "per", "key", "per", "generator=2", "numkeys=1000000000", "num", "keys=1000000000", "worker", "0:2181", "zookeep", "connect=cdh544", "timeslicesize=60000", "time", "slice", "size=60000", "eventskerpey=1", "event", "ker", "pey=1", "genpar=1", "gen", "par=1", "chang", "run", "code"], "B_title": "runtime Prevent canceling Execution from failing", "B_clean_title": ["runtim", "prevent", "cancel", "execut", "fail"]},
{"A_title": "MockTables addMutation does not check for empty mutationWhen calling addMutation or addMutations on a MockBatchWriter the updates stored in the mutation are iterated over then committed in the MockTable class.   When this occurs in the TabletServerBatchWriter (eventually called from the BatchWriterImpl) however the mutation size is first checked and if the mutation size is 0 an IllegalArgumentException is thrown.  In practice if you have code that tries to submit an empty mutation to a BatchWriter it will fail and throw an exception in the real world but this will not be caught in tests against MockAccumulo.", "A_clean_title": ["mocktabl", "mock", "tabl", "addmut", "add", "mutat", "not", "check", "empti", "mutationwhen", "mutat", "when", "call", "addmut", "add", "mutat", "or", "addmut", "add", "mutat", "mockbatchwrit", "mock", "batch", "writer", "updat", "store", "mutat", "are", "iter", "over", "then", "commit", "mocktabl", "mock", "tabl", "class", "when", "thi", "occur", "tabletserverbatchwrit", "tablet", "server", "batch", "writer", "eventu", "call", "batchwriterimpl", "batch", "writer", "impl", "howev", "mutat", "size", "first", "check", "mutat", "size", "illegalargumentexcept", "illeg", "argument", "except", "thrown", "practic", "you", "have", "code", "that", "tri", "submit", "empti", "mutat", "batchwrit", "batch", "writer", "it", "will", "fail", "throw", "except", "real", "world", "but", "thi", "will", "not", "caught", "test", "against", "mockaccumulo", "mock", "accumulo"], "B_title": "- merging Ryan Learys patch with modifications to update and remove warnings", "B_clean_title": ["merg", "ryan", "leari", "patch", "modif", "updat", "remov", "warn"]},
{"A_title": "use @InjectMocks for final fields.Im trying to upgrade the mockito version that were using (1.8.5) to a newer version but there is a problem with @InjectMocks which since 1.9.0 doesnt inject into final field anymore.", "A_clean_title": ["use", "injectmock", "inject", "mock", "final", "field", "im", "tri", "upgrad", "mockito", "version", "that", "were", "newer", "version", "but", "there", "problem", "injectmock", "inject", "mock", "which", "sinc", "doesnt", "inject", "into", "final", "field", "anymor"], "B_title": "issue 352 : With answer ReturnsEmptyValues Mockito-mocked methods returning primitive types other than int throw ClassCastException: java.lang.Integer. Thanks to Jesse Wilson for spotting that one and providing a patch. I changed a few things however  especially:  - used Primitives utility class instead  - applied underscored names for test cases as decided internally", "B_clean_title": ["issu", "352", "answer", "returnsemptyvalu", "return", "empti", "valu", "mockito", "mock", "method", "return", "primit", "type", "other", "than", "int", "throw", "classcastexcept", "class", "cast", "except", "java", "lang", "integ", "thank", "jess", "wilson", "spot", "that", "one", "provid", "patch", "chang", "few", "thing", "howev", "especi", "use", "primit", "util", "class", "instead", "appli", "underscor", "name", "test", "case", "as", "decid", "intern"]},
{"A_title": "Return error code 400 when an Ajax request has no base url set in header/request parameters.Hello  currently weve got a problem with faked ajax requests. these ajax  requests misses some parameters but the wicket-ajax header flag is set.  So ServletWebRequest throws an exception:  java.lang.IllegalStateException: Current ajax request is missing the base url header or parameter          at org.apache.wicket.util.lang.Checks.notNull(Checks.java:38)          at org.apache.wicket.protocol.http.servlet.ServletWebRequest.getClientUrl(ServletWebRequest.java:171)          at org.apache.wicket.request.UrlRenderer.<init>(UrlRenderer.java:59)   These faked requests are so massive that our application is no longer  monitorable. Our workaround rejects these requests via apache config.   Instead of logging an exception in deployment mode wicket should log a warning and reject the request", "A_clean_title": ["return", "error", "code", "400", "when", "ajax", "request", "ha", "no", "base", "url", "set", "header", "request", "paramet", "hello", "current", "weve", "got", "problem", "fake", "ajax", "request", "these", "ajax", "request", "miss", "some", "paramet", "but", "wicket", "ajax", "header", "flag", "set", "so", "servletwebrequest", "servlet", "web", "request", "throw", "except", "java", "lang", "illegalstateexcept", "illeg", "state", "except", "current", "ajax", "request", "miss", "base", "url", "header", "or", "paramet", "at", "org", "apach", "wicket", "util", "lang", "check", "notnul", "not", "null", "check", "java:38", "at", "org", "apach", "wicket", "protocol", "http", "servlet", "servletwebrequest", "getclienturl", "servlet", "web", "request", "get", "client", "url", "servletwebrequest", "java:171", "servlet", "web", "request", "at", "org", "apach", "wicket", "request", "urlrender", "url", "render", "init", "urlrender", "java:59", "url", "render", "these", "fake", "request", "are", "so", "massiv", "that", "our", "applic", "no", "longer", "monitor", "our", "workaround", "reject", "these", "request", "via", "apach", "config", "instead", "log", "except", "deploy", "mode", "wicket", "log", "warn", "reject", "request"], "B_title": "Return error code 400 when an Ajax request has no base url set in header/request parameters.", "B_clean_title": ["return", "error", "code", "400", "when", "ajax", "request", "ha", "no", "base", "url", "set", "header", "request", "paramet"]},
{"A_title": "meitrack temp sensor with negative valueHi Anton  Im receiving a wrong value from temp sensor for negative temp values example: data for temp is 06FB2E which should translate to -12.34C according to doc for protocol v3.  Could you please check if traccar is reading temp correctly for negative values?  a sample data to test with:   Regards", "A_clean_title": ["meitrack", "temp", "sensor", "neg", "valuehi", "valu", "hi", "anton", "im", "receiv", "wrong", "valu", "temp", "sensor", "neg", "temp", "valu", "exampl", "data", "temp", "06fb2e", "which", "translat", "12", "34c", "accord", "doc", "protocol", "v3", "could", "you", "pleas", "check", "traccar", "read", "temp", "correctli", "neg", "valu", "sampl", "data", "test", "regard"], "B_title": "Fix Meitrack temperature (fix #3604)", "B_clean_title": ["fix", "meitrack", "temperatur", "fix", "3604"]},
{"A_title": "Simplex Solver arrives at incorrect solutionI have reduced the problem reported to me down to a minimal test case which I will attach.", "A_clean_title": ["simplex", "solver", "arriv", "at", "incorrect", "solutioni", "solut", "have", "reduc", "problem", "report", "me", "down", "minim", "test", "case", "which", "will", "attach"], "B_title": "Fixed a problem when setting some variables (several variables were set instead of only one) JIRA: MATH-272", "B_clean_title": ["fix", "problem", "when", "set", "some", "variabl", "sever", "variabl", "were", "set", "instead", "onli", "one", "jira", "math", "272"]},
{"A_title": "Incorrect date parsed when week and month used togetherIt should print 2011-01-03 but it is printing 2010-01-04.", "A_clean_title": ["incorrect", "date", "pars", "when", "week", "month", "use", "togetherit", "togeth", "it", "print", "2011", "01", "03", "but", "it", "print", "2010", "01", "04"], "B_title": "Conversion from parsed values to a date-time handles weird sets of fields better 3161586 This change is mostly for combinations like weekyear-month-week The new code doesnt handle all combinations perfectly but its better than it was", "B_clean_title": ["convers", "pars", "valu", "date", "time", "handl", "weird", "set", "field", "better", "3161586", "thi", "chang", "mostli", "combin", "like", "weekyear", "month", "week", "new", "code", "doesnt", "handl", "all", "combin", "perfectli", "but", "it", "better", "than", "it", "wa"]},
{"A_title": "NativeQuery with Pagination validation error at startup DATAJPA-928opened and commented According to Example 50 at  Using @Query docs  its possible to use a native query with pagination using Pageable but in my case its failing with a org.springframework.data.jpa.repository.query.InvalidJpaQueryMethodException. NativeJpaQuery constructor is checking if the query has a Pageable parameter and if the queryString contains a #pageable or #sort sequence. The query has Pageable parameter but it  does not contain a #pageable string:   If I provide a #pageable string at the end of the query validation passes but when the query executes it fails saying that its expecting 3 parameters instead of 2.  Funny thing is that when the server is starting if I set a breakpoint inside NativeJpaQuery and change containsPageableOrSortInQueryExpression from false to true manually validation passes just fine and the query executes well paginating    Affects: 1.10.1 (Hopper SR1) 1.10.2 (Hopper SR2)  Reference URL:  http://stackoverflow.com/questions/38349930/spring-data-and-native-query-with-pagination  Issue Links:     Referenced from: pull request #246  and commits     Backported to:  2.0.4 (Kay SR4) 1 votes 12 watchers", "A_clean_title": ["nativequeri", "nativ", "queri", "pagin", "valid", "error", "at", "startup", "datajpa", "928open", "comment", "accord", "exampl", "50", "at", "queri", "doc", "it", "possibl", "use", "nativ", "queri", "pagin", "pageabl", "but", "my", "case", "it", "fail", "org", "springframework", "data", "jpa", "repositori", "queri", "invalidjpaquerymethodexcept", "invalid", "jpa", "queri", "method", "except", "nativejpaqueri", "nativ", "jpa", "queri", "constructor", "check", "queri", "ha", "pageabl", "paramet", "querystr", "queri", "string", "contain", "pageabl", "or", "sort", "sequenc", "queri", "ha", "pageabl", "paramet", "but", "it", "not", "contain", "pageabl", "string", "provid", "pageabl", "string", "at", "end", "queri", "valid", "pass", "but", "when", "queri", "execut", "it", "fail", "say", "that", "it", "expect", "paramet", "instead", "funni", "thing", "that", "when", "server", "start", "set", "breakpoint", "insid", "nativejpaqueri", "nativ", "jpa", "queri", "chang", "containspageableorsortinqueryexpress", "contain", "pageabl", "or", "sort", "queri", "express", "fals", "true", "manual", "valid", "pass", "just", "fine", "queri", "execut", "well", "pagin", "affect", "10", "hopper", "sr1", "10", "hopper", "sr2", "refer", "url", "http", "data", "nativ", "queri", "pagin", "stackoverflow", "com", "question", "38349930", "spring", "issu", "link", "referenc", "pull", "request", "246", "commit", "backport", "kay", "sr4", "vote", "12", "watcher"], "B_title": "DATAJPA-928 - Enabled native query with Pageable.  Just removed the check that was actively preventing the use of Pageable. Migrated to tests to AssertJ where applicable.  Original pull request: #246.", "B_clean_title": ["datajpa", "928", "enabl", "nativ", "queri", "pageabl", "just", "remov", "check", "that", "wa", "activ", "prevent", "use", "pageabl", "migrat", "test", "assertj", "assert", "where", "applic", "origin", "pull", "request", "246"]},
{"A_title": "BSPTree class and recovery of a Euclidean 3D BRepNew to the work here. Thanks for your efforts on this code.  I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(xyz) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem.  Any ideas?", "A_clean_title": ["bsptree", "bsp", "tree", "class", "recoveri", "euclidean", "3d", "brepnew", "rep", "new", "work", "here", "thank", "your", "effort", "thi", "code", "creat", "bsptree", "bsp", "tree", "boundaryrep", "boundari", "rep", "brep", "my", "test", "brep", "cube", "as", "repres", "by", "float", "array", "contain", "3d", "point", "xyz", "order", "array", "indic", "12", "triplet", "12", "face", "cube", "construct", "bspmesh", "bsp", "mesh", "as", "shown", "code", "below", "construct", "polyhedronsset", "polyhedron", "set", "but", "have", "problem", "extract", "face", "bsptree", "bsp", "tree", "reconstruct", "brep", "rep", "attach", "code", "bspmesh2", "java", "bsp", "mesh2", "show", "that", "small", "chang", "vertex", "posit", "caus", "correct", "problem", "ani", "idea"], "B_title": "Fixed a wrong assumption on BSP tree attributes.", "B_clean_title": ["fix", "wrong", "assumpt", "bsp", "tree", "attribut"]},
{"A_title": "UnmergedBranch state growing with empty BranchCommit leading to performance degradationIn some cluster deployment cases it has been seen that in memory state of UnmergedBranches contains large number of empty commits. For e.g. in  one of of the runs there were 750 entries in the UnmergedBranches and each Branch had empty branch commits.  If there are large number of UnmergedBranches then read performance would degrade as for determining revision validity currently logic scans all branches  Below is some part of UnmergedBranch state  noformat Branch 1 1 -> br146d2edb7a7-0-1 (true) (revision: br146d2edb7a7-0-1 clusterId: 1 time: 2014-06-25 05:08:52.903 branch: true) 2 -> br146d2f0450b-0-1 (true) (revision: br146d2f0450b-0-1 clusterId: 1 time: 2014-06-25 05:11:40.171 branch: true) Branch 2 1 -> br146d2ef1d08-0-1 (true) (revision: br146d2ef1d08-0-1 clusterId: 1 time: 2014-06-25 05:10:24.392 branch: true) Branch 3 1 -> br146d2ed26ca-0-1 (true) (revision: br146d2ed26ca-0-1 clusterId: 1 time: 2014-06-25 05:08:15.818 branch: true) 2 -> br146d2edfd0e-0-1 (true) (revision: br146d2edfd0e-0-1 clusterId: 1 time: 2014-06-25 05:09:10.670 branch: true) Branch 4 1 -> br146d2ecd85b-0-1 (true) (revision: br146d2ecd85b-0-1 clusterId: 1 time: 2014-06-25 05:07:55.739 branch: true) Branch 5 1 -> br146d2ec21a0-0-1 (true) (revision: br146d2ec21a0-0-1 clusterId: 1 time: 2014-06-25 05:07:08.960 branch: true) 2 -> br146d2ec8eca-0-1 (true) (revision: br146d2ec8eca-0-1 clusterId: 1 time: 2014-06-25 05:07:36.906 branch: true) Branch 6 1 -> br146d2eaf159-1-1 (true) (revision: br146d2eaf159-1-1 clusterId: 1 time: 2014-06-25 05:05:51.065 counter: 1 branch: true) Branch 7 1 -> br146d2e9a513-0-1 (true) (revision: br146d2e9a513-0-1 clusterId: 1 time: 2014-06-25 05:04:26.003 branch: true) noformat  ~mreutegg Suggested that these branch might be for those revision which have resulted in a collision and upon checking it indeed appears to be the case  (value true in brackets above indicate that). Further given the age of such revision it looks like they get populated upon startup itself  *Fix* * Need to check why we need to populate the UnermgedBranch * Possibly implement some purge job which would remove such stale entries", "A_clean_title": ["unmergedbranch", "unmerg", "branch", "state", "grow", "empti", "branchcommit", "branch", "commit", "lead", "perform", "degradationin", "degrad", "some", "cluster", "deploy", "case", "it", "ha", "been", "seen", "that", "memori", "state", "unmergedbranch", "unmerg", "branch", "contain", "larg", "number", "empti", "commit", "one", "run", "there", "were", "750", "entri", "unmergedbranch", "unmerg", "branch", "each", "branch", "had", "empti", "branch", "commit", "there", "are", "larg", "number", "unmergedbranch", "unmerg", "branch", "then", "read", "perform", "would", "degrad", "as", "determin", "revis", "valid", "current", "logic", "scan", "all", "branch", "below", "some", "part", "unmergedbranch", "unmerg", "branch", "state", "noformat", "branch", "br146d2edb7a7", "true", "revis", "br146d2edb7a7", "clusterid", "cluster", "id", "time", "2014", "06", "25", "05:08:52", "903", "branch", "true", "br146d2f0450b", "true", "revis", "br146d2f0450b", "clusterid", "cluster", "id", "time", "2014", "06", "25", "05:11:40", "171", "branch", "true", "branch", "br146d2ef1d08", "true", "revis", "br146d2ef1d08", "clusterid", "cluster", "id", "time", "2014", "06", "25", "05:10:24", "392", "branch", "true", "branch", "br146d2ed26ca", "true", "revis", "br146d2ed26ca", "clusterid", "cluster", "id", "time", "2014", "06", "25", "05:08:15", "818", "branch", "true", "br146d2edfd0", "true", "revis", "br146d2edfd0", "clusterid", "cluster", "id", "time", "2014", "06", "25", "05:09:10", "670", "branch", "true", "branch", "br146d2ecd85b", "true", "revis", "br146d2ecd85b", "clusterid", "cluster", "id", "time", "2014", "06", "25", "05:07:55", "739", "branch", "true", "branch", "br146d2ec21a0", "true", "revis", "br146d2ec21a0", "clusterid", "cluster", "id", "time", "2014", "06", "25", "05:07:08", "960", "branch", "true", "br146d2ec8eca", "true", "revis", "br146d2ec8eca", "clusterid", "cluster", "id", "time", "2014", "06", "25", "05:07:36", "906", "branch", "true", "branch", "br146d2eaf159", "true", "revis", "br146d2eaf159", "clusterid", "cluster", "id", "time", "2014", "06", "25", "05:05:51", "065", "counter", "branch", "true", "branch", "br146d2e9a513", "true", "revis", "br146d2e9a513", "clusterid", "cluster", "id", "time", "2014", "06", "25", "05:04:26", "003", "branch", "true", "noformat", "~mreutegg", "suggest", "that", "these", "branch", "might", "those", "revis", "which", "have", "result", "collis", "upon", "check", "it", "inde", "appear", "case", "valu", "true", "bracket", "abov", "indic", "that", "further", "given", "age", "such", "revis", "it", "look", "like", "they", "get", "popul", "upon", "startup", "itself", "fix", "need", "check", "whi", "we", "need", "popul", "unermgedbranch", "unermg", "branch", "possibl", "implement", "some", "purg", "job", "which", "would", "remov", "such", "stale", "entri"], "B_title": "- UnmergedBranch state growing with empty BranchCommit leading to performance degradation", "B_clean_title": ["unmergedbranch", "unmerg", "branch", "state", "grow", "empti", "branchcommit", "branch", "commit", "lead", "perform", "degrad"]},
{"A_title": "NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing e and E is passed inNumberUtils createNumber throws a StringIndexOutOfBoundsException instead of NumberFormatException when a String containing both possible exponent indicators is passed in. One example of such a String is 1eE.", "A_clean_title": ["numberutil", "number", "util", "createnumb", "creat", "number", "throw", "stringindexoutofboundsexcept", "string", "index", "out", "bound", "except", "when", "argument", "contain", "pass", "innumberutil", "number", "util", "createnumb", "creat", "number", "throw", "stringindexoutofboundsexcept", "string", "index", "out", "bound", "except", "instead", "numberformatexcept", "number", "format", "except", "when", "string", "contain", "both", "possibl", "expon", "indic", "pass", "one", "exampl", "such", "string", "1ee", "1e"], "B_title": "Making it so that NumberUtils.createNumber throws a NumberFormatException instead of a StringIndexOutOfBoundsException when Strings such as 1eE are passed in. Thanks to Ingo Heinrichs report and patch in LANG-638", "B_clean_title": ["make", "it", "so", "that", "numberutil", "createnumb", "number", "util", "creat", "number", "throw", "numberformatexcept", "number", "format", "except", "instead", "stringindexoutofboundsexcept", "string", "index", "out", "bound", "except", "when", "string", "such", "as", "1ee", "1e", "are", "pass", "thank", "ingo", "heinrich", "report", "patch", "lang", "638"]},
{"A_title": "Need range checks for elitismRate in ElitisticListPopulation constructors.There is a range check for setting the elitismRate via ElitisticListPopulations setElitismRate method but not via the constructors.", "A_clean_title": ["need", "rang", "check", "elitismr", "elit", "rate", "elitisticlistpopul", "elitist", "list", "popul", "constructor", "there", "rang", "check", "set", "elitismr", "elit", "rate", "via", "elitisticlistpopul", "elitist", "list", "popul", "setelitismr", "set", "elit", "rate", "method", "but", "not", "via", "constructor"], "B_title": "Use same range check in ctor as in setter for ElitisticListPopulation. Thanks to Reid Hochstedler", "B_clean_title": ["use", "same", "rang", "check", "ctor", "as", "setter", "elitisticlistpopul", "elitist", "list", "popul", "thank", "reid", "hochstedl"]},
{"A_title": "IndexRule not respecting inheritence based on mixinsIndexRule are meant to be applied based on both primaryType and minin type based inheritance. Currently it appears that only primaryType based inheritance is working", "A_clean_title": ["indexrul", "index", "rule", "not", "respect", "inherit", "base", "mixinsindexrul", "mixin", "index", "rule", "are", "meant", "appli", "base", "both", "primarytyp", "primari", "type", "minin", "type", "base", "inherit", "current", "it", "appear", "that", "onli", "primarytyp", "primari", "type", "base", "inherit", "work"], "B_title": "- IndexRule not respecting inheritence based on mixins", "B_clean_title": ["indexrul", "index", "rule", "not", "respect", "inherit", "base", "mixin"]},
{"A_title": "Broken Link in Tomcat because of Page MountI post this message on the user mailing List (http://apache-wicket.1842946.n4.nabble.com/Broken-Link-in-Tomcat-because-of-Page-Mount-tt4659663.html) and Martin Grigorov asked me to create a ticket on Jira.  Broken Link in Tomcat because of Page Mount  Following situation: -I have a Wicket Application(6.8.0) which runs under the context webapp on a Tomcat 7.0.41 -I mount a Page with two parameters (this is important) in the WicketApplication. mountPage(/mount/ parameter1/ parameter2 MountedPage.class); -The mounted Page(MountedPage.class) has only a simple Link -There are two links on the HomePage to the mounted Page.  They are declared as follows:   add(new Link<Void>(link)  @Override public void onClick()  setResponsePage(MountedPage.class linkParameters);  );  add(new Link<Void>(brokenLink)  @Override public void onClick()  setResponsePage(new MountedPage(linkParameters));  );  I deploy this Application as a war file on a Tomcat under the context webapp. When I call the first Link on the HomePage and then the Link on the mounted Page everything works fine.  But if I call the second Link and then the Link on the mounted Page the link is broken. The context is missing in the generated link http://localhost:8080/wicket/bookmarkable/com.mycompany.LinkedPage  Does anyone have an idea why the second link does not work on Tomcat?  I add a Quickstart and the war file as attachment.  Ps: Both links works fine in Jetty.  Pss:If I remove the mount command both links will work in  Tomcat too.", "A_clean_title": ["broken", "link", "tomcat", "becaus", "page", "mounti", "mount", "post", "thi", "messag", "user", "mail", "list", "http", "link", "tomcat", "becaus", "page", "mount", "apach", "wicket", "1842946", "n4", "nabbl", "tt4659663", "html", "com", "broken", "martin", "grigorov", "ask", "me", "creat", "ticket", "jira", "broken", "link", "tomcat", "becaus", "page", "mount", "follow", "situat", "have", "wicket", "applic", "which", "run", "under", "context", "webapp", "tomcat", "41", "mount", "page", "two", "paramet", "thi", "import", "wicketappl", "wicket", "applic", "mountpag", "mount", "page", "mount", "parameter1", "parameter2", "mountedpag", "class", "mount", "page", "mount", "page", "mountedpag", "class", "mount", "page", "ha", "onli", "simpl", "link", "there", "are", "two", "link", "homepag", "home", "page", "mount", "page", "they", "are", "declar", "as", "follow", "add", "new", "link", "void", "link", "overrid", "public", "void", "onclick", "click", "setresponsepag", "set", "respons", "page", "mountedpag", "class", "mount", "page", "linkparamet", "link", "paramet", "add", "new", "link", "void", "brokenlink", "broken", "link", "overrid", "public", "void", "onclick", "click", "setresponsepag", "set", "respons", "page", "new", "mountedpag", "mount", "page", "linkparamet", "link", "paramet", "deploy", "thi", "applic", "as", "war", "file", "tomcat", "under", "context", "webapp", "when", "call", "first", "link", "homepag", "home", "page", "then", "link", "mount", "page", "everyth", "work", "fine", "but", "call", "second", "link", "then", "link", "mount", "page", "link", "broken", "context", "miss", "gener", "link", "http", "mycompani", "linkedpag", "localhost:8080", "wicket", "bookmark", "com", "link", "page", "anyon", "have", "idea", "whi", "second", "link", "not", "work", "tomcat", "add", "quickstart", "war", "file", "as", "attach", "ps", "both", "link", "work", "fine", "jetti", "pss", "remov", "mount", "command", "both", "link", "will", "work", "tomcat", "too"], "B_title": "dont map handler when value for required placeholder is missing", "B_clean_title": ["dont", "map", "handler", "when", "valu", "requir", "placehold", "miss"]},
{"A_title": "JavaScriptStripper fails with single line commentsThe valid input x++ // x++  gets transformed to x++ x++  which is syntactically invalid. This breaks the unminified version of bootstrap 2.1.1.  The problem doesnt occur with multiline comments because the linebreaks are preserved there.", "A_clean_title": ["javascriptstripp", "java", "script", "stripper", "fail", "singl", "line", "commentsth", "comment", "valid", "input", "x++", "x++", "get", "transform", "x++", "x++", "which", "syntact", "invalid", "thi", "break", "unminifi", "version", "bootstrap", "problem", "doesnt", "occur", "multilin", "comment", "becaus", "linebreak", "are", "preserv", "there"], "B_title": "JavaScriptStripper fails with single line comments", "B_clean_title": ["javascriptstripp", "java", "script", "stripper", "fail", "singl", "line", "comment"]},
{"A_title": "Non-blocking reindexing doesnt finish properlyThe non blocking reindexer needs to run at least 2 cycles before setting the index definition back to synchronous mode. Currently it is too eager to mark the status as done which confuses the _PropertyIndexAsyncReindex_ mbean into thinking the indexing is over and so skipping the final round that is supposed to do the switch back to sync mode.", "A_clean_title": ["non", "block", "reindex", "doesnt", "finish", "properlyth", "properli", "non", "block", "reindex", "need", "run", "at", "least", "cycl", "befor", "set", "index", "definit", "back", "synchron", "mode", "current", "it", "too", "eager", "mark", "statu", "as", "done", "which", "confus", "propertyindexasyncreindex", "properti", "index", "async", "reindex", "mbean", "into", "think", "index", "over", "so", "skip", "final", "round", "that", "suppos", "switch", "back", "sync", "mode"], "B_title": "Non-blocking reindexing doesnt finish properly", "B_clean_title": ["non", "block", "reindex", "doesnt", "finish", "properli"]},
{"A_title": "SimplexSolver returns unfeasible solutionThe SimplexSolver is returning an unfeasible solution:  import java.util.ArrayList; import java.text.DecimalFormat; import org.apache.commons.math.linear.ArrayRealVector; import org.apache.commons.math.optimization.GoalType; import org.apache.commons.math.optimization.OptimizationException; import org.apache.commons.math.optimization.linear.*;  public class SimplexSolverBug           public static void main(String args) throws OptimizationException                   LinearObjectiveFunction c = new LinearObjectiveFunction(new double0.0d 1.0d 1.0d 0.0d 0.0d 0.0d 0.0d 0.0d);                  ArrayList<LinearConstraint> cnsts = new ArrayList<LinearConstraint>(5);         LinearConstraint cnst;         cnst = new LinearConstraint(new double 1.0d -0.1d 0.0d 0.0d 0.0d 0.0d 0.0d Relationship.EQ -0.1d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double 1.0d 0.0d 0.0d 0.0d 0.0d 0.0d 0.0d Relationship.GEQ -1e-18d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double 0.0d 1.0d 0.0d 0.0d 0.0d 0.0d 0.0d Relationship.GEQ 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double 0.0d 0.0d 0.0d 1.0d 0.0d -0.0128588d 1e-5d Relationship.EQ 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double 0.0d 0.0d 0.0d 0.0d 1.0d 1e-5d -0.0128586d Relationship.EQ 1e-10d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double 0.0d 0.0d 1.0d -1.0d 0.0d 0.0d 0.0d Relationship.GEQ 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double 0.0d 0.0d 1.0d 1.0d 0.0d 0.0d 0.0d Relationship.GEQ 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double 0.0d 0.0d 1.0d 0.0d -1.0d 0.0d 0.0d Relationship.GEQ 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double 0.0d 0.0d 1.0d 0.0d 1.0d 0.0d 0.0d Relationship.GEQ 0.0d);         cnsts.add(cnst);                          DecimalFormat df = new java.text.DecimalFormat(0.#####E0);                  System.out.println(Constraints:);         for(LinearConstraint con : cnsts)              for (int i = 0; i < con.getCoefficients().getDimension(); ++i)                 System.out.print(df.format(con.getCoefficients().getData()i) +  );             System.out.println(con.getRelationship() +   + con.getValue());                           SimplexSolver simplex = new SimplexSolver(1e-7);         double sol = simplex.optimize(c cnsts GoalType.MINIMIZE false).getPointRef();         System.out.println(Solution:n + new ArrayRealVector(sol));         System.out.println(Second constraint is violated!);         Its an odd problem but something I ran across.  I tracked the problem to the getPivotRow routine in SimplexSolver.  It was choosing a pivot that resulted in a negative right-hand-side.  I recommend a fix by replacing                 ...                 if (MathUtils.equals(ratio minRatio epsilon))                  ... with                 ...                 if (MathUtils.equals(ratio minRatio Math.abs(epsilon/entry)))                  ...  I believe this would be more appropriate (and at least resolves this particular problem).  Also you may want to consider making a change in getPivotColumn to replace             ...             if (MathUtils.compareTo(tableau.getEntry(0 i) minValue epsilon) < 0)              ... with             ...             if (tableau.getEntry(0 i) < minValue)              ... because I dont see the point of biasing earlier columns when multiple entries are within epsilon of each other.  Why not pick the absolute smallest.  I dont know that any problem can result from doing it the other way but the latter may be a safer bet.  VERY IMPORTANT: I discovered another bug that occurs when not restricting to non-negatives.  In SimplexTableu::getSolution()            ...                     if (basicRows.contains(basicRow))                // if multiple variables can take a given value               // then we choose the first and set the rest equal to 0               coefficientsi = 0;           ... should be           ...                     if (basicRows.contains(basicRow))                // if multiple variables can take a given value               // then we choose the first and set the rest equal to 0               coefficientsi = (restrictToNonNegative ? 0 : -mostNegative);           ... If necessary I can give an example of where this bug causes a problem but it should be fairly obvious why this was wrong.", "A_clean_title": ["simplexsolv", "simplex", "solver", "return", "unfeas", "solutionth", "solut", "simplexsolv", "simplex", "solver", "return", "unfeas", "solut", "import", "java", "util", "arraylist", "array", "list", "import", "java", "text", "decimalformat", "decim", "format", "import", "org", "apach", "common", "math", "linear", "arrayrealvector", "array", "real", "vector", "import", "org", "apach", "common", "math", "optim", "goaltyp", "goal", "type", "import", "org", "apach", "common", "math", "optim", "optimizationexcept", "optim", "except", "import", "org", "apach", "common", "math", "optim", "linear", "public", "class", "simplexsolverbug", "simplex", "solver", "bug", "public", "static", "void", "main", "string", "arg", "throw", "optimizationexcept", "optim", "except", "linearobjectivefunct", "linear", "object", "function", "new", "linearobjectivefunct", "linear", "object", "function", "new", "double0", "0d", "0d", "0d", "0d", "0d", "0d", "0d", "0d", "arraylist", "array", "list", "linearconstraint", "linear", "constraint", "cnst", "new", "arraylist", "array", "list", "linearconstraint", "linear", "constraint", "linearconstraint", "linear", "constraint", "cnst", "cnst", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "0d", "1d", "0d", "0d", "0d", "0d", "0d", "relationship", "eq", "1d", "cnst", "add", "cnst", "cnst", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "0d", "0d", "0d", "0d", "0d", "0d", "0d", "relationship", "geq", "1e", "18d", "cnst", "add", "cnst", "cnst", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "0d", "0d", "0d", "0d", "0d", "0d", "0d", "relationship", "geq", "0d", "cnst", "add", "cnst", "cnst", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "0d", "0d", "0d", "0d", "0d", "0128588d", "1e", "5d", "relationship", "eq", "0d", "cnst", "add", "cnst", "cnst", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "0d", "0d", "0d", "0d", "0d", "1e", "5d", "0128586d", "relationship", "eq", "1e", "10d", "cnst", "add", "cnst", "cnst", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "0d", "0d", "0d", "0d", "0d", "0d", "0d", "relationship", "geq", "0d", "cnst", "add", "cnst", "cnst", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "0d", "0d", "0d", "0d", "0d", "0d", "0d", "relationship", "geq", "0d", "cnst", "add", "cnst", "cnst", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "0d", "0d", "0d", "0d", "0d", "0d", "0d", "relationship", "geq", "0d", "cnst", "add", "cnst", "cnst", "new", "linearconstraint", "linear", "constraint", "new", "doubl", "0d", "0d", "0d", "0d", "0d", "0d", "0d", "relationship", "geq", "0d", "cnst", "add", "cnst", "decimalformat", "decim", "format", "df", "new", "java", "text", "decimalformat", "decim", "format", "e0", "system", "out", "println", "constraint", "linearconstraint", "linear", "constraint", "con", "cnst", "int", "con", "getcoeffici", "get", "coeffici", "getdimens", "get", "dimens", "++i", "system", "out", "print", "df", "format", "con", "getcoeffici", "get", "coeffici", "getdata", "get", "data", "system", "out", "println", "con", "getrelationship", "get", "relationship", "con", "getvalu", "get", "valu", "simplexsolv", "simplex", "solver", "simplex", "new", "simplexsolv", "simplex", "solver", "1e", "doubl", "sol", "simplex", "optim", "cnst", "goaltyp", "minim", "goal", "type", "fals", "getpointref", "get", "point", "ref", "system", "out", "println", "solut", "new", "arrayrealvector", "array", "real", "vector", "sol", "system", "out", "println", "second", "constraint", "violat", "it", "odd", "problem", "but", "someth", "ran", "across", "track", "problem", "getpivotrow", "get", "pivot", "row", "routin", "simplexsolv", "simplex", "solver", "it", "wa", "choos", "pivot", "that", "result", "neg", "right", "hand", "side", "recommend", "fix", "by", "replac", "mathutil", "equal", "math", "util", "ratio", "minratio", "min", "ratio", "epsilon", "mathutil", "equal", "math", "util", "ratio", "minratio", "min", "ratio", "math", "ab", "epsilon", "entri", "believ", "thi", "would", "more", "appropri", "at", "least", "resolv", "thi", "particular", "problem", "also", "you", "may", "want", "consid", "make", "chang", "getpivotcolumn", "get", "pivot", "column", "replac", "mathutil", "compareto", "math", "util", "compar", "tableau", "getentri", "get", "entri", "minvalu", "min", "valu", "epsilon", "tableau", "getentri", "get", "entri", "minvalu", "min", "valu", "becaus", "dont", "see", "point", "bias", "earlier", "column", "when", "multipl", "entri", "are", "within", "epsilon", "each", "other", "whi", "not", "pick", "absolut", "smallest", "dont", "know", "that", "ani", "problem", "result", "do", "it", "other", "way", "but", "latter", "may", "safer", "bet", "veri", "import", "discov", "anoth", "bug", "that", "occur", "when", "not", "restrict", "non", "neg", "simplextableu", "simplex", "tableu", ":getsolut", ":get", "solut", "basicrow", "contain", "basic", "row", "basicrow", "basic", "row", "multipl", "variabl", "take", "given", "valu", "then", "we", "choos", "first", "set", "rest", "equal", "coefficientsi", "basicrow", "contain", "basic", "row", "basicrow", "basic", "row", "multipl", "variabl", "take", "given", "valu", "then", "we", "choos", "first", "set", "rest", "equal", "coefficientsi", "restricttononneg", "restrict", "non", "neg", "mostneg", "most", "neg", "necessari", "give", "exampl", "where", "thi", "bug", "caus", "problem", "but", "it", "fairli", "obviou", "whi", "thi", "wa", "wrong"], "B_title": "Fixed two errors in simplex solver when entries are close together or when variables are not restricted to non-negative.", "B_clean_title": ["fix", "two", "error", "simplex", "solver", "when", "entri", "are", "close", "togeth", "or", "when", "variabl", "are", "not", "restrict", "non", "neg"]},
{"A_title": "Validity checks missing for readFields and Thrift deserializationClasses in o.a.a.core.data (and potentially elsewhere) that support construction from a Thrift object and/or population from a DataInput (via a readFields() method) often lack data validity checks that the classes constructors enforce. The missing checks make it possible for an attacker to create invalid objects by manipulating the bytes being read. The situation is analogous to the need to check objects deserialized from their Java serialized form within the readObject() method.", "A_clean_title": ["valid", "check", "miss", "readfield", "read", "field", "thrift", "deserializationclass", "deseri", "class", "core", "data", "potenti", "elsewher", "that", "support", "construct", "thrift", "object", "or", "popul", "datainput", "data", "input", "via", "readfield", "read", "field", "method", "often", "lack", "data", "valid", "check", "that", "class", "constructor", "enforc", "miss", "check", "make", "it", "possibl", "attack", "creat", "invalid", "object", "by", "manipul", "byte", "be", "read", "situat", "analog", "need", "check", "object", "deseri", "their", "java", "serial", "form", "within", "readobject", "read", "object", "method"], "B_title": "merge failed to pull null pointer checks from 1.4", "B_clean_title": ["merg", "fail", "pull", "null", "pointer", "check"]},
{"A_title": "No Object Id found for an instance when using @ConstructorPropertiesHi! We recently migrated from 2.4.6. to 2.8.1. and we encountered the issue. We use lomboks  @AllArgsConstructor which adds @ConstructorProperties to constructor. We also used @JsonIdentityInfo on our POJO which lead to JsonMappingException: No Object Id found for an instance exception. The following test code demonstrates the issue   Stack trace   Prior to the version 2.5.0 this was not an issue because the offending method (  com.fasterxml.jackson.databind.deser.impl.PropertyValueBuffer.handleIdValue(DeserializationContext Object) ) had a comment // TODO: is this an error case? and did nothing else.", "A_clean_title": ["no", "object", "id", "found", "instanc", "when", "constructorpropertieshi", "constructor", "properti", "hi", "we", "recent", "migrat", "we", "encount", "issu", "we", "use", "lombok", "allargsconstructor", "all", "arg", "constructor", "which", "add", "constructorproperti", "constructor", "properti", "constructor", "we", "also", "use", "jsonidentityinfo", "json", "ident", "info", "our", "pojo", "which", "lead", "jsonmappingexcept", "json", "map", "except", "no", "object", "id", "found", "instanc", "except", "follow", "test", "code", "demonstr", "issu", "stack", "trace", "prior", "version", "thi", "wa", "not", "issu", "becaus", "offend", "method", "com", "fasterxml", "jackson", "databind", "deser", "impl", "propertyvaluebuff", "handleidvalu", "properti", "valu", "buffer", "handl", "id", "valu", "deserializationcontext", "deseri", "context", "object", "had", "comment", "todo", "thi", "error", "case", "did", "noth"], "B_title": "Fixed #1367", "B_clean_title": ["fix", "1367"]},
{"A_title": "Simple versionable nodes are invalid after migrationOAK-3836 introduces a support for migrating mix:simpleVersionable nodes from JCR2 to mix:versionable nodes in Oak. It changes the mixin type however it doesnt add required properties: jcr:versionHistory jcr:baseVersion and jcr:predecessors. As a result versioning-related methods invoked on such nodes doesnt work correctly.", "A_clean_title": ["simpl", "version", "node", "are", "invalid", "after", "migrationoak", "3836", "migrat", "oak", "introduc", "support", "migrat", "mix", "simpleversion", "simpl", "version", "node", "jcr2", "mix", "version", "node", "oak", "it", "chang", "mixin", "type", "howev", "it", "doesnt", "add", "requir", "properti", "jcr", "versionhistori", "version", "histori", "jcr", "basevers", "base", "version", "jcr", "predecessor", "as", "result", "version", "relat", "method", "invok", "such", "node", "doesnt", "work", "correctli"], "B_title": "Fixed version-related properties for the simple versionable nodes", "B_clean_title": ["fix", "version", "relat", "properti", "simpl", "version", "node"]},
{"A_title": "Mockito cant create mock on public class that extends package-private classEven if it cant be implemented I think that mockito should throw some normal exception at time of creation. In my variant on first creation it returns wrong-working mock (invokes real method instead of stubbed). On second creation throws exception that doesnt really connected with problem. Everything works fine if you mock package-private parent.", "A_clean_title": ["mockito", "cant", "creat", "mock", "public", "class", "that", "extend", "packag", "privat", "classeven", "class", "even", "it", "cant", "implement", "think", "that", "mockito", "throw", "some", "normal", "except", "at", "time", "creation", "my", "variant", "first", "creation", "it", "return", "wrong", "work", "mock", "invok", "real", "method", "instead", "stub", "second", "creation", "throw", "except", "that", "doesnt", "realli", "connect", "problem", "everyth", "work", "fine", "you", "mock", "packag", "privat", "parent"], "B_title": "Fixed issue 216 @Spy did not have nice names in the verification errors", "B_clean_title": ["fix", "issu", "216", "spi", "did", "not", "have", "nice", "name", "verif", "error"]},
{"A_title": "Binaries might get removed by garbage collection while still referencedThe Microkernel contract|http://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-mk-api/src/main/java/org/apache/jackrabbit/mk/api/MicroKernel.java specifies a specific format for references to binaries: :blobId:<blobId>. Currently oak-core uses a different format and thus risks premature garbage collection of such binaries.", "A_clean_title": ["binari", "might", "get", "remov", "by", "garbag", "collect", "while", "still", "referencedth", "referenc", "microkernel", "contract|http", "mk", "apach", "java", "svn", "org", "repo", "asf", "jackrabbit", "oak", "trunk", "oak", "api", "src", "main", "java", "org", "apach", "jackrabbit", "mk", "api", "microkernel", "micro", "kernel", "specifi", "specif", "format", "refer", "binari", "blobid", "blob", "id", "blobid", "blob", "id", "current", "oak", "core", "use", "differ", "format", "thu", "risk", "prematur", "garbag", "collect", "such", "binari"], "B_title": "Binaries might get removed by garbage collection while still referenced change prefix for binaries from bin: to :blobId:", "B_clean_title": ["binari", "might", "get", "remov", "by", "garbag", "collect", "while", "still", "referenc", "chang", "prefix", "binari", "bin", "blobid", "blob", "id"]},
{"A_title": "WebApplication doesnt recognize if an incoming request is multipart.Thanks to the mail at http://apache-wicket.1842946.n4.nabble.com/Read-POST-based-request-from-external-site-td4651269.html we have spotted a problem with method  newWebRequest of class WebApplication.  It seems that this method doesnt test if the original request is multipart and doing so post parameters go lost.  We should create a  MultipartServletWebRequestImpl when such a type of request is being served. I attach a possible patch but Im not 100% about two things: - which is the best way to determinate if a HttpServletRequest is multipart? - in order to build a MultipartServletWebRequestImpl we need to provide a string identifier for the upload.   How can we generate it (in my patch its a constant value)?", "A_clean_title": ["webappl", "web", "applic", "doesnt", "recogn", "incom", "request", "multipart", "thank", "mail", "at", "http", "post", "base", "request", "extern", "site", "apach", "wicket", "1842946", "n4", "nabbl", "td4651269", "html", "com", "read", "we", "have", "spot", "problem", "method", "newwebrequest", "new", "web", "request", "class", "webappl", "web", "applic", "it", "seem", "that", "thi", "method", "doesnt", "test", "origin", "request", "multipart", "do", "so", "post", "paramet", "go", "lost", "we", "creat", "multipartservletwebrequestimpl", "multipart", "servlet", "web", "request", "impl", "when", "such", "type", "request", "be", "serv", "attach", "possibl", "patch", "but", "im", "not", "100", "about", "two", "thing", "which", "best", "way", "determin", "httpservletrequest", "http", "servlet", "request", "multipart", "order", "build", "multipartservletwebrequestimpl", "multipart", "servlet", "web", "request", "impl", "we", "need", "provid", "string", "identifi", "upload", "how", "we", "gener", "it", "my", "patch", "it", "constant", "valu"], "B_title": "WebApplication doesnt recognize if an incoming request is multipart.", "B_clean_title": ["webappl", "web", "applic", "doesnt", "recogn", "incom", "request", "multipart"]},
{"A_title": "ClassCastExecptionHi  I have started using a meitrack tc68s device however when I try and generate a summary/trip report it returns the following error in console:  The device is not in the future and has the correct timezone also I am using the official build. Any recommendation?  All the other osram works fine.", "A_clean_title": ["classcastexecptionhi", "class", "cast", "execpt", "hi", "have", "start", "meitrack", "tc68", "devic", "howev", "when", "tri", "gener", "summari", "trip", "report", "it", "return", "follow", "error", "consol", "devic", "not", "futur", "ha", "correct", "timezon", "also", "am", "offici", "build", "ani", "recommend", "all", "other", "osram", "work", "fine"], "B_title": "Ensure that odometer is a number (fix #2967)", "B_clean_title": ["ensur", "that", "odomet", "number", "fix", "2967"]},
{"A_title": "Queueing component in autocomponentThere is an exception when a component is added to queue when its parent is an auto component  <body> <a href=panier.html> <span wicket:id=inlink></span> </a> </body>   Last cause: Unable to find component with id inlink in TransparentWebMarkupContainer Component id = wicket_relative_path_prefix_1 Expected: wicket_relative_path_prefix_1:inlink. Found with similar names:", "A_clean_title": ["queue", "compon", "autocomponentther", "autocompon", "there", "except", "when", "compon", "ad", "queue", "when", "it", "parent", "auto", "compon", "bodi", "href=pani", "html", "span", "wicket", "id=inlink", "span", "bodi", "last", "caus", "unabl", "find", "compon", "id", "inlink", "transparentwebmarkupcontain", "transpar", "web", "markup", "contain", "compon", "id", "wicket", "rel", "path", "prefix", "expect", "wicket", "rel", "path", "prefix", "inlink", "found", "similar", "name"], "B_title": "Queueing component in autocomponent", "B_clean_title": ["queue", "compon", "autocompon"]},
{"A_title": "LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to itLevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it. This makes it hard to specify custom stopping criteria for the optimizer.", "A_clean_title": ["levenbergmarquardtoptim", "levenberg", "marquardt", "optim", "ignor", "vectorialconvergencecheck", "vectori", "converg", "checker", "paramet", "pass", "itlevenbergmarquardtoptim", "it", "levenberg", "marquardt", "optim", "ignor", "vectorialconvergencecheck", "vectori", "converg", "checker", "paramet", "pass", "it", "thi", "make", "it", "hard", "specifi", "custom", "stop", "criteria", "optim"], "B_title": "Fixed Levenberg-Marquardt optimizer that did not use the vectorial convergence checker. Now this optimizer can use either the general vectorial convergence checker or its own specialized convergence settings. Minor changes had to be introduced in the test data they have been validated JIRA: MATH-362", "B_clean_title": ["fix", "levenberg", "marquardt", "optim", "that", "did", "not", "use", "vectori", "converg", "checker", "now", "thi", "optim", "use", "either", "gener", "vectori", "converg", "checker", "or", "it", "own", "special", "converg", "set", "minor", "chang", "had", "introduc", "test", "data", "they", "have", "been", "valid", "jira", "math", "362"]},
{"A_title": "Simple Whitespace only compression removing each keyword from for each (var x in arr) loopNone", "A_clean_title": ["simpl", "whitespac", "onli", "compress", "remov", "each", "keyword", "each", "var", "arr", "loopnon", "loop", "none"], "B_title": "Dont silently ignore transform for each into for loops report an error instead. Fixes issue 644", "B_clean_title": ["dont", "silent", "ignor", "transform", "each", "into", "loop", "report", "error", "instead", "fix", "issu", "644"]},
{"A_title": "ProxyServer ignores value of isDeleted on ColumnUpdateThe ProxyServer ignores the actual boolean value of the isDeleted flag on a ColumnUpdate.  If the isDeleted value is set regardless of the actual boolean value the ProxyServer marks the update as a delete.  The ProxyServer should be updated to check the value of the flag.", "A_clean_title": ["proxyserv", "proxi", "server", "ignor", "valu", "isdelet", "delet", "columnupdateth", "column", "updat", "proxyserv", "proxi", "server", "ignor", "actual", "boolean", "valu", "isdelet", "delet", "flag", "columnupd", "column", "updat", "isdelet", "delet", "valu", "set", "regardless", "actual", "boolean", "valu", "proxyserv", "proxi", "server", "mark", "updat", "as", "delet", "proxyserv", "proxi", "server", "updat", "check", "valu", "flag"], "B_title": "use the value of the delete flag (with test)", "B_clean_title": ["use", "valu", "delet", "flag", "test"]},
{"A_title": "RETURNS_DEEP_STUBS automatically tries to create serializable mocksYou are using the setting withSettings().serializable() however the type you are trying to mock NotSerializableReturnValue do not implement Serializable AND do not have a no-arg constructor.", "A_clean_title": ["return", "deep", "stub", "automat", "tri", "creat", "serializ", "mocksyou", "mock", "you", "are", "set", "withset", "set", "serializ", "howev", "type", "you", "are", "tri", "mock", "notserializablereturnvalu", "not", "serializ", "return", "valu", "not", "implement", "serializ", "not", "have", "no", "arg", "constructor"], "B_title": "Merge pull request #103 from mockito/fixes-issue-99", "B_clean_title": ["merg", "pull", "request", "103", "issu", "99", "mockito", "fix"]},
{"A_title": "Query: unexpected result on negative limit / offsetCurrently running a query with limit of -1 never returns any rows the same as when using limit = 0.  Either the query engine should fail with a negative limit or offset (IllegalArgumentException) or it should ignore negative values (unlimited result rows for limit probably no offset for offset = -1).  I would prefer IllegalArgumentException but I can also live with -1 = unlimited at least for limit.", "A_clean_title": ["queri", "unexpect", "result", "neg", "limit", "offsetcurr", "offset", "current", "run", "queri", "limit", "never", "return", "ani", "row", "same", "as", "when", "limit", "either", "queri", "engin", "fail", "neg", "limit", "or", "offset", "illegalargumentexcept", "illeg", "argument", "except", "or", "it", "ignor", "neg", "valu", "unlimit", "result", "row", "limit", "probabl", "no", "offset", "offset", "would", "prefer", "illegalargumentexcept", "illeg", "argument", "except", "but", "also", "live", "unlimit", "at", "least", "limit"], "B_title": "Query: unexpected result on negative limit / offset", "B_clean_title": ["queri", "unexpect", "result", "neg", "limit", "offset"]},
{"A_title": "Classify non-rightmost expressions as problematicNone", "A_clean_title": ["classifi", "non", "rightmost", "express", "as", "problematicnon", "problemat", "none"], "B_title": "You are in a maze of twisty if branches all alike. cleanup the logic for identifying dead expressions. Fixes issue 753", "B_clean_title": ["you", "are", "maze", "twisti", "branch", "all", "alik", "cleanup", "logic", "identifi", "dead", "express", "fix", "issu", "753"]},
{"A_title": "IllegalArgumentException on Row.getValues()Calling row.getValues() is throwing an IllegalArgumentException when called on the QueryResult of the query SELECT properties FROM nt:base WHERE sling:resourceType=cq/personalization/components/contextstores/surferinfo  quote java.lang.IllegalArgumentException at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76) at org.apache.jackrabbit.oak.plugins.value.ValueImpl.checkSingleValued(ValueImpl.java:85) at org.apache.jackrabbit.oak.plugins.value.ValueImpl.<init>(ValueImpl.java:72) at org.apache.jackrabbit.oak.plugins.value.ValueFactoryImpl.createValue(ValueFactoryImpl.java:95) at org.apache.jackrabbit.oak.jcr.query.QueryResultImpl.createValue(QueryResultImpl.java:266) at org.apache.jackrabbit.oak.jcr.query.RowImpl.getValues(RowImpl.java:99) at com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.getListProperty(FrameworkComponentImpl.java:128) at com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.<init>(FrameworkComponentImpl.java:91) quote", "A_clean_title": ["illegalargumentexcept", "illeg", "argument", "except", "row", "getvalu", "get", "valu", "call", "row", "getvalu", "get", "valu", "throw", "illegalargumentexcept", "illeg", "argument", "except", "when", "call", "queryresult", "queri", "result", "queri", "select", "properti", "nt", "base", "where", "sling", "resourcetype=cq", "person", "compon", "contextstor", "surferinfo", "resourc", "type=cq", "quot", "java", "lang", "illegalargumentexcept", "illeg", "argument", "except", "at", "com", "googl", "common", "base", "precondit", "checkargu", "check", "argument", "precondit", "java:76", "at", "org", "apach", "jackrabbit", "oak", "plugin", "valu", "valueimpl", "checksinglevalu", "valu", "impl", "check", "singl", "valu", "valueimpl", "java:85", "valu", "impl", "at", "org", "apach", "jackrabbit", "oak", "plugin", "valu", "valueimpl", "valu", "impl", "init", "valueimpl", "java:72", "valu", "impl", "at", "org", "apach", "jackrabbit", "oak", "plugin", "valu", "valuefactoryimpl", "createvalu", "valu", "factori", "impl", "creat", "valu", "valuefactoryimpl", "java:95", "valu", "factori", "impl", "at", "org", "apach", "jackrabbit", "oak", "jcr", "queri", "queryresultimpl", "createvalu", "queri", "result", "impl", "creat", "valu", "queryresultimpl", "java:266", "queri", "result", "impl", "at", "org", "apach", "jackrabbit", "oak", "jcr", "queri", "rowimpl", "getvalu", "row", "impl", "get", "valu", "rowimpl", "java:99", "row", "impl", "at", "com", "day", "cq", "analyt", "sitecatalyst", "impl", "frameworkcomponentimpl", "getlistproperti", "framework", "compon", "impl", "get", "list", "properti", "frameworkcomponentimpl", "java:128", "framework", "compon", "impl", "at", "com", "day", "cq", "analyt", "sitecatalyst", "impl", "frameworkcomponentimpl", "framework", "compon", "impl", "init", "frameworkcomponentimpl", "java:91", "framework", "compon", "impl", "quot"], "B_title": "IllegalArgumentException on Row.getValues()", "B_clean_title": ["illegalargumentexcept", "illeg", "argument", "except", "row", "getvalu", "get", "valu"]},
{"A_title": "ajax not working due to bugs in resource handlingA couple of bugs were found that were preventing .js resources to be returned to the client correctly. One bug was returning the jar file size as the content length of the resource if it is in a jar file. The other was copying past a source buffer into the response.  After fixing these bugs the ajax functions in the trunk seems to be working.  A patch is provided. Test cases included.", "A_clean_title": ["ajax", "not", "work", "due", "bug", "resourc", "handlinga", "handl", "coupl", "bug", "were", "found", "that", "were", "prevent", "js", "resourc", "return", "client", "correctli", "one", "bug", "wa", "return", "jar", "file", "size", "as", "content", "length", "resourc", "it", "jar", "file", "other", "wa", "copi", "past", "sourc", "buffer", "into", "respons", "after", "fix", "these", "bug", "ajax", "function", "trunk", "seem", "work", "patch", "provid", "test", "case", "includ"], "B_title": "ajax not working due to bugs in resource handling Issue: WICKET-2839", "B_clean_title": ["ajax", "not", "work", "due", "bug", "resourc", "handl", "issu", "wicket", "2839"]},
{"A_title": "getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries. The current implementation in ArrayRealVector has a typo:      public double getLInfNorm()          double max = 0;         for (double a : data)              max += Math.max(max Math.abs(a));                  return max;        the += should just be an =. There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test not a test for correctness). Worse the implementation in OpenMapRealVector is not even positive semi-definite:          public double getLInfNorm()          double max = 0;         Iterator iter = entries.iterator();         while (iter.hasNext())              iter.advance();             max += iter.value();                  return max;        I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():    public double getLInfNorm()      double norm = 0;     Iterator<Entry> it = sparseIterator();     Entry e;     while(it.hasNext() && (e = it.next()) != null)        norm = Math.max(norm Math.abs(e.getValue()));          return norm;      Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.", "A_clean_title": ["getlinfnorm", "get", "inf", "norm", "use", "wrong", "formula", "both", "arrayrealvector", "array", "real", "vector", "openmaprealvector", "open", "map", "real", "vector", "differ", "way", "infin", "norm", "finit", "dimension", "vector", "just", "max", "absolut", "valu", "it", "entri", "current", "implement", "arrayrealvector", "array", "real", "vector", "ha", "typo", "public", "doubl", "getlinfnorm", "get", "inf", "norm", "doubl", "max", "doubl", "data", "max", "math", "max", "max", "math", "ab", "return", "max", "just", "there", "sadli", "unit", "test", "assur", "us", "that", "thi", "correct", "behavior", "effect", "regress", "onli", "test", "not", "test", "correct", "wors", "implement", "openmaprealvector", "open", "map", "real", "vector", "not", "even", "posit", "semi", "definit", "public", "doubl", "getlinfnorm", "get", "inf", "norm", "doubl", "max", "iter", "iter", "entri", "iter", "while", "iter", "hasnext", "ha", "next", "iter", "advanc", "max", "iter", "valu", "return", "max", "would", "suggest", "that", "thi", "method", "move", "up", "abstractrealvector", "abstract", "real", "vector", "superclass", "implement", "sparseiter", "spars", "iter", "public", "doubl", "getlinfnorm", "get", "inf", "norm", "doubl", "norm", "iter", "entri", "it", "sparseiter", "spars", "iter", "entri", "while", "it", "hasnext", "ha", "next", "it", "next", "null", "norm", "math", "max", "norm", "math", "ab", "getvalu", "get", "valu", "return", "norm", "unit", "test", "neg", "valu", "vector", "would", "help", "check", "thi", "kind", "thing", "futur"], "B_title": "fixed a wrong implementation of the Linf vector norm JIRA: MATH-326", "B_clean_title": ["fix", "wrong", "implement", "linf", "vector", "norm", "jira", "math", "326"]},
{"A_title": "Components markup cannot be found in Ajax requests if the parent is transparentWhen TransparentWebMarkupContainer is used an inner markup container cannot find its markup on Ajax updates. The problem seems to be caused by the fact that ComponentResolvers#resolve() is not executed and since there is transparent container involved Markup.find(String) cannot find the markup for non-transparent markup containers. Ill commit a disabled test case that shows the problem.", "A_clean_title": ["compon", "markup", "not", "found", "ajax", "request", "parent", "transparentwhen", "transpar", "when", "transparentwebmarkupcontain", "transpar", "web", "markup", "contain", "use", "inner", "markup", "contain", "not", "find", "it", "markup", "ajax", "updat", "problem", "seem", "caus", "by", "fact", "that", "componentresolv", "compon", "resolv", "resolv", "not", "execut", "sinc", "there", "transpar", "contain", "involv", "markup", "find", "string", "not", "find", "markup", "non", "transpar", "markup", "contain", "ill", "commit", "disabl", "test", "case", "that", "show", "problem"], "B_title": "fixed: Components markup cannot be found in Ajax requests if the parent is transparent Issue: WICKET-3719", "B_clean_title": ["fix", "compon", "markup", "not", "found", "ajax", "request", "parent", "transpar", "issu", "wicket", "3719"]},
{"A_title": "BOM in UTF markup file breaks encoding detectionI have project with internationalization and experienced this problem with one of the pages with non-english content. Page had UTF-8 encoding but my JVM encoding is different. I always use <?xml encoding ... ?> to specify encoding for markup pages (and MarkupSettings.defaultMarkupEncoding is not set).  Unexpectedly I got problem with bad encoding on page. After several hours of debugging I found what source of this issue was UTF BOM (Byte order mark) at the beggining of file and inability of XmlReader to process it. XmlReader.getXmlDeclaration tries to match xml declaration with regular expression but fails because of BOM. After that encoding defaults to JVM encoding.  Its possible to use org.apache.commons.io.input.BOMInputStream to handle BOM or you could handle it manually inside XmlReader.  PS: issue found with Wicket 1.5.10 and I see same code in 6.12.0 without BOM handling so I added it to Affects Version/s but no proof-in-code available from me at this moment.", "A_clean_title": ["bom", "utf", "markup", "file", "break", "encod", "detectioni", "detect", "have", "project", "internation", "experienc", "thi", "problem", "one", "page", "non", "english", "content", "page", "had", "utf", "encod", "but", "my", "jvm", "encod", "differ", "alway", "use", "xml", "encod", "specifi", "encod", "markup", "page", "markupset", "defaultmarkupencod", "markup", "set", "default", "markup", "encod", "not", "set", "unexpectedli", "got", "problem", "bad", "encod", "page", "after", "sever", "hour", "debug", "found", "what", "sourc", "thi", "issu", "wa", "utf", "bom", "byte", "order", "mark", "at", "beggin", "file", "inabl", "xmlreader", "xml", "reader", "process", "it", "xmlreader", "getxmldeclar", "xml", "reader", "get", "xml", "declar", "tri", "match", "xml", "declar", "regular", "express", "but", "fail", "becaus", "bom", "after", "that", "encod", "default", "jvm", "encod", "it", "possibl", "use", "org", "apach", "common", "io", "input", "bominputstream", "bom", "input", "stream", "handl", "bom", "or", "you", "could", "handl", "it", "manual", "insid", "xmlreader", "xml", "reader", "ps", "issu", "found", "wicket", "10", "see", "same", "code", "12", "without", "bom", "handl", "so", "ad", "it", "affect", "version", "but", "no", "proof", "code", "avail", "me", "at", "thi", "moment"], "B_title": "BOM in UTF markup file breaks encoding detection", "B_clean_title": ["bom", "utf", "markup", "file", "break", "encod", "detect"]},
{"A_title": "SQL2 query with union limit and offset can return invalid resultswhen using order limit and offset and a SQL2 query that contains an union of two subqueries that have common results can return invalid results  Example: assuming content tree /test/a/b/c/d/e exists code:sql SELECT jcr:path FROM nt:base AS a WHERE ISDESCENDANTNODE(a /test) UNION SELECT jcr:path FROM nt:base AS a WHERE ISDESCENDANTNODE(a /test) ORDER BY jcr:path code with limit=3 and offset 2 returns only one row ( instead of 3 )  the correct result set is noformat /test/a/b/c /test/a/b/c/d /test/a/b/c/d/e noformat", "A_clean_title": ["sql2", "queri", "union", "limit", "offset", "return", "invalid", "resultswhen", "order", "limit", "offset", "sql2", "queri", "that", "contain", "union", "two", "subqueri", "that", "have", "common", "result", "return", "invalid", "result", "exampl", "assum", "content", "tree", "test", "exist", "code", "sql", "select", "jcr", "path", "nt", "base", "as", "where", "isdescendantnod", "test", "union", "select", "jcr", "path", "nt", "base", "as", "where", "isdescendantnod", "test", "order", "by", "jcr", "path", "code", "limit=3", "offset", "return", "onli", "one", "row", "instead", "correct", "result", "set", "noformat", "test", "test", "test", "noformat"], "B_title": "SQL2 query with union limit and offset can return invalid results", "B_clean_title": ["sql2", "queri", "union", "limit", "offset", "return", "invalid", "result"]},
{"A_title": "Optimisation: convert array.join() to array.join()None", "A_clean_title": ["optimis", "convert", "array", "join", "array", "join", "none"], "B_title": ".join() --> .join() Fixes issue 558", "B_clean_title": ["join", "join", "fix", "issu", "558"]},
{"A_title": "adding (and querying) feedback messages at construction time fails.See http://www.nabble.com/error%28...%29-No-page-found-for-component-tf3497125.html  Currently adding (and querying) feedback messages fails whenever it is done on components that are not yet added to a page (or were removed from them due to component replacement).  There are two ways to fix this. The first fix is attached as a patch and basically uses a thread local to temporarily store the messages and distribute them to the relevant page instances just in time or when rendering starts. The advantage of this method is that it is completely back wards compatible.  The other way to fix this is to store all messages whether component specific or not in the session and pull them from there. We need to be careful about how/ when to clean these error messages up though. We can use this issue to think about it a little bit more.", "A_clean_title": ["ad", "queri", "feedback", "messag", "at", "construct", "time", "fail", "see", "http", "nabbl", "www", "com", "error", "28", "29", "no", "page", "found", "compon", "tf3497125", "html", "current", "ad", "queri", "feedback", "messag", "fail", "whenev", "it", "done", "compon", "that", "are", "not", "yet", "ad", "page", "or", "were", "remov", "them", "due", "compon", "replac", "there", "are", "two", "way", "fix", "thi", "first", "fix", "attach", "as", "patch", "basic", "use", "thread", "local", "temporarili", "store", "messag", "distribut", "them", "relev", "page", "instanc", "just", "time", "or", "when", "render", "start", "advantag", "thi", "method", "that", "it", "complet", "back", "ward", "compat", "other", "way", "fix", "thi", "store", "all", "messag", "whether", "compon", "specif", "or", "not", "session", "pull", "them", "there", "we", "need", "care", "about", "how", "when", "clean", "these", "error", "messag", "up", "though", "we", "use", "thi", "issu", "think", "about", "it", "littl", "bit", "more"], "B_title": "(with aggressive cleanup but with the option to override)", "B_clean_title": ["aggress", "cleanup", "but", "option", "overrid"]},
{"A_title": "HtmlHandler wrongly handles tags not requiring closed tags if the markup does not have top level tagHi   I have custom component (extends MarkupContainer implements IMarkupCacheKeyProvider IMarkupResourceStreamProvider) which fetches its HTML markup from database.  Following HTML markup:   <img alt= src=logo.png>  <br>Some text  <br>Some more text   causes following error:   2012-04-12 10:52:53012 http-8080-6 ERROR: Unexpected error occurred  Unable to find close tag for: <img alt=logo src=logo.png> in org.apache.wicket.util.resource.StringResourceStream@3d7e16fc   MarkupStream: unknown          at org.apache.wicket.markup.MarkupFragment.<init>(MarkupFragment.java:127)          at org.apache.wicket.markup.MarkupStream.getMarkupFragment(MarkupStream.java:485)          at org.apache.wicket.MarkupContainer.autoAdd(MarkupContainer.java:244)          at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1421)          at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1596)          at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1571)          at org.apache.wicket.MarkupContainer.onComponentTagBody(MarkupContainer.java:1525)   I think the problem is that org.apache.wicket.markup.parser.filter.HtmlHandler does not handle such markup correctly. It does not call ComponentTag.setHasNoCloseTag(true) for the img tag. Such call is missing in postProcess() method. I think that this problem can be fixed by inserting:   top.setHasNoCloseTag(true);   after line 80 in HtmlHandler.java file.    Michal", "A_clean_title": ["htmlhandler", "html", "handler", "wrongli", "handl", "tag", "not", "requir", "close", "tag", "markup", "not", "have", "top", "level", "taghi", "tag", "hi", "have", "custom", "compon", "extend", "markupcontain", "markup", "contain", "implement", "imarkupcachekeyprovid", "markup", "cach", "key", "provid", "imarkupresourcestreamprovid", "markup", "resourc", "stream", "provid", "which", "fetch", "it", "html", "markup", "databas", "follow", "html", "markup", "img", "alt=", "src=logo", "png", "br", "some", "text", "br", "some", "more", "text", "caus", "follow", "error", "2012", "04", "12", "10:52:53012", "http", "8080", "error", "unexpect", "error", "occur", "unabl", "find", "close", "tag", "img", "alt=logo", "src=logo", "png", "org", "apach", "wicket", "util", "resourc", "stringresourcestream", "string", "resourc", "stream", "3d7e16fc", "markupstream", "markup", "stream", "unknown", "at", "org", "apach", "wicket", "markup", "markupfrag", "markup", "fragment", "init", "markupfrag", "java:127", "markup", "fragment", "at", "org", "apach", "wicket", "markup", "markupstream", "getmarkupfrag", "markup", "stream", "get", "markup", "fragment", "markupstream", "java:485", "markup", "stream", "at", "org", "apach", "wicket", "markupcontain", "autoadd", "markup", "contain", "auto", "add", "markupcontain", "java:244", "markup", "contain", "at", "org", "apach", "wicket", "markupcontain", "rendernext", "markup", "contain", "render", "next", "markupcontain", "java:1421", "markup", "contain", "at", "org", "apach", "wicket", "markupcontain", "renderal", "markup", "contain", "render", "all", "markupcontain", "java:1596", "markup", "contain", "at", "org", "apach", "wicket", "markupcontain", "rendercomponenttagbodi", "markup", "contain", "render", "compon", "tag", "bodi", "markupcontain", "java:1571", "markup", "contain", "at", "org", "apach", "wicket", "markupcontain", "oncomponenttagbodi", "markup", "contain", "compon", "tag", "bodi", "markupcontain", "java:1525", "markup", "contain", "think", "problem", "that", "org", "apach", "wicket", "markup", "parser", "filter", "htmlhandler", "html", "handler", "not", "handl", "such", "markup", "correctli", "it", "not", "call", "componenttag", "sethasnoclosetag", "compon", "tag", "set", "ha", "no", "close", "tag", "true", "img", "tag", "such", "call", "miss", "postprocess", "post", "process", "method", "think", "that", "thi", "problem", "fix", "by", "insert", "top", "sethasnoclosetag", "set", "ha", "no", "close", "tag", "true", "after", "line", "80", "htmlhandler", "java", "html", "handler", "file", "michal"], "B_title": "HtmlHandler wrongly handles tags not requiring closed tags if the markup does not have top level tag", "B_clean_title": ["htmlhandler", "html", "handler", "wrongli", "handl", "tag", "not", "requir", "close", "tag", "markup", "not", "have", "top", "level", "tag"]},
{"A_title": "Form Input example fails when changing the languageTrying to change the language of http://localhost:8080/forminput example fails with:   Caused by: java.lang.ArrayIndexOutOfBoundsException: -1 at java.util.ArrayList.remove(ArrayList.java:390) at org.apache.wicket.request.Url.resolveRelative(Url.java:884) at org.apache.wicket.markup.html.form.Form.dispatchEvent(Form.java:1028) at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:699) at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:670) ... 37 more", "A_clean_title": ["form", "input", "exampl", "fail", "when", "chang", "languagetri", "languag", "tri", "chang", "languag", "http", "localhost:8080", "forminput", "exampl", "fail", "caus", "by", "java", "lang", "arrayindexoutofboundsexcept", "array", "index", "out", "bound", "except", "at", "java", "util", "arraylist", "remov", "array", "list", "arraylist", "java:390", "array", "list", "at", "org", "apach", "wicket", "request", "url", "resolverel", "resolv", "rel", "url", "java:884", "at", "org", "apach", "wicket", "markup", "html", "form", "form", "dispatchev", "dispatch", "event", "form", "java:1028", "at", "org", "apach", "wicket", "markup", "html", "form", "form", "onformsubmit", "form", "submit", "form", "java:699", "at", "org", "apach", "wicket", "markup", "html", "form", "form", "onformsubmit", "form", "submit", "form", "java:670", "37", "more"], "B_title": "", "B_clean_title": []},
{"A_title": "PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilonSimilar to the issue described in MATH-201 using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that. In MATH-201 the problem was described as such: > So in essence the p-value returned by TTestImpl.tTest() is: >  > 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t)) >  > For large-ish t-statistics cumulativeProbabilty(-t) can get quite small and cumulativeProbabilty(t) can get very close to 1.0. When  > cumulativeProbability(-t) is less than the machine epsilon we get p-values equal to zero because: >  > 1.0 - 1.0 + 0.0 = 0.0 The solution in MATH-201 was to modify the p-value calculation to this: > p = 2.0 * cumulativeProbability(-t) Here the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():   p = 2 * (1 - tDistribution.cumulativeProbability(t)); Directly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues() but with the following change seems to solve the problem:   p = 2 * (tDistribution.cumulativeProbability(-t));", "A_clean_title": ["pearsonscorrel", "getcorrelationpvalu", "pearson", "correl", "get", "correl", "valu", "precis", "limit", "by", "machin", "epsilonsimilar", "epsilon", "similar", "issu", "describ", "math", "201", "pearsonscorrel", "getcorrelationpvalu", "pearson", "correl", "get", "correl", "valu", "mani", "treatment", "result", "valu", "that", "are", "continu", "down", "16", "2e", "but", "that", "drop", "after", "that", "math", "201", "problem", "wa", "describ", "as", "such", "so", "essenc", "valu", "return", "by", "ttestimpl", "ttest", "test", "impl", "test", "cumulativeprob", "cumul", "probabl", "cumulativeprobabili", "cumul", "probabili", "larg", "ish", "statist", "cumulativeprobabilti", "cumul", "probabilti", "get", "quit", "small", "cumulativeprobabilti", "cumul", "probabilti", "get", "veri", "close", "when", "cumulativeprob", "cumul", "probabl", "less", "than", "machin", "epsilon", "we", "get", "valu", "equal", "zero", "becaus", "solut", "math", "201", "wa", "modifi", "valu", "calcul", "thi", "cumulativeprob", "cumul", "probabl", "here", "problem", "similar", "pearsonscorrel", "getcorrelationpvalu", "pearson", "correl", "get", "correl", "valu", "tdistribut", "cumulativeprob", "distribut", "cumul", "probabl", "directli", "calcul", "valu", "ident", "code", "as", "pearsonscorrel", "getcorrelationpvalu", "pearson", "correl", "get", "correl", "valu", "but", "follow", "chang", "seem", "solv", "problem", "tdistribut", "cumulativeprob", "distribut", "cumul", "probabl"], "B_title": "Fixed loss of significance error in PersonsCorrelation p-value computation causing p-values smaller than the machine epsilon (~1E-16) to be reported as 0. JIRA: MATH-371 Reported and patched by Kevin Childs", "B_clean_title": ["fix", "loss", "signific", "error", "personscorrel", "person", "correl", "valu", "comput", "caus", "valu", "smaller", "than", "machin", "epsilon", "~1e", "16", "report", "as", "jira", "math", "371", "report", "patch", "by", "kevin", "child"]},
{"A_title": "WicketTester MockHttpRequest.getCookies very slow / OutOfMemoryWe have an extensive set of WicketTester tests. Recently the wicket RELEASE in the maven repository changed to 6.7.0. After the new version our tests got very slow.  When profiling I discovered that the MockHttpRequest.getCookies() was taking up a lot of time. Also tests failed because of OutOfMemory exceptions. My guess is that somehow a lot of objects are created at such speeds that the GC cannot clean them  I will investigate further but switching back to 6.6.0 solved the issue.   Edit The tests are run with TestNG and using mvn test", "A_clean_title": ["wickettest", "wicket", "tester", "mockhttprequest", "getcooki", "mock", "http", "request", "get", "cooki", "veri", "slow", "outofmemoryw", "out", "memori", "we", "have", "extens", "set", "wickettest", "wicket", "tester", "test", "recent", "wicket", "releas", "maven", "repositori", "chang", "after", "new", "version", "our", "test", "got", "veri", "slow", "when", "profil", "discov", "that", "mockhttprequest", "getcooki", "mock", "http", "request", "get", "cooki", "wa", "take", "up", "lot", "time", "also", "test", "fail", "becaus", "outofmemori", "out", "memori", "except", "my", "guess", "that", "somehow", "lot", "object", "are", "creat", "at", "such", "speed", "that", "gc", "not", "clean", "them", "will", "investig", "further", "but", "switch", "back", "solv", "issu", "edit", "test", "are", "run", "testng", "test", "ng", "mvn", "test"], "B_title": "Merge remote-tracking branch upstream/master into bugfix-WICKET-5147-cookies-wickettester-second-try", "B_clean_title": ["merg", "remot", "track", "branch", "upstream", "master", "into", "bugfix", "wicket", "5147", "cooki", "wickettest", "second", "tri"]},
{"A_title": "DerivativeStructure.atan2(yx) does not handle special cases properlyThe four special cases +/-0 for both x and y should give the same values as Math.atan2 and FastMath.atan2. However they give NaN for the value in all cases.", "A_clean_title": ["derivativestructur", "atan2", "deriv", "structur", "yx", "not", "handl", "special", "case", "properlyth", "properli", "four", "special", "case", "both", "give", "same", "valu", "as", "math", "atan2", "fastmath", "atan2", "fast", "math", "howev", "they", "give", "nan", "na", "valu", "all", "case"], "B_title": "Fixed DerivativeStructure.atan2 for special cases when both arguments are +/-0.", "B_clean_title": ["fix", "derivativestructur", "atan2", "deriv", "structur", "special", "case", "when", "both", "argument", "are"]},
{"A_title": "Interfaces and abstract classes are not valid typesI dont know whether this is by design or is a bug but I am having trouble working with DataSet and traits in scala which is a major limitation.  A simple example is shown below.    Compile time warning is Type Main.SimpleTrait has no fields that are visible from Scala Type analysis. Falling back to Java Type Analysis...  Run time error is Interfaces and abstract classes are not valid types: interface Main SimpleTrait  Regards John    val env = ExecutionEnvironment.getExecutionEnvironment    trait SimpleTrait      def contains(x: String): Boolean       class SimpleClass extends SimpleTrait      def contains(x: String) = true       val data: DataSetDouble = env.fromElements(1.0 2.0 3.0 4.0)    def f(data: DataSetDouble): DataSetSimpleTrait =       data.mapPartition(iterator =>        Iterator(new SimpleClass)     )        val g = f(data)   g.print()     env.execute(Simple example)", "A_clean_title": ["interfac", "abstract", "class", "are", "not", "valid", "typesi", "type", "dont", "know", "whether", "thi", "by", "design", "or", "bug", "but", "am", "have", "troubl", "work", "dataset", "data", "set", "trait", "scala", "which", "major", "limit", "simpl", "exampl", "shown", "below", "compil", "time", "warn", "type", "main", "simpletrait", "simpl", "trait", "ha", "no", "field", "that", "are", "visibl", "scala", "type", "analysi", "fall", "back", "java", "type", "analysi", "run", "time", "error", "interfac", "abstract", "class", "are", "not", "valid", "type", "interfac", "main", "simpletrait", "simpl", "trait", "regard", "john", "val", "env", "executionenviron", "getexecutionenviron", "execut", "environ", "get", "execut", "environ", "trait", "simpletrait", "simpl", "trait", "def", "contain", "string", "boolean", "class", "simpleclass", "simpl", "class", "extend", "simpletrait", "simpl", "trait", "def", "contain", "string", "true", "val", "data", "datasetdoubl", "data", "set", "doubl", "env", "fromel", "element", "def", "data", "datasetdoubl", "data", "set", "doubl", "datasetsimpletrait", "data", "set", "simpl", "trait", "data", "mappartit", "map", "partit", "iter", "iter", "new", "simpleclass", "simpl", "class", "val", "data", "print", "env", "execut", "simpl", "exampl"], "B_title": "Allow Interfaces and abstract types in TypeExtractor", "B_clean_title": ["allow", "interfac", "abstract", "type", "typeextractor", "type", "extractor"]},
{"A_title": "BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundaryIn some cases the aging feature in BracketingNthOrderBrentSolver fails. It attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However the chosen target is too close too zero and the inverse polynomial approximation is always on the same side thus always updates the same bracket. In the real used case for a large program I had a bracket point xA = 12500.0 yA = 3.7e-16 agingA = 0 which is the (really good) estimate of the zero on one side of the root and xB = 12500.03 yB = -7.0e-5 agingB = 97. This shows that the bracketing interval is completely unbalanced and we never succeed to rebalance it as we always updates (xA yA) and never updates (xB yB).", "A_clean_title": ["bracketingnthorderbrentsolv", "bracket", "nth", "order", "brent", "solver", "exce", "maxiterationcount", "max", "iter", "count", "while", "updat", "alway", "same", "boundaryin", "boundari", "some", "case", "age", "featur", "bracketingnthorderbrentsolv", "bracket", "nth", "order", "brent", "solver", "fail", "it", "attempt", "balanc", "bracket", "point", "by", "target", "non", "zero", "valu", "instead", "real", "root", "howev", "chosen", "target", "too", "close", "too", "zero", "invers", "polynomi", "approxim", "alway", "same", "side", "thu", "alway", "updat", "same", "bracket", "real", "use", "case", "larg", "program", "had", "bracket", "point", "xa", "12500", "ya", "16", "7e", "aginga", "age", "which", "realli", "good", "estim", "zero", "one", "side", "root", "xb", "12500", "03", "yb", "0e", "agingb", "age", "97", "thi", "show", "that", "bracket", "interv", "complet", "unbalanc", "we", "never", "succeed", "rebal", "it", "as", "we", "alway", "updat", "xa", "ya", "never", "updat", "xb", "yb"], "B_title": "Fixed bracketing interval balancing in BracketingNthOrderBrentSolver.", "B_clean_title": ["fix", "bracket", "interv", "balanc", "bracketingnthorderbrentsolv", "bracket", "nth", "order", "brent", "solver"]},
{"A_title": "Constructing invalid PartialsPartials can be constructed by invoking a constructor Partial(DateTimeFieldType int) or by merging together a set of partials using with each constructed by calling Partial(DateTimeFieldType int). However the above doesnt work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  Theres also a related issue (probably stems from the fact that the Partial is invalid):", "A_clean_title": ["construct", "invalid", "partialsparti", "partial", "partial", "construct", "by", "invok", "constructor", "partial", "datetimefieldtyp", "date", "time", "field", "type", "int", "or", "by", "merg", "togeth", "set", "partial", "each", "construct", "by", "call", "partial", "datetimefieldtyp", "date", "time", "field", "type", "int", "howev", "abov", "doesnt", "work", "all", "case", "suppos", "partial", "not", "allow", "construct", "either", "case", "that", "right", "there", "also", "relat", "issu", "probabl", "stem", "fact", "that", "partial", "invalid"], "B_title": "Prevent creation of invalid partials via Partial.with(DateTimeFieldTypeint)", "B_clean_title": ["prevent", "creation", "invalid", "partial", "via", "partial", "datetimefieldtypeint", "date", "time", "field", "typeint"]},
{"A_title": "Guard against invalid/missing checkpointsPlaying with the backup revealed a case where a checkpoint can become invalid after a manual restore of the repository. 0 The NodeStore#retrieve apis already specify that this can return null in the case the checkpoint doesnt exist anymore but it looks like the storage bits arent yet prepared for that scenario.    0 noformat org.apache.sling.commons.scheduler.impl.QuartzScheduler Exception during job execution of org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate@3a6d47 : Failed to load segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8 java.lang.IllegalStateException: Failed to load segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8 at org.apache.jackrabbit.oak.plugins.segment.AbstractStore.readSegment(AbstractStore.java:109) ~na:na at org.apache.jackrabbit.oak.plugins.segment.Segment.getSegment(Segment.java:189) ~na:na at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:97) ~na:na at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:56) ~na:na at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getChildNode(SegmentNodeState.java:209) ~na:na at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.retrieve(SegmentNodeStore.java:175) ~na:na at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.retrieve(SegmentNodeStoreService.java:198) ~na:na at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:97) ~na:na at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) ~org.apache.sling.commons.scheduler-2.4.2.jar:na at org.quartz.core.JobRunShell.run(JobRunShell.java:207) org.apache.sling.commons.scheduler-2.4.2.jar:na at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) na:1.7.0_40 at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) na:1.7.0_40 at java.lang.Thread.run(Thread.java:724) na:1.7.0_40 Caused by: java.lang.IllegalStateException: Segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8 not found at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.loadSegment(FileStore.java:184) ~na:na noformat", "A_clean_title": ["guard", "against", "invalid", "miss", "checkpointsplay", "checkpoint", "play", "backup", "reveal", "case", "where", "checkpoint", "becom", "invalid", "after", "manual", "restor", "repositori", "nodestor", "node", "store", "retriev", "api", "alreadi", "specifi", "that", "thi", "return", "null", "case", "checkpoint", "doesnt", "exist", "anymor", "but", "it", "look", "like", "storag", "bit", "arent", "yet", "prepar", "that", "scenario", "noformat", "org", "apach", "sling", "common", "schedul", "impl", "quartzschedul", "quartz", "schedul", "except", "dure", "job", "execut", "org", "apach", "jackrabbit", "oak", "plugin", "index", "asyncindexupd", "async", "index", "updat", "3a6d47", "fail", "load", "segment", "8a8b281c", "1a02", "4950", "aad5", "aad8e436a0d8", "java", "lang", "illegalstateexcept", "illeg", "state", "except", "fail", "load", "segment", "8a8b281c", "1a02", "4950", "aad5", "aad8e436a0d8", "at", "org", "apach", "jackrabbit", "oak", "plugin", "segment", "abstractstor", "readseg", "abstract", "store", "read", "segment", "abstractstor", "java:109", "abstract", "store", "~na", "na", "at", "org", "apach", "jackrabbit", "oak", "plugin", "segment", "segment", "getseg", "get", "segment", "segment", "java:189", "~na", "na", "at", "org", "apach", "jackrabbit", "oak", "plugin", "segment", "record", "getseg", "get", "segment", "record", "java:97", "~na", "na", "at", "org", "apach", "jackrabbit", "oak", "plugin", "segment", "segmentnodest", "gettempl", "segment", "node", "state", "get", "templat", "segmentnodest", "java:56", "segment", "node", "state", "~na", "na", "at", "org", "apach", "jackrabbit", "oak", "plugin", "segment", "segmentnodest", "getchildnod", "segment", "node", "state", "get", "child", "node", "segmentnodest", "java:209", "segment", "node", "state", "~na", "na", "at", "org", "apach", "jackrabbit", "oak", "plugin", "segment", "segmentnodestor", "retriev", "segment", "node", "store", "segmentnodestor", "java:175", "segment", "node", "store", "~na", "na", "at", "org", "apach", "jackrabbit", "oak", "plugin", "segment", "segmentnodestoreservic", "retriev", "segment", "node", "store", "servic", "segmentnodestoreservic", "java:198", "segment", "node", "store", "servic", "~na", "na", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "asyncindexupd", "run", "async", "index", "updat", "asyncindexupd", "java:97", "async", "index", "updat", "~na", "na", "at", "org", "apach", "sling", "common", "schedul", "impl", "quartzjobexecutor", "execut", "quartz", "job", "executor", "quartzjobexecutor", "java:105", "quartz", "job", "executor", "~org", "apach", "sling", "common", "schedul", "jar", "na", "at", "org", "quartz", "core", "jobrunshel", "run", "job", "run", "shell", "jobrunshel", "java:207", "job", "run", "shell", "org", "apach", "sling", "common", "schedul", "jar", "na", "at", "java", "util", "concurr", "threadpoolexecutor", "runwork", "thread", "pool", "executor", "run", "worker", "threadpoolexecutor", "java:1145", "thread", "pool", "executor", "na:1", "40", "at", "java", "util", "concurr", "threadpoolexecutor", "thread", "pool", "executor", "worker", "run", "threadpoolexecutor", "java:615", "thread", "pool", "executor", "na:1", "40", "at", "java", "lang", "thread", "run", "thread", "java:724", "na:1", "40", "caus", "by", "java", "lang", "illegalstateexcept", "illeg", "state", "except", "segment", "8a8b281c", "1a02", "4950", "aad5", "aad8e436a0d8", "not", "found", "at", "org", "apach", "jackrabbit", "oak", "plugin", "segment", "file", "filestor", "loadseg", "file", "store", "load", "segment", "filestor", "java:184", "file", "store", "~na", "na", "noformat"], "B_title": "Guard against invalid/missing checkpoints", "B_clean_title": ["guard", "against", "invalid", "miss", "checkpoint"]},
{"A_title": "potential clash of commit ids after restartthe commit ids in the current implementation are counter-based i.e. every commit (on HEAD or on a branch) gets its id by incrementing counter.  only the current HEAD id is recorded/persisted. on startup the counter is initialized with the current HEAD id.   assume the following sequence:  - ...startup... - counter == HEAD == 99 - commit on HEAD -> new HEAD rev: ++counter == 100 - create branch -> new branch rev: ++counter == 101 - ...restart... - counter == HEAD == 100 - commit on HEAD -> new HEAD rev: ++counter == 101 => clashes with older branch rev!   since a commit is never overwritten the above scenario results in a private branch revision marked as HEAD i.e. the revision history is corrupted.", "A_clean_title": ["potenti", "clash", "commit", "id", "after", "restartth", "commit", "id", "current", "implement", "are", "counter", "base", "everi", "commit", "head", "or", "branch", "get", "it", "id", "by", "increment", "counter", "onli", "current", "head", "id", "record", "persist", "startup", "counter", "initi", "current", "head", "id", "assum", "follow", "sequenc", "startup", "counter", "head", "99", "commit", "head", "new", "head", "rev", "++counter", "100", "creat", "branch", "new", "branch", "rev", "++counter", "101", "restart", "counter", "head", "100", "commit", "head", "new", "head", "rev", "++counter", "101", "clash", "older", "branch", "rev", "sinc", "commit", "never", "overwritten", "abov", "scenario", "result", "privat", "branch", "revis", "mark", "as", "head", "revis", "histori", "corrupt"], "B_title": "- potential clash of commit ids after restart - added test case", "B_clean_title": ["potenti", "clash", "commit", "id", "after", "restart", "ad", "test", "case"]},
{"A_title": "PagingNavigator.setEnabled(false) doesnt work1. Create paging navigator PagingNavigator  2. call PagingNavigator.setEnabled(false) 3. navigator will be rendered as enabled if click on any link (1 2 etc) - content of the data view will be changed.  In many cases its necessary disable navigator for example when user need to edit only single line of DataView other controls need to be disabled.", "A_clean_title": ["pagingnavig", "seten", "page", "navig", "set", "enabl", "fals", "doesnt", "work1", "creat", "page", "navig", "pagingnavig", "page", "navig", "call", "pagingnavig", "seten", "page", "navig", "set", "enabl", "fals", "navig", "will", "render", "as", "enabl", "click", "ani", "link", "etc", "content", "data", "view", "will", "chang", "mani", "case", "it", "necessari", "disabl", "navig", "exampl", "when", "user", "need", "edit", "onli", "singl", "line", "dataview", "data", "view", "other", "control", "need", "disabl"], "B_title": "", "B_clean_title": []},
{"A_title": "regression on strategy to integrate cas authenticationyes It happens in org.apache.wicket.request.handler.PageProvider.getPageInstance()  but not for the WelcomePage but the redirection page (RedirectPage). the CASPageAuthorizationStrategy as we are not authentified a org.apache.wicket.RestartResponseAtInterceptPageException with in parameter an instance of RedirectPage. On the second call of PageProvider.getPageInstance the pageId is of 0 an all other parameters are nulls.  The run seems quite different on 1.5-RC4.2 version - There is only one call of the method PageProvider.getPageInstance() and it come after the CASPageAuthorizationStrategy.isPageAuthorized  Le 27/06/2011 11:24 Martin Grigorov a crit : > Put a breakpoint in > org.apache.wicket.request.handler.PageProvider.getPageInstance() and > see what happens. > It seems the test tries to retrieve a page from the page store by id > but there is no such. > > On Mon Jun 27 2011 at 12:20 PM Thomas Franconville > <tfranconville@tetraedge.com>  wrote: >> Hi >> >> Upgrading wicket from 1.5-RC4.2 to 1.5-RC5.1  make my Junit Test down with >> the error Page expired >> >> /** >>   * Simple test using the WicketTester >>   */ >> public class TestHomePage >>  >>     private WicketTester tester; >> >>     @Before >>     public void setUp() >>      >>         tester = new WicketTester(new MyApplication()); >>      >> >>     @Test >>     public void homepageRendersSuccessfully() >>      >>         //start and render the test page >>         tester.startPage(WelcomePage.class); >> >>         //assert rendered page class >>         tester.assertRenderedPage(RedirectPage.class); >>      >>  >> >> My application use a CASPageAuthorizationStrategy inspired of >> http://www.lunikon.net/2009/11/24/integrating-cas-and-wicket/ >> >> >> Kind Regards >> >> Thomas", "A_clean_title": ["regress", "strategi", "integr", "ca", "authenticationy", "it", "happen", "org", "apach", "wicket", "request", "handler", "pageprovid", "getpageinst", "page", "provid", "get", "page", "instanc", "but", "not", "welcomepag", "welcom", "page", "but", "redirect", "page", "redirectpag", "redirect", "page", "caspageauthorizationstrategi", "ca", "page", "author", "strategi", "as", "we", "are", "not", "authentifi", "org", "apach", "wicket", "restartresponseatinterceptpageexcept", "restart", "respons", "at", "intercept", "page", "except", "paramet", "instanc", "redirectpag", "redirect", "page", "second", "call", "pageprovid", "getpageinst", "page", "provid", "get", "page", "instanc", "pageid", "page", "id", "all", "other", "paramet", "are", "null", "run", "seem", "quit", "differ", "rc4", "version", "there", "onli", "one", "call", "method", "pageprovid", "getpageinst", "page", "provid", "get", "page", "instanc", "it", "come", "after", "caspageauthorizationstrategi", "ispageauthor", "ca", "page", "author", "strategi", "page", "author", "le", "27", "06", "2011", "11:24", "martin", "grigorov", "crit", "put", "breakpoint", "org", "apach", "wicket", "request", "handler", "pageprovid", "getpageinst", "page", "provid", "get", "page", "instanc", "see", "what", "happen", "it", "seem", "test", "tri", "retriev", "page", "page", "store", "by", "id", "but", "there", "no", "such", "mon", "jun", "27", "2011", "at", "12:20", "pm", "thoma", "franconvil", "tfranconvil", "tetraedg", "com", "wrote", "hi", "upgrad", "wicket", "rc4", "rc5", "make", "my", "junit", "test", "down", "error", "page", "expir", "simpl", "test", "wickettest", "wicket", "tester", "public", "class", "testhomepag", "test", "home", "page", "privat", "wickettest", "wicket", "tester", "tester", "befor", "public", "void", "setup", "set", "up", "tester", "new", "wickettest", "wicket", "tester", "new", "myapplic", "my", "applic", "test", "public", "void", "homepagerenderssuccess", "homepag", "render", "success", "start", "render", "test", "page", "tester", "startpag", "start", "page", "welcomepag", "class", "welcom", "page", "assert", "render", "page", "class", "tester", "assertrenderedpag", "assert", "render", "page", "redirectpag", "class", "redirect", "page", "my", "applic", "use", "caspageauthorizationstrategi", "ca", "page", "author", "strategi", "inspir", "http", "ca", "lunikon", "wicket", "www", "net", "2009", "11", "24", "integr", "kind", "regard", "thoma"], "B_title": "regression on strategy to integrate cas authentication", "B_clean_title": ["regress", "strategi", "integr", "ca", "authent"]},
{"A_title": "stat.correlation.Covariance should allow one-column matricesCurrently (rev 1453206) passing 1-by-M matrix to the Covariance constructor throws IllegalArgumentException. For consistency the Covariance class should work for a single-column matrix (i.e. for a N-dimensional random variable with N=1) and it should return 1-by-1 covariance matrix with the variables variance in its only element.", "A_clean_title": ["stat", "correl", "covari", "allow", "one", "column", "matricescurr", "matric", "current", "rev", "1453206", "pass", "by", "matrix", "covari", "constructor", "throw", "illegalargumentexcept", "illeg", "argument", "except", "consist", "covari", "class", "work", "singl", "column", "matrix", "dimension", "random", "variabl", "n=1", "it", "return", "by", "covari", "matrix", "variabl", "varianc", "it", "onli", "element"], "B_title": "Allow covariance to be computed for one-dimensional variables.", "B_clean_title": ["allow", "covari", "comput", "one", "dimension", "variabl"]},
{"A_title": "ClassCastException during TypeCheck passNone", "A_clean_title": ["classcastexcept", "class", "cast", "except", "dure", "typecheck", "type", "check", "passnon", "pass", "none"], "B_title": "Fix a bunch of bugs in record type sup/inf with proxy types Fixes issue 603", "B_clean_title": ["fix", "bunch", "bug", "record", "type", "sup", "inf", "proxi", "type", "fix", "issu", "603"]},
{"A_title": "PropertyIndex cost calculation is faultyThe cost calculation can easily go out of bounds when it needs to estimate (whenever there are more than 100 nodes). The high value it returns can be higher than the traversal index which has a max of 10M but can be less smaller.   For example:   100 nodes in the index:   with a single level /content cost is 6250000   adding a second level /content/data cost jumps to 1.544804416E9    101 nodes in the index:   with a single level /content cost is 100   adding a second level /content/data stays at 100    100 nodes 12 levels deep cost is 2.147483647E9   101 nodes 12 levels deep cost is 6.7108864E7", "A_clean_title": ["propertyindex", "properti", "index", "cost", "calcul", "faultyth", "faulti", "cost", "calcul", "easili", "go", "out", "bound", "when", "it", "need", "estim", "whenev", "there", "are", "more", "than", "100", "node", "high", "valu", "it", "return", "higher", "than", "travers", "index", "which", "ha", "max", "10m", "but", "less", "smaller", "exampl", "100", "node", "index", "singl", "level", "content", "cost", "6250000", "ad", "second", "level", "content", "data", "cost", "jump", "544804416e9", "101", "node", "index", "singl", "level", "content", "cost", "100", "ad", "second", "level", "content", "data", "stay", "at", "100", "100", "node", "12", "level", "deep", "cost", "147483647e9", "101", "node", "12", "level", "deep", "cost", "7108864e7"], "B_title": "PropertyIndex cost calculation is faulty", "B_clean_title": ["propertyindex", "properti", "index", "cost", "calcul", "faulti"]},
{"A_title": "ContentMirrorStoreStrategy should utilize path restriction when availableCurrently ContentStoreMirrorStrategy has a mirror of content path under :index. Yet while query (and count) methods doesnt jump directly into restricted path.  This would be very useful for PropertyIndex where the queries can be optimized by supplying a path restriction along with an indexed property restriction (I dont know if queries with references would use paths so much though)", "A_clean_title": ["contentmirrorstorestrategi", "content", "mirror", "store", "strategi", "util", "path", "restrict", "when", "availablecurr", "avail", "current", "contentstoremirrorstrategi", "content", "store", "mirror", "strategi", "ha", "mirror", "content", "path", "under", "index", "yet", "while", "queri", "count", "method", "doesnt", "jump", "directli", "into", "restrict", "path", "thi", "would", "veri", "use", "propertyindex", "properti", "index", "where", "queri", "optim", "by", "suppli", "path", "restrict", "along", "index", "properti", "restrict", "dont", "know", "queri", "refer", "would", "use", "path", "so", "much", "though"], "B_title": "ContentMirrorStoreStrategy should utilize path restriction when available", "B_clean_title": ["contentmirrorstorestrategi", "content", "mirror", "store", "strategi", "util", "path", "restrict", "when", "avail"]},
{"A_title": "Operator checkpoint statistics state size overflowState sizes (long) of checkpoint stats overflow when summing them up per operator because the sum is stored in an int.", "A_clean_title": ["oper", "checkpoint", "statist", "state", "size", "overflowst", "overflow", "state", "size", "long", "checkpoint", "stat", "overflow", "when", "sum", "them", "up", "per", "oper", "becaus", "sum", "store", "int"], "B_title": "runtime Fix checkpoint statistics state size overflow", "B_clean_title": ["runtim", "fix", "checkpoint", "statist", "state", "size", "overflow"]},
{"A_title": "compiler-20110811 crashes with index(1) must be less than size(1)None", "A_clean_title": ["compil", "20110811", "crash", "index", "must", "less", "than", "size", "none"], "B_title": "Fix edge case in InlineObjectLiteral. Fixes issue 545", "B_clean_title": ["fix", "edg", "case", "inlineobjectliter", "inlin", "object", "liter", "fix", "issu", "545"]},
{"A_title": "Input type validation often fails on custom TypeInfo implementationsInput type validation often fails when used with custom type infos. One example of this behaviour can be reproduced by creating a custom type info with our own field type:  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  env.generateSequence(1 10).map(new MapFunction<Long Tuple1<Optional<Long>>>()  @Override public Tuple1<Optional<Long>> map(Long value) throws Exception  return Tuple1.of(Optional.of(value));  ).returns(new TupleTypeInfo<>(new OptionTypeInfo<Long>(BasicTypeInfo.LONG_TYPE_INFO))) .keyBy(new KeySelector<Tuple1<Optional<Long>> Optional<Long>>()   @Override public Optional<Long> getKey(Tuple1<Optional<Long>> value) throws Exception  return value.f0;  );  This will fail on Input type validation at the KeySelector (or any other function for example a mapper) with the following exception:  Input mismatch: Basic type expected.", "A_clean_title": ["input", "type", "valid", "often", "fail", "custom", "typeinfo", "type", "info", "implementationsinput", "implement", "input", "type", "valid", "often", "fail", "when", "use", "custom", "type", "info", "one", "exampl", "thi", "behaviour", "reproduc", "by", "creat", "custom", "type", "info", "our", "own", "field", "type", "streamexecutionenviron", "stream", "execut", "environ", "env", "streamexecutionenviron", "getexecutionenviron", "stream", "execut", "environ", "get", "execut", "environ", "env", "generatesequ", "gener", "sequenc", "10", "map", "new", "mapfunct", "map", "function", "long", "tuple1", "option", "long", "overrid", "public", "tuple1", "option", "long", "map", "long", "valu", "throw", "except", "return", "tuple1", "option", "valu", "return", "new", "tupletypeinfo", "tupl", "type", "info", "new", "optiontypeinfo", "option", "type", "info", "long", "basictypeinfo", "basic", "type", "info", "long", "type", "info", "keybi", "key", "by", "new", "keyselector", "key", "selector", "tuple1", "option", "long", "option", "long", "overrid", "public", "option", "long", "getkey", "get", "key", "tuple1", "option", "long", "valu", "throw", "except", "return", "valu", "f0", "thi", "will", "fail", "input", "type", "valid", "at", "keyselector", "key", "selector", "or", "ani", "other", "function", "exampl", "mapper", "follow", "except", "input", "mismatch", "basic", "type", "expect"], "B_title": "FLINK-3563 core TypeExtraction input type validation fixes", "B_clean_title": ["flink", "3563", "core", "typeextract", "type", "extract", "input", "type", "valid", "fix"]},
{"A_title": "AbstractMarkupParser doesnt remove Comments correctlyAbstractMarkupParser removeComment(...) doesnt remove Comments correctly  if two html comments stand to close together <!-- foo --> <!-- bar --> foo will be removed but not bar.  see:  https://github.com/mafulafunk/wicketComments  git@github.com:mafulafunk/wicketComments.git", "A_clean_title": ["abstractmarkuppars", "abstract", "markup", "parser", "doesnt", "remov", "comment", "correctlyabstractmarkuppars", "correctli", "abstract", "markup", "parser", "removecom", "remov", "comment", "doesnt", "remov", "comment", "correctli", "two", "html", "comment", "stand", "close", "togeth", "foo", "bar", "foo", "will", "remov", "but", "not", "bar", "see", "http", "github", "com", "mafulafunk", "wicketcom", "wicket", "comment", "git", "github", "com", "git", "mafulafunk", "wicketcom", "wicket", "comment"], "B_title": "fixed WICKET-3222 AbstractMarkupParser doesnt remove Comments correctly Issue: WICKET-3222", "B_clean_title": ["fix", "wicket", "3222", "abstractmarkuppars", "abstract", "markup", "parser", "doesnt", "remov", "comment", "correctli", "issu", "wicket", "3222"]},
{"A_title": "Tree has wrong parent after moveAfter a move operation Tree.getParent() still returns the old parent.  code Tree x = r.getChild(x); Tree y = r.getChild(y);  root.move(x y/x); assertEquals(y x.getParent().getName());  // Fails code", "A_clean_title": ["tree", "ha", "wrong", "parent", "after", "moveaft", "move", "after", "move", "oper", "tree", "getpar", "get", "parent", "still", "return", "old", "parent", "code", "tree", "getchild", "get", "child", "tree", "getchild", "get", "child", "root", "move", "assertequ", "assert", "equal", "getpar", "get", "parent", "getnam", "get", "name", "fail", "code"], "B_title": "Tree has wrong parent after move - Initial fix - Regression tests", "B_clean_title": ["tree", "ha", "wrong", "parent", "after", "move", "initi", "fix", "regress", "test"]},
{"A_title": "Compiler ignores delete statements can break functionality.None", "A_clean_title": ["compil", "ignor", "delet", "statement", "break", "function", "none"], "B_title": "Do not inline an object literal if it has a property that gets deleted.", "B_clean_title": ["not", "inlin", "object", "liter", "it", "ha", "properti", "that", "get", "delet"]},
{"A_title": "DataTable row groups are present in markup even when they contain no rows.As per the HTML spec : When present each THEAD TFOOT and TBODY contains a row group. Each row group must contain at least one row defined by the TR element.  There is no check in place to remove the row group tags from the output if they dont contain any row.", "A_clean_title": ["datat", "data", "tabl", "row", "group", "are", "present", "markup", "even", "when", "they", "contain", "no", "row", "as", "per", "html", "spec", "when", "present", "each", "thead", "tfoot", "tbodi", "contain", "row", "group", "each", "row", "group", "must", "contain", "at", "least", "one", "row", "defin", "by", "tr", "element", "there", "no", "check", "place", "remov", "row", "group", "tag", "output", "they", "dont", "contain", "ani", "row"], "B_title": "DataTable row groups are present in markup even when they contain no rows.", "B_clean_title": ["datat", "data", "tabl", "row", "group", "are", "present", "markup", "even", "when", "they", "contain", "no", "row"]},
{"A_title": "MemoryPropertyBuilder.assignFrom leads to ClassCastException on getPropertyState with date propertiesNone", "A_clean_title": ["memorypropertybuild", "assignfrom", "memori", "properti", "builder", "assign", "lead", "classcastexcept", "class", "cast", "except", "getpropertyst", "get", "properti", "state", "date", "propertiesnon", "properti", "none"], "B_title": "MemoryPropertyBuilder.assignFrom leads to ClassCastException on getPropertyState with date properties", "B_clean_title": ["memorypropertybuild", "assignfrom", "memori", "properti", "builder", "assign", "lead", "classcastexcept", "class", "cast", "except", "getpropertyst", "get", "properti", "state", "date", "properti"]},
{"A_title": "PojoType fields not supported by field position keysTuple fields which are Pojos (or any other non-tuple composite type) cannot be selected as keys by field position keys.  Something like   code DataSet<Tuple2<Integer MyPojo>> data = ... data.groupBy(1).reduce(...) code  fails with an exception.", "A_clean_title": ["pojotyp", "pojo", "type", "field", "not", "support", "by", "field", "posit", "keystupl", "key", "tupl", "field", "which", "are", "pojo", "or", "ani", "other", "non", "tupl", "composit", "type", "not", "select", "as", "key", "by", "field", "posit", "key", "someth", "like", "code", "dataset", "data", "set", "tuple2", "integ", "mypojo", "my", "pojo", "data", "data", "groupbi", "group", "by", "reduc", "code", "fail", "except"], "B_title": "fix FieldPositionKeys support Pojo fields", "B_clean_title": ["fix", "fieldpositionkey", "field", "posit", "key", "support", "pojo", "field"]},
{"A_title": "WicketTester Cookie handlingWhile trying to test my SecureForm implementation (https://issues.apache.org/jira/browse/WICKET-1885) with WicketTester I ran into this issue: A cookie set in the response never shows up in the next request because both have their own lists of cookies that arent shared.  Afaik both should share the same List instance to handle cookies. That way its possible to set a cookie in the response and read it from the request.  A simple testcase is attached.", "A_clean_title": ["wickettest", "wicket", "tester", "cooki", "handlingwhil", "handl", "while", "tri", "test", "my", "secureform", "secur", "form", "implement", "http", "1885", "apach", "issu", "org", "jira", "brows", "wicket", "wickettest", "wicket", "tester", "ran", "into", "thi", "issu", "cooki", "set", "respons", "never", "show", "up", "next", "request", "becaus", "both", "have", "their", "own", "list", "cooki", "that", "arent", "share", "afaik", "both", "share", "same", "list", "instanc", "handl", "cooki", "that", "way", "it", "possibl", "set", "cooki", "respons", "read", "it", "request", "simpl", "testcas", "attach"], "B_title": "Fixed testing things that require cookies persisted over multiple requests such as CSRF protection in a hidden Form fields. An even more straight forward option would have been to remove clearing cookies in MockWebApplication.initialize() and copy cookies to each request from response because after all the lifecycle of a WicketTester (MockWebApplication) instance should be such that cookies could be preserved there.", "B_clean_title": ["fix", "test", "thing", "that", "requir", "cooki", "persist", "over", "multipl", "request", "such", "as", "csrf", "protect", "hidden", "form", "field", "even", "more", "straight", "forward", "option", "would", "have", "been", "remov", "clear", "cooki", "mockwebappl", "initi", "mock", "web", "applic", "copi", "cooki", "each", "request", "respons", "becaus", "after", "all", "lifecycl", "wickettest", "wicket", "tester", "mockwebappl", "mock", "web", "applic", "instanc", "such", "that", "cooki", "could", "preserv", "there"]},
{"A_title": "unable to add nodes to an empty rootless Tree (e.g. LinkTree)2 scenarios which adding new nodes (via ajax) to a rootless Tree is not working as expected. the node is getting added to the treemodel but non is displayed.  1) adding a node to the rootnode. the newly added node is not displayed. 2) the rootless tree already has a node. if you add additional nodes to the root node they will be displayed (compare to 1) if you add an additional node to one of the added nodes the complete tree will disappear.  see attached quickstart", "A_clean_title": ["unabl", "add", "node", "empti", "rootless", "tree", "linktre", "link", "tree", "scenario", "which", "ad", "new", "node", "via", "ajax", "rootless", "tree", "not", "work", "as", "expect", "node", "get", "ad", "treemodel", "but", "non", "display", "ad", "node", "rootnod", "newli", "ad", "node", "not", "display", "rootless", "tree", "alreadi", "ha", "node", "you", "add", "addit", "node", "root", "node", "they", "will", "display", "compar", "you", "add", "addit", "node", "one", "ad", "node", "complet", "tree", "will", "disappear", "see", "attach", "quickstart"], "B_title": "Testing for not presented root nodes on root less trees Issue: WICKET-3309", "B_clean_title": ["test", "not", "present", "root", "node", "root", "less", "tree", "issu", "wicket", "3309"]},
{"A_title": "Lucene Index property definition is ignored if its not in includePropertyNames configLucene index property definition will not be used unless that property is in includePropertyNames config. This enforces including that property in includePropertyNames. includePropertyNames restricts all properties from getting indexed so user is now enforced to include all properties in includePropertyNames to be indexed.", "A_clean_title": ["lucen", "index", "properti", "definit", "ignor", "it", "not", "includepropertynam", "includ", "properti", "name", "configlucen", "config", "lucen", "index", "properti", "definit", "will", "not", "use", "unless", "that", "properti", "includepropertynam", "includ", "properti", "name", "config", "thi", "enforc", "includ", "that", "properti", "includepropertynam", "includ", "properti", "name", "includepropertynam", "includ", "properti", "name", "restrict", "all", "properti", "get", "index", "so", "user", "now", "enforc", "includ", "all", "properti", "includepropertynam", "includ", "properti", "name", "index"], "B_title": "- Lucene Index property definition is ignored if its not in includePropertyNames config", "B_clean_title": ["lucen", "index", "properti", "definit", "ignor", "it", "not", "includepropertynam", "includ", "properti", "name", "config"]},
{"A_title": "Mock does not implement locality groups or mergingThe Mock Instance does not implement locality groups and throws an exception if one attempts to set them. It would be useful for the unit tests that I am writing for the Accumulo proxy to have at least minimal locality group functionality in the Mock instance for example simply storing the groups and returning the stored groups when asked for.  *Edit: Tablet merging would be useful as well.", "A_clean_title": ["mock", "not", "implement", "local", "group", "or", "mergingth", "merg", "mock", "instanc", "not", "implement", "local", "group", "throw", "except", "one", "attempt", "set", "them", "it", "would", "use", "unit", "test", "that", "am", "write", "accumulo", "proxi", "have", "at", "least", "minim", "local", "group", "function", "mock", "instanc", "exampl", "simpli", "store", "group", "return", "store", "group", "when", "ask", "edit", "tablet", "merg", "would", "use", "as", "well"], "B_title": "implement deleteRows in MockAccumulo fake locality groups ignore merge requests", "B_clean_title": ["implement", "deleterow", "delet", "row", "mockaccumulo", "mock", "accumulo", "fake", "local", "group", "ignor", "merg", "request"]},
{"A_title": "OakDirectory not usable in readOnly mode with a readOnly builderWhen using OakDirectory with a read only builder say in LuceneCommand in oak-console following error is seen  noformat lc info /oak:index/users ERROR java.lang.UnsupportedOperationException: This builder is read-only.        at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.unsupported (ReadOnlyBuilder.java:45)        at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.child (ReadOnlyBuilder.java:190)        at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.child (ReadOnlyBuilder.java:35)        at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.<init> (OakDirectory.java:93)        at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.<init> (OakDirectory.java:87)        at org.apache.jackrabbit.oak.console.commands.LuceneCommand.getDirectory (LuceneCommand.groovy:128)        at org.apache.jackrabbit.oak.console.commands.LuceneCommand.this 4 getDirectory (LuceneCommand.groovy)        at org.apache.jackrabbit.oak.console.commands.LuceneCommand _closure1.doCall (LuceneCommand.groovy:55) noformat", "A_clean_title": ["oakdirectori", "oak", "directori", "not", "usabl", "readonli", "read", "onli", "mode", "readonli", "read", "onli", "builderwhen", "builder", "when", "oakdirectori", "oak", "directori", "read", "onli", "builder", "say", "lucenecommand", "lucen", "command", "oak", "consol", "follow", "error", "seen", "noformat", "lc", "info", "oak", "index", "user", "error", "java", "lang", "unsupportedoperationexcept", "unsupport", "oper", "except", "thi", "builder", "read", "onli", "at", "org", "apach", "jackrabbit", "oak", "spi", "state", "readonlybuild", "unsupport", "read", "onli", "builder", "readonlybuild", "java:45", "read", "onli", "builder", "at", "org", "apach", "jackrabbit", "oak", "spi", "state", "readonlybuild", "child", "read", "onli", "builder", "readonlybuild", "java:190", "read", "onli", "builder", "at", "org", "apach", "jackrabbit", "oak", "spi", "state", "readonlybuild", "child", "read", "onli", "builder", "readonlybuild", "java:35", "read", "onli", "builder", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "oakdirectori", "oak", "directori", "init", "oakdirectori", "java:93", "oak", "directori", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "oakdirectori", "oak", "directori", "init", "oakdirectori", "java:87", "oak", "directori", "at", "org", "apach", "jackrabbit", "oak", "consol", "command", "lucenecommand", "getdirectori", "lucen", "command", "get", "directori", "lucenecommand", "groovy:128", "lucen", "command", "at", "org", "apach", "jackrabbit", "oak", "consol", "command", "lucenecommand", "thi", "lucen", "command", "getdirectori", "get", "directori", "lucenecommand", "groovi", "lucen", "command", "at", "org", "apach", "jackrabbit", "oak", "consol", "command", "lucenecommand", "lucen", "command", "docal", "closure1", "call", "lucenecommand", "groovy:55", "lucen", "command", "noformat"], "B_title": "- OakDirectory not usable in readOnly mode with a readOnly builder", "B_clean_title": ["oakdirectori", "oak", "directori", "not", "usabl", "readonli", "read", "onli", "mode", "readonli", "read", "onli", "builder"]},
{"A_title": "FirstEntryInRowIterator is broken and has no testIn 1.4 and trunk the iterator throws a NullPointerException when seeked.  In 1.3 the iterator runs but there is a question as to what it should do when it is seeked to the middle of a row.  Currently it returns the first key found within the range.  I believe this should be changed to ignore the remaining portion of that row and return the first key of the next row.  Should this change be made in 1.3 or should I leave it as is and just change it in 1.4 and greater?", "A_clean_title": ["firstentryinrowiter", "first", "entri", "row", "iter", "broken", "ha", "no", "testin", "test", "trunk", "iter", "throw", "nullpointerexcept", "null", "pointer", "except", "when", "seek", "iter", "run", "but", "there", "question", "as", "what", "it", "when", "it", "seek", "middl", "row", "current", "it", "return", "first", "key", "found", "within", "rang", "believ", "thi", "chang", "ignor", "remain", "portion", "that", "row", "return", "first", "key", "next", "row", "thi", "chang", "made", "or", "leav", "it", "as", "just", "chang", "it", "greater"], "B_title": "created test and fixed seek behavior of FirstEntryInRowIterator - merged to trunk", "B_clean_title": ["creat", "test", "fix", "seek", "behavior", "firstentryinrowiter", "first", "entri", "row", "iter", "merg", "trunk"]},
{"A_title": "AsyncIndexer fails due to FileNotFoundException thrown by CopyOnWrite logicAt times the CopyOnWrite reports following exception  noformat 15.07.2015 14:20:35.930 *WARN* pool-58-thread-1 org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate The async index update failed org.apache.jackrabbit.oak.api.CommitFailedException: OakLucene0004: Failed to close the Lucene index at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:204) at org.apache.jackrabbit.oak.plugins.index.IndexUpdate.leave(IndexUpdate.java:219) at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:63) at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:56) at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.updateIndex(AsyncIndexUpdate.java:366) at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:311) at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) at org.quartz.core.JobRunShell.run(JobRunShell.java:207) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.FileNotFoundException: _2s7.fdt at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:261) at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier CopyOnWriteDirectory COWLocalFileReference.fileLength(IndexCopier.java:837) at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier CopyOnWriteDirectory.fileLength(IndexCopier.java:607) at org.apache.lucene.index.SegmentCommitInfo.sizeInBytes(SegmentCommitInfo.java:141) at org.apache.lucene.index.DocumentsWriterPerThread.sealFlushedSegment(DocumentsWriterPerThread.java:529) at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:502) at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:508) at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:618) at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3147) at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3123) at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:988) at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:932) at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:894) at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditorContext.closeWriter(LuceneIndexEditorContext.java:192) at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:202) ... 10 common frames omitted noformat", "A_clean_title": ["asyncindex", "async", "index", "fail", "due", "filenotfoundexcept", "file", "not", "found", "except", "thrown", "by", "copyonwrit", "copi", "write", "logicat", "logic", "at", "time", "copyonwrit", "copi", "write", "report", "follow", "except", "noformat", "15", "07", "2015", "14:20:35", "930", "warn", "pool", "58", "thread", "org", "apach", "jackrabbit", "oak", "plugin", "index", "asyncindexupd", "async", "index", "updat", "async", "index", "updat", "fail", "org", "apach", "jackrabbit", "oak", "api", "commitfailedexcept", "commit", "fail", "except", "oaklucene0004", "oak", "lucene0004", "fail", "close", "lucen", "index", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "luceneindexeditor", "leav", "lucen", "index", "editor", "luceneindexeditor", "java:204", "lucen", "index", "editor", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "indexupd", "leav", "index", "updat", "indexupd", "java:219", "index", "updat", "at", "org", "apach", "jackrabbit", "oak", "spi", "commit", "visibleeditor", "leav", "visibl", "editor", "visibleeditor", "java:63", "visibl", "editor", "at", "org", "apach", "jackrabbit", "oak", "spi", "commit", "editordiff", "process", "editor", "diff", "editordiff", "java:56", "editor", "diff", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "asyncindexupd", "updateindex", "async", "index", "updat", "updat", "index", "asyncindexupd", "java:366", "async", "index", "updat", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "asyncindexupd", "run", "async", "index", "updat", "asyncindexupd", "java:311", "async", "index", "updat", "at", "org", "apach", "sling", "common", "schedul", "impl", "quartzjobexecutor", "execut", "quartz", "job", "executor", "quartzjobexecutor", "java:105", "quartz", "job", "executor", "at", "org", "quartz", "core", "jobrunshel", "run", "job", "run", "shell", "jobrunshel", "java:207", "job", "run", "shell", "at", "java", "util", "concurr", "threadpoolexecutor", "runwork", "thread", "pool", "executor", "run", "worker", "threadpoolexecutor", "java:1145", "thread", "pool", "executor", "at", "java", "util", "concurr", "threadpoolexecutor", "thread", "pool", "executor", "worker", "run", "threadpoolexecutor", "java:615", "thread", "pool", "executor", "at", "java", "lang", "thread", "run", "thread", "java:745", "caus", "by", "java", "io", "filenotfoundexcept", "file", "not", "found", "except", "fdt", "2s7", "at", "org", "apach", "lucen", "store", "fsdirectori", "filelength", "fs", "directori", "file", "length", "fsdirectori", "java:261", "fs", "directori", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "indexcopi", "index", "copier", "copyonwritedirectori", "copi", "write", "directori", "cowlocalfilerefer", "filelength", "cow", "local", "file", "refer", "file", "length", "indexcopi", "java:837", "index", "copier", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "indexcopi", "index", "copier", "copyonwritedirectori", "filelength", "copi", "write", "directori", "file", "length", "indexcopi", "java:607", "index", "copier", "at", "org", "apach", "lucen", "index", "segmentcommitinfo", "sizeinbyt", "segment", "commit", "info", "size", "byte", "segmentcommitinfo", "java:141", "segment", "commit", "info", "at", "org", "apach", "lucen", "index", "documentswriterperthread", "sealflushedseg", "document", "writer", "per", "thread", "seal", "flush", "segment", "documentswriterperthread", "java:529", "document", "writer", "per", "thread", "at", "org", "apach", "lucen", "index", "documentswriterperthread", "flush", "document", "writer", "per", "thread", "documentswriterperthread", "java:502", "document", "writer", "per", "thread", "at", "org", "apach", "lucen", "index", "documentswrit", "doflush", "document", "writer", "flush", "documentswrit", "java:508", "document", "writer", "at", "org", "apach", "lucen", "index", "documentswrit", "flushallthread", "document", "writer", "flush", "all", "thread", "documentswrit", "java:618", "document", "writer", "at", "org", "apach", "lucen", "index", "indexwrit", "doflush", "index", "writer", "flush", "indexwrit", "java:3147", "index", "writer", "at", "org", "apach", "lucen", "index", "indexwrit", "flush", "index", "writer", "indexwrit", "java:3123", "index", "writer", "at", "org", "apach", "lucen", "index", "indexwrit", "closeintern", "index", "writer", "close", "intern", "indexwrit", "java:988", "index", "writer", "at", "org", "apach", "lucen", "index", "indexwrit", "close", "index", "writer", "indexwrit", "java:932", "index", "writer", "at", "org", "apach", "lucen", "index", "indexwrit", "close", "index", "writer", "indexwrit", "java:894", "index", "writer", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "luceneindexeditorcontext", "closewrit", "lucen", "index", "editor", "context", "close", "writer", "luceneindexeditorcontext", "java:192", "lucen", "index", "editor", "context", "at", "org", "apach", "jackrabbit", "oak", "plugin", "index", "lucen", "luceneindexeditor", "leav", "lucen", "index", "editor", "luceneindexeditor", "java:202", "lucen", "index", "editor", "10", "common", "frame", "omit", "noformat"], "B_title": "- AsyncIndexer fails due to FileNotFoundException thrown by CopyOnWrite logic", "B_clean_title": ["asyncindex", "async", "index", "fail", "due", "filenotfoundexcept", "file", "not", "found", "except", "thrown", "by", "copyonwrit", "copi", "write", "logic"]},
{"A_title": "Dangerous code in PoissonDistributionImplIn the following excerpt from class PoissonDistributionImpl:  code:title=PoissonDistributionImpl.java|borderStyle=solid     public PoissonDistributionImpl(double p NormalDistribution z)          super();         setNormal(z);         setMean(p);      code  (1) Overridable methods are called within the constructor. (2) The reference z is stored and modified within the class.  Ive encountered problem (1) in several classes while working on issue 348. In those cases in order to remove potential problems I copied/pasted the body of the setter methods inside the constructor but I think that a more elegant solution would be to remove the setters altogether (i.e. make the classes immutable). Problem (2) can also create unexpected behaviour. Is it really necessary to pass the NormalDistribution object; cant it be always created within the class?", "A_clean_title": ["danger", "code", "poissondistributionimplin", "poisson", "distribut", "impl", "follow", "excerpt", "class", "poissondistributionimpl", "poisson", "distribut", "impl", "code", "title=poissondistributionimpl", "java|borderstyle=solid", "title=poisson", "distribut", "impl", "java|bord", "style=solid", "public", "poissondistributionimpl", "poisson", "distribut", "impl", "doubl", "normaldistribut", "normal", "distribut", "super", "setnorm", "set", "normal", "setmean", "set", "mean", "code", "overrid", "method", "are", "call", "within", "constructor", "refer", "store", "modifi", "within", "class", "ive", "encount", "problem", "sever", "class", "while", "work", "issu", "348", "those", "case", "order", "remov", "potenti", "problem", "copi", "past", "bodi", "setter", "method", "insid", "constructor", "but", "think", "that", "more", "eleg", "solut", "would", "remov", "setter", "altogeth", "make", "class", "immut", "problem", "also", "creat", "unexpect", "behaviour", "it", "realli", "necessari", "pass", "normaldistribut", "normal", "distribut", "object", "cant", "it", "alway", "creat", "within", "class"], "B_title": "Removed deprecated methods.", "B_clean_title": ["remov", "deprec", "method"]},
{"A_title": "not possible to create a Mutation object from scala w/o some extra helper codeissue:   its not possible to create a Mutation object from scala without employing a standalone java jar wrapper. the preferred method for creating the object has you do it in two stages: create with table row then employ Mutation.put() to populate the object with the actual mutation data. when you do this in scala you get a  java.lang.IllegalStateException: Can not add to mutation after serializing it at org.apache.accumulo.core.data.Mutation.put(Mutation.java:168) at org.apache.accumulo.core.data.Mutation.put(Mutation.java:163) at org.apache.accumulo.core.data.Mutation.put(Mutation.java:211)  error. I *think* this has something to do with the byte array going out of scope in Scala but somehow not in Java. If you concat the operations (constuctor().put(data data ...) you dont run into the error but scala sees a Unit return type so you cant actually add the mutation to a BatchWriter. The only way I was able to get around this was to create a stand-alone jar with a method that created then returned a populated mutation object.   I wasnt sure whether or not to call this a bug or an enhancement. given that you probably want Accumulo to play nice with Scala I decided to call it a bug.   below is a link to the stack overflow thread I created whilst figuring all this out:   http://stackoverflow.com/questions/29497547/odd-error-when-populating-accumulo-1-6-mutation-object-via-spark-notebook/29527189#29527189", "A_clean_title": ["not", "possibl", "creat", "mutat", "object", "scala", "some", "extra", "helper", "codeissu", "it", "not", "possibl", "creat", "mutat", "object", "scala", "without", "employ", "standalon", "java", "jar", "wrapper", "prefer", "method", "creat", "object", "ha", "you", "it", "two", "stage", "creat", "tabl", "row", "then", "employ", "mutat", "put", "popul", "object", "actual", "mutat", "data", "when", "you", "thi", "scala", "you", "get", "java", "lang", "illegalstateexcept", "illeg", "state", "except", "not", "add", "mutat", "after", "serial", "it", "at", "org", "apach", "accumulo", "core", "data", "mutat", "put", "mutat", "java:168", "at", "org", "apach", "accumulo", "core", "data", "mutat", "put", "mutat", "java:163", "at", "org", "apach", "accumulo", "core", "data", "mutat", "put", "mutat", "java:211", "error", "think", "thi", "ha", "someth", "byte", "array", "go", "out", "scope", "scala", "but", "somehow", "not", "java", "you", "concat", "oper", "constuctor", "put", "data", "data", "you", "dont", "run", "into", "error", "but", "scala", "see", "unit", "return", "type", "so", "you", "cant", "actual", "add", "mutat", "batchwrit", "batch", "writer", "onli", "way", "wa", "abl", "get", "around", "thi", "wa", "creat", "stand", "alon", "jar", "method", "that", "creat", "then", "return", "popul", "mutat", "object", "wasnt", "sure", "whether", "or", "not", "call", "thi", "bug", "or", "enhanc", "given", "that", "you", "probabl", "want", "accumulo", "play", "nice", "scala", "decid", "call", "it", "bug", "below", "link", "stack", "overflow", "thread", "creat", "whilst", "figur", "all", "thi", "out", "http", "error", "when", "popul", "accumulo", "mutat", "object", "via", "spark", "notebook", "29527189", "stackoverflow", "com", "question", "29497547", "odd", "29527189"], "B_title": "make Mutation#hashCode and Mutation#equals not change the state of the mutation", "B_clean_title": ["make", "mutat", "hashcod", "hash", "code", "mutat", "equal", "not", "chang", "state", "mutat"]},
{"A_title": "FastDateParser does not handle unterminated quotes correctlyFDP does not handled unterminated quotes the same way as SimpleDateFormat For example: Format: dd Date: d3 This should fail to parse the format and date but it actually works. The format is parsed as: Pattern: d(p IsNd ++)", "A_clean_title": ["fastdatepars", "fast", "date", "parser", "not", "handl", "untermin", "quot", "correctlyfdp", "correctli", "fdp", "not", "handl", "untermin", "quot", "same", "way", "as", "simpledateformat", "simpl", "date", "format", "exampl", "format", "dd", "date", "d3", "thi", "fail", "pars", "format", "date", "but", "it", "actual", "work", "format", "pars", "as", "pattern", "isnd", "nd"], "B_title": "FastDateParser does not handle unterminated quotes correctly", "B_clean_title": ["fastdatepars", "fast", "date", "parser", "not", "handl", "untermin", "quot", "correctli"]},
{"A_title": "Accumulo Shell does not respect exit when executing fileIf there is an exit statement in the file given via accumulo shell -f file the execution seems to skip it and go on to the next command instead of terminating.  To recreate: noformat mike@home ~ cat bug.accumulo exit scan -np -t !METADATA mike@home ~ bin/accumulo shell -f /home/mike/bug.accumulo noformat  Expected output: None Actual output: A full scan of the !METADATA", "A_clean_title": ["accumulo", "shell", "not", "respect", "exit", "when", "execut", "fileif", "file", "there", "exit", "statement", "file", "given", "via", "accumulo", "shell", "file", "execut", "seem", "skip", "it", "go", "next", "command", "instead", "termin", "recreat", "noformat", "mike", "home", "cat", "bug", "accumulo", "exit", "scan", "np", "metadata", "mike", "home", "bin", "accumulo", "shell", "accumulo", "home", "mike", "bug", "noformat", "expect", "output", "none", "actual", "output", "full", "scan", "metadata"], "B_title": "Applied patch from Mike Drob to 1.5 branch", "B_clean_title": ["appli", "patch", "mike", "drob", "branch"]},
{"A_title": "LocaleUtils.toLocale() rejects strings with only language+variantLocaleUtils.toLocale() throws an exception on strings containing a language and a variant but no country code. For example : fr__POSIX This string can be produced with the JDK by instanciating a Locale with an empty string for the country : new Locale(fr  POSIX).toString(). According to the javadoc for the Locale class a variant is allowed with just a language code or just a country code. Commons Configuration handles this case in its PropertyConverter.toLocale() method. Id like to replace our implementation by the one provided by LocaleUtils but our tests fail due to this case.", "A_clean_title": ["localeutil", "tolocal", "local", "util", "local", "reject", "string", "onli", "language+variantlocaleutil", "tolocal", "language+vari", "local", "util", "local", "throw", "except", "string", "contain", "languag", "variant", "but", "no", "countri", "code", "exampl", "fr", "posix", "thi", "string", "produc", "jdk", "by", "instanci", "local", "empti", "string", "countri", "new", "local", "fr", "posix", "tostr", "string", "accord", "javadoc", "local", "class", "variant", "allow", "just", "languag", "code", "or", "just", "countri", "code", "common", "configur", "handl", "thi", "case", "it", "propertyconvert", "tolocal", "properti", "convert", "local", "method", "id", "like", "replac", "our", "implement", "by", "one", "provid", "by", "localeutil", "local", "util", "but", "our", "test", "fail", "due", "thi", "case"], "B_title": "Applying unit test/fix for LANG-328", "B_clean_title": ["appli", "unit", "test", "fix", "lang", "328"]},
{"A_title": "Vector3D.crossProduct is sensitive to numerical cancellationCross product implementation uses the naive formulas (y1 z2 - y2 z1 ...). These formulas fail when vectors are almost colinear like in the following example:  Vector3D v1 = new Vector3D(9070467121.0 4535233560.0 1); Vector3D v2 = new Vector3D(9070467123.0 4535233561.0 1); System.out.println(Vector3D.crossProduct(v1 v2));   The previous code displays   -1 2 0   instead of the correct answer   -1 2 1", "A_clean_title": ["vector3d", "crossproduct", "cross", "product", "sensit", "numer", "cancellationcross", "cancel", "cross", "product", "implement", "use", "naiv", "formula", "y1", "z2", "y2", "z1", "these", "formula", "fail", "when", "vector", "are", "almost", "colinear", "like", "follow", "exampl", "vector3d", "v1", "new", "vector3d", "9070467121", "4535233560", "vector3d", "v2", "new", "vector3d", "9070467123", "4535233561", "system", "out", "println", "vector3d", "crossproduct", "cross", "product", "v1", "v2", "previou", "code", "display", "instead", "correct", "answer"], "B_title": "Reduced cancellation errors in Vector3D.crossProduct", "B_clean_title": ["reduc", "cancel", "error", "vector3d", "crossproduct", "cross", "product"]},
{"A_title": "Bugs in Frequency APII think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object but one cannot add anything further to the instance as the second add fails with IllegalArgumentException. In fact the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods dont fail when given a non-Comparable object (because the class cast exception is caught) however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail but doesnt causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable it seems to me that the API would be much improved by using Comparable instead of Object. Also it should make it easier to implement generics. However this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly as the first attempt to add a non-Comparable object would fail rather than the second add of a possibly valid object. But is that a viable program? It can only add one object and any attempt to get statistics will either return 0 or an Exception and applying the instanceof fix would also cause it to fail.", "A_clean_title": ["bug", "frequenc", "apii", "think", "exist", "frequenc", "api", "ha", "some", "bug", "it", "addvalu", "add", "valu", "object", "method", "allow", "one", "add", "plain", "object", "but", "one", "not", "add", "anyth", "further", "instanc", "as", "second", "add", "fail", "illegalargumentexcept", "illeg", "argument", "except", "fact", "problem", "first", "call", "addvalu", "add", "valu", "object", "which", "not", "allow", "plain", "object", "ad", "it", "onli", "allow", "compar", "object", "thi", "could", "fix", "by", "check", "that", "object", "compar", "similar", "consider", "appli", "getcumfreq", "get", "cum", "freq", "object", "getcumpct", "get", "cum", "pct", "object", "method", "they", "will", "onli", "work", "object", "that", "implement", "compar", "getcount", "get", "count", "object", "getpct", "get", "pct", "object", "method", "dont", "fail", "when", "given", "non", "compar", "object", "becaus", "class", "cast", "except", "caught", "howev", "they", "just", "return", "as", "object", "wa", "not", "present", "final", "object", "obj", "new", "object", "addvalu", "add", "valu", "obj", "thi", "ought", "fail", "but", "doesnt", "caus", "unexpect", "behaviour", "below", "system", "out", "println", "getcount", "get", "count", "obj", "system", "out", "println", "getpct", "get", "pct", "obj", "rather", "than", "ad", "extra", "check", "compar", "it", "seem", "me", "that", "api", "would", "much", "improv", "by", "compar", "instead", "object", "also", "it", "make", "it", "easier", "implement", "gener", "howev", "thi", "would", "caus", "compil", "failur", "some", "program", "that", "pass", "object", "rather", "than", "compar", "class", "these", "would", "need", "recod", "but", "think", "they", "would", "continu", "run", "ok", "against", "new", "api", "it", "would", "also", "affect", "run", "time", "behaviour", "slightli", "as", "first", "attempt", "add", "non", "compar", "object", "would", "fail", "rather", "than", "second", "add", "possibl", "valid", "object", "but", "that", "viabl", "program", "it", "onli", "add", "one", "object", "ani", "attempt", "get", "statist", "will", "either", "return", "or", "except", "appli", "instanceof", "fix", "would", "also", "caus", "it", "fail"], "B_title": "- throw IllegalArgument rather than ClassCast to better retain original behaviour", "B_clean_title": ["throw", "illegalargu", "illeg", "argument", "rather", "than", "classcast", "class", "cast", "better", "retain", "origin", "behaviour"]},
{"A_title": "Combiner default behavior is dangerousCurrently if the users does not give the combiner any columns to work against it will work against all columns.  This is dangerous if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().", "A_clean_title": ["combin", "default", "behavior", "dangerouscurr", "danger", "current", "user", "not", "give", "combin", "ani", "column", "work", "against", "it", "will", "work", "against", "all", "column", "thi", "danger", "user", "accident", "forget", "specifi", "column", "then", "their", "data", "could", "unintent", "corrupt", "someth", "differ", "need", "done", "also", "class", "that", "extend", "combin", "call", "super", "validateopt", "valid", "option"], "B_title": "changed default behavior of Combiner and modified default test", "B_clean_title": ["chang", "default", "behavior", "combin", "modifi", "default", "test"]},
{"A_title": "NPE for google cloud storageHi   Any time I try to access Resource from the google storage it throws the following NPE. It happens on both read and write . Here is how the pom looks like   Here is the code   I even tried checking out the entire repo and just run the code as it is  from spring-cloud-gcp-storage-resource-sample and class WebController by changing my bucket name and still get the same NPE  I removed the entire .m2 directory and started all over all and still the issue persists  I double checked I have access to my bucket and even tried the same form my service account that have project admin access  but the problem persists", "A_clean_title": ["npe", "googl", "cloud", "storagehi", "storag", "hi", "ani", "time", "tri", "access", "resourc", "googl", "storag", "it", "throw", "follow", "npe", "it", "happen", "both", "read", "write", "here", "how", "pom", "look", "like", "here", "code", "even", "tri", "check", "out", "entir", "repo", "just", "run", "code", "as", "it", "spring", "cloud", "gcp", "storag", "resourc", "sampl", "class", "webcontrol", "web", "control", "by", "chang", "my", "bucket", "name", "still", "get", "same", "npe", "remov", "entir", "m2", "directori", "start", "all", "over", "all", "still", "issu", "persist", "doubl", "check", "have", "access", "my", "bucket", "even", "tri", "same", "form", "my", "servic", "account", "that", "have", "project", "admin", "access", "but", "problem", "persist"], "B_title": "Fix NPE for GCS buckets that have underscores (#316)  Fixes #314.", "B_clean_title": ["fix", "npe", "gc", "bucket", "that", "have", "underscor", "316", "fix", "314"]},
{"A_title": "closure-compiler @define annotation does not allow line to be split on 80 characters.None", "A_clean_title": ["closur", "compil", "defin", "annot", "not", "allow", "line", "split", "80", "charact", "none"], "B_title": "Generalize define value handling allow ADD and other basic ops fix BITxxx handling so they dont allow invalid operands.", "B_clean_title": ["gener", "defin", "valu", "handl", "allow", "add", "other", "basic", "op", "fix", "bitxxx", "bi", "txxx", "handl", "so", "they", "dont", "allow", "invalid", "operand"]},
{"A_title": "Assignments within conditions are sometimes incorrectly removedNone", "A_clean_title": ["assign", "within", "condit", "are", "sometim", "incorrectli", "removednon", "remov", "none"], "B_title": "Correct handling of conditional branches within expressions when doing dead assignment elminination. Fixes issue 384.", "B_clean_title": ["correct", "handl", "condit", "branch", "within", "express", "when", "do", "dead", "assign", "elminin", "fix", "issu", "384"]},
{"A_title": "Hibernate Subselect Entity not supported by EntityMetamodelImplDescription  After playing a bit with table functions (see also  #181 ) I had the idea to experiment with mapping a generate_series query to an entity using the annotation. Find below the produced stack trace. A trivial work around is to wrap the query into to a view and map that instead. I am uncertain whether a fix for this is worthwhile but it could perhaps be a step towards implementing table functions (  #181 ).   Expected behavior  Actual behavior  Steps to reproduce  Environment  Version:            1.2.0.Alpha3  JPA-Provider:       Hibernate 5.2.12 DBMS:               PostgresSQL Application Server: Wildfly", "A_clean_title": ["hibern", "subselect", "entiti", "not", "support", "by", "entitymetamodelimpldescript", "entiti", "metamodel", "impl", "descript", "after", "play", "bit", "tabl", "function", "see", "also", "181", "had", "idea", "experi", "map", "gener", "seri", "queri", "entiti", "annot", "find", "below", "produc", "stack", "trace", "trivial", "work", "around", "wrap", "queri", "into", "view", "map", "that", "instead", "am", "uncertain", "whether", "fix", "thi", "worthwhil", "but", "it", "could", "perhap", "step", "toward", "implement", "tabl", "function", "181", "expect", "behavior", "actual", "behavior", "step", "reproduc", "environ", "version", "alpha3", "jpa", "provid", "hibern", "12", "dbm", "postgressql", "postgr", "sql", "applic", "server", "wildfli"], "B_title": "#519 Little cleanup and ensure tests run on all supported DBMS", "B_clean_title": ["519", "littl", "cleanup", "ensur", "test", "run", "all", "support", "dbm"]},
{"A_title": "PageProvider should create a new Page instance if PageParameters are changed even if a stored page exists.The getStoredPage(int) method returns a stored page instance even if user changes parameter values encoded into URL and the PageParameters object of the stored page instance is never changed. So same page is displayed always though user changes url on browser manually.  ** HOW TO REPRODUCT **  1. unpack the attached sample project pagebug.tar.gz. 2. mvn jetty:run 3. access to http://localhost:8080/user/user1  You will see a form filled with information about user 1. The users name is user 1 age is 30 and country is Japan. The mount path of this page is /user/ userId. so user1 in the accessed url is a parameter value.  after accessing to the url the url will be changed to http://localhost:8080/user/user1?0 .  it contains the page id of the currently displayed page.  4. change some values and submit the form. page id will be changed on every submit.  5. change only parameter value in url to user2. Never change page-id.  for example if you now access to http://localhost:8080/user/user1?5 change the url to http://localhost:8080/user/user2?5 .  6. This program must display information about user2 because the parameter value of url is changed. But you will see the information of user 1. Wicket always display the page of page-id = 5 (even though user changed url manually).  In this sample program I use LoadableDetachableModel for retrieving current parameter-value. But I dont get the new parameter-value because pageParameters object in a page instance is never changed after the construction. pageParameters is fixed in the constructor of Page class.  I think that there are no easy way to retrieve parameter-values encoded into mount-path. Request.getRequestParameters() does not contain parameters encoded into mount-path. So there are no work-around for this issue.   ** HOW TO FIX THIS ISSUE **  We must return null from getStoredPage(int) method of PageProvider class if current PageParameters is not same with the PageParameters of a stored page. In current code getStoredPage(int) checks only if the class of both pages are same. We must check the PageParameters of both pages.   ** PATCH **  I attached a pache for PageProvider class. try it.", "A_clean_title": ["pageprovid", "page", "provid", "creat", "new", "page", "instanc", "pageparamet", "page", "paramet", "are", "chang", "even", "store", "page", "exist", "getstoredpag", "get", "store", "page", "int", "method", "return", "store", "page", "instanc", "even", "user", "chang", "paramet", "valu", "encod", "into", "url", "pageparamet", "page", "paramet", "object", "store", "page", "instanc", "never", "chang", "so", "same", "page", "display", "alway", "though", "user", "chang", "url", "browser", "manual", "how", "reproduct", "unpack", "attach", "sampl", "project", "pagebug", "tar", "gz", "mvn", "jetti", "run", "access", "http", "localhost:8080", "user", "user1", "you", "will", "see", "form", "fill", "inform", "about", "user", "user", "name", "user", "age", "30", "countri", "japan", "mount", "path", "thi", "page", "user", "userid", "user", "id", "so", "user1", "access", "url", "paramet", "valu", "after", "access", "url", "url", "will", "chang", "http", "localhost:8080", "user", "user1", "it", "contain", "page", "id", "current", "display", "page", "chang", "some", "valu", "submit", "form", "page", "id", "will", "chang", "everi", "submit", "chang", "onli", "paramet", "valu", "url", "user2", "never", "chang", "page", "id", "exampl", "you", "now", "access", "http", "localhost:8080", "user", "user1", "chang", "url", "http", "localhost:8080", "user", "user2", "thi", "program", "must", "display", "inform", "about", "user2", "becaus", "paramet", "valu", "url", "chang", "but", "you", "will", "see", "inform", "user", "wicket", "alway", "display", "page", "page", "id", "even", "though", "user", "chang", "url", "manual", "thi", "sampl", "program", "use", "loadabledetachablemodel", "loadabl", "detach", "model", "retriev", "current", "paramet", "valu", "but", "dont", "get", "new", "paramet", "valu", "becaus", "pageparamet", "page", "paramet", "object", "page", "instanc", "never", "chang", "after", "construct", "pageparamet", "page", "paramet", "fix", "constructor", "page", "class", "think", "that", "there", "are", "no", "easi", "way", "retriev", "paramet", "valu", "encod", "into", "mount", "path", "request", "getrequestparamet", "get", "request", "paramet", "not", "contain", "paramet", "encod", "into", "mount", "path", "so", "there", "are", "no", "work", "around", "thi", "issu", "how", "fix", "thi", "issu", "we", "must", "return", "null", "getstoredpag", "get", "store", "page", "int", "method", "pageprovid", "page", "provid", "class", "current", "pageparamet", "page", "paramet", "not", "same", "pageparamet", "page", "paramet", "store", "page", "current", "code", "getstoredpag", "get", "store", "page", "int", "check", "onli", "class", "both", "page", "are", "same", "we", "must", "check", "pageparamet", "page", "paramet", "both", "page", "patch", "attach", "pach", "pageprovid", "page", "provid", "class", "tri", "it"], "B_title": "PageProvider should create a new Page instance if PageParameters are changed even if a stored page exists.", "B_clean_title": ["pageprovid", "page", "provid", "creat", "new", "page", "instanc", "pageparamet", "page", "paramet", "are", "chang", "even", "store", "page", "exist"]},
{"A_title": "UniformIntegerDistribution should make constructer a exclusive bound or made parameter check more relaxUniformIntegerDistribution constructer  public UniformIntegerDistribution(RandomGenerator rng                                       int lower                                       int upper)  the lower and the upper all inclusive. but the parameter check made a   if (lower >= upper)              throw new NumberIsTooLargeException(                             LocalizedFormats.LOWER_BOUND_NOT_BELOW_UPPER_BOUND                             lower upper false); check i think it is too strict to construct UniformIntegerDistribution (00)  this should make it possible", "A_clean_title": ["uniformintegerdistribut", "uniform", "integ", "distribut", "make", "construct", "exclus", "bound", "or", "made", "paramet", "check", "more", "relaxuniformintegerdistribut", "relax", "uniform", "integ", "distribut", "construct", "public", "uniformintegerdistribut", "uniform", "integ", "distribut", "randomgener", "random", "gener", "rng", "int", "lower", "int", "upper", "lower", "upper", "all", "inclus", "but", "paramet", "check", "made", "lower", "upper", "throw", "new", "numberistoolargeexcept", "number", "too", "larg", "except", "localizedformat", "local", "format", "lower", "bound", "not", "below", "upper", "bound", "lower", "upper", "fals", "check", "think", "it", "too", "strict", "construct", "uniformintegerdistribut", "uniform", "integ", "distribut", "00", "thi", "make", "it", "possibl"], "B_title": "Allow same value for lower and upper bounds.", "B_clean_title": ["allow", "same", "valu", "lower", "upper", "bound"]},
{"A_title": "FormTester doesnt correctly submit a form when a FileUploadField was not set (which is not required)FormTester doesnt correctly submit a form when  a FileUploadField was not set. This file is not required.  So it is impossible to create a real test because I am forced to always set a File to check to whole form.  There was discussion about this problem here: http://www.nabble.com/FormTester-and-FileUploadField-td18566869.html   I will be very grateful if you can fix it :) Artur", "A_clean_title": ["formtest", "form", "tester", "doesnt", "correctli", "submit", "form", "when", "fileuploadfield", "file", "upload", "field", "wa", "not", "set", "which", "not", "requir", "formtest", "form", "tester", "doesnt", "correctli", "submit", "form", "when", "fileuploadfield", "file", "upload", "field", "wa", "not", "set", "thi", "file", "not", "requir", "so", "it", "imposs", "creat", "real", "test", "becaus", "am", "forc", "alway", "set", "file", "check", "whole", "form", "there", "wa", "discuss", "about", "thi", "problem", "here", "http", "fileuploadfield", "nabbl", "file", "upload", "field", "td18566869", "html", "www", "com", "formtest", "form", "tester", "will", "veri", "grate", "you", "fix", "it", "artur"], "B_title": "", "B_clean_title": []},
{"A_title": "bogus missing return warningNone", "A_clean_title": ["bogu", "miss", "return", "warningnon", "warn", "none"], "B_title": "when handling a finally block like so try  alert(1)  finally   There needs to be 2 edges: an unconditional edge to the statement after the finally and an edge for the code path that continues handling the exception. Label the second edge with ON_EX instead of UNCOND. Fixes issue 779", "B_clean_title": ["when", "handl", "final", "block", "like", "so", "tri", "alert", "final", "there", "need", "edg", "uncondit", "edg", "statement", "after", "final", "edg", "code", "path", "that", "continu", "handl", "except", "label", "second", "edg", "ex", "instead", "uncond", "fix", "issu", "779"]},
{"A_title": "isVisibleInHierarchy() possibly unnecessarily checks children whose parents are invisible?Hi!  See attached quickstart with junit test reproducing the bug. See also patch proposal.  I have a page with two panels:  page.form.add(panel1); page.form.add(panel2);  in some situations panel1 is not visible.  However a form submit event will visit all formcomponents of panel1 via:         at org.apache.wicket.markup.html.form.FormComponent.visitFormComponentsPostOrder(FormComponent.java:400)        at org.apache.wicket.markup.html.form.Form.visitFormComponentsPostOrder(Form.java:1209)        at org.apache.wicket.markup.html.form.Form.inputChanged(Form.java:1403)        at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:865)  This results in a crash because panel1 components are not prepared to be invoked via isvisible when the panel itself is not visible.  I wonder if the component.isVisibleInHierarchy could be changed as follows to first check parent visibility:   public final boolean isVisibleInHierarchy()      Component component = this;    while (component != null)          Component componentParent = component.getParent();       if (((componentParent == null) || componentParent.isVisibleInHierarchy()) && component.determineVisibility())              component = componentParent;            else              return false;              return true;    Similar change could/should maybe be possible also for isEnabledInHierarchy ?", "A_clean_title": ["isvisibleinhierarchi", "visibl", "hierarchi", "possibl", "unnecessarili", "check", "children", "whose", "parent", "are", "invis", "hi", "see", "attach", "quickstart", "junit", "test", "reproduc", "bug", "see", "also", "patch", "propos", "have", "page", "two", "panel", "page", "form", "add", "panel1", "page", "form", "add", "panel2", "some", "situat", "panel1", "not", "visibl", "howev", "form", "submit", "event", "will", "visit", "all", "formcompon", "panel1", "via", "at", "org", "apach", "wicket", "markup", "html", "form", "formcompon", "visitformcomponentspostord", "form", "compon", "visit", "form", "compon", "post", "order", "formcompon", "java:400", "form", "compon", "at", "org", "apach", "wicket", "markup", "html", "form", "form", "visitformcomponentspostord", "visit", "form", "compon", "post", "order", "form", "java:1209", "at", "org", "apach", "wicket", "markup", "html", "form", "form", "inputchang", "input", "chang", "form", "java:1403", "at", "org", "apach", "wicket", "markup", "html", "form", "form", "onformsubmit", "form", "submit", "form", "java:865", "thi", "result", "crash", "becaus", "panel1", "compon", "are", "not", "prepar", "invok", "via", "isvis", "when", "panel", "itself", "not", "visibl", "wonder", "compon", "isvisibleinhierarchi", "visibl", "hierarchi", "could", "chang", "as", "follow", "first", "check", "parent", "visibl", "public", "final", "boolean", "isvisibleinhierarchi", "visibl", "hierarchi", "compon", "compon", "thi", "while", "compon", "null", "compon", "componentpar", "compon", "parent", "compon", "getpar", "get", "parent", "componentpar", "compon", "parent", "null", "componentpar", "isvisibleinhierarchi", "compon", "parent", "visibl", "hierarchi", "compon", "determinevis", "determin", "visibl", "compon", "componentpar", "compon", "parent", "return", "fals", "return", "true", "similar", "chang", "could", "mayb", "possibl", "also", "isenabledinhierarchi", "enabl", "hierarchi"], "B_title": "meh. this caching will probably slow things down. and it can be difficult to find all the places in code where caching should be invalidated. lets hold off until it is a hotspot. Issue: WICKET-3166", "B_clean_title": ["meh", "thi", "cach", "will", "probabl", "slow", "thing", "down", "it", "difficult", "find", "all", "place", "code", "where", "cach", "invalid", "let", "hold", "off", "until", "it", "hotspot", "issu", "wicket", "3166"]},
{"A_title": "MethodGetAndSet.setValue uses wrong source to determine which type to convert to when theres no setterMethodGetAndSet.setValue uses wrong source to determine which type to convert to when theres no setter resulting in exceptions like this: org.apache.wicket.WicketRuntimeException: Error setting field: private int PropertyResolverTest DirectFieldSetWithDifferentTypeThanGetter.value on object: PropertyResolverTest DirectFieldSetWithDifferentTypeThanGetter@396477d9 at org.apache.wicket.util.lang.PropertyResolver MethodGetAndSet.setValue(PropertyResolver.java:1150) at org.apache.wicket.util.lang.PropertyResolver ObjectAndGetSetter.setValue(PropertyResolver.java:588) at org.apache.wicket.util.lang.PropertyResolver.setValue(PropertyResolver.java:136) at PropertyResolverTest.testDirectFieldSetWithDifferentTypeThanGetter(PropertyResolverTest.java:12)  Bug is located in: converted = converter.convert(value getMethod.getReturnType());  Instead it should read: converted = converter.convert(value type);  Testcase attached.  Additional thoughts: if (setMethod != null)    type = getMethod.getReturnType();  This is really confusing (we check setMethod presence but get type from getMethod). Luckily this works as expected because in MethodGetAndSet.findSetter only methods with same (or superclass) type as getter are returned.", "A_clean_title": ["methodgetandset", "setvalu", "method", "get", "set", "set", "valu", "use", "wrong", "sourc", "determin", "which", "type", "convert", "when", "there", "no", "settermethodgetandset", "setvalu", "setter", "method", "get", "set", "set", "valu", "use", "wrong", "sourc", "determin", "which", "type", "convert", "when", "there", "no", "setter", "result", "except", "like", "thi", "org", "apach", "wicket", "wicketruntimeexcept", "wicket", "runtim", "except", "error", "set", "field", "privat", "int", "propertyresolvertest", "properti", "resolv", "test", "directfieldsetwithdifferenttypethangett", "valu", "direct", "field", "set", "differ", "type", "than", "getter", "object", "propertyresolvertest", "properti", "resolv", "test", "directfieldsetwithdifferenttypethangett", "direct", "field", "set", "differ", "type", "than", "getter", "396477d9", "at", "org", "apach", "wicket", "util", "lang", "propertyresolv", "properti", "resolv", "methodgetandset", "setvalu", "method", "get", "set", "set", "valu", "propertyresolv", "java:1150", "properti", "resolv", "at", "org", "apach", "wicket", "util", "lang", "propertyresolv", "properti", "resolv", "objectandgetsett", "setvalu", "object", "get", "setter", "set", "valu", "propertyresolv", "java:588", "properti", "resolv", "at", "org", "apach", "wicket", "util", "lang", "propertyresolv", "setvalu", "properti", "resolv", "set", "valu", "propertyresolv", "java:136", "properti", "resolv", "at", "propertyresolvertest", "testdirectfieldsetwithdifferenttypethangett", "properti", "resolv", "test", "test", "direct", "field", "set", "differ", "type", "than", "getter", "propertyresolvertest", "java:12", "properti", "resolv", "test", "bug", "locat", "convert", "convert", "convert", "valu", "getmethod", "getreturntyp", "get", "method", "get", "return", "type", "instead", "it", "read", "convert", "convert", "convert", "valu", "type", "testcas", "attach", "addit", "thought", "setmethod", "set", "method", "null", "type", "getmethod", "getreturntyp", "get", "method", "get", "return", "type", "thi", "realli", "confus", "we", "check", "setmethod", "set", "method", "presenc", "but", "get", "type", "getmethod", "get", "method", "luckili", "thi", "work", "as", "expect", "becaus", "methodgetandset", "findsett", "method", "get", "set", "find", "setter", "onli", "method", "same", "or", "superclass", "type", "as", "getter", "are", "return"], "B_title": "fixed MethodGetAndSet.setValue uses wrong source to determine which type to convert to when theres no setter Issue: WICKET-2624", "B_clean_title": ["fix", "methodgetandset", "setvalu", "method", "get", "set", "set", "valu", "use", "wrong", "sourc", "determin", "which", "type", "convert", "when", "there", "no", "setter", "issu", "wicket", "2624"]},
{"A_title": "calling MiniAccumuloCluster.stop multiple times fails with NPEOn the mailing list ~ctubbsii mentioned seeing some NPEs in the stderr for mvn verify.  I see one here when running mvn verify with either hadoop profile:  quote Exception in thread Thread-0 java.lang.NullPointerException at org.apache.accumulo.minicluster.MiniAccumuloCluster.stopProcessWithTimeout(MiniAccumuloCluster.java:449) at org.apache.accumulo.minicluster.MiniAccumuloCluster.stop(MiniAccumuloCluster.java:376) at org.apache.accumulo.minicluster.MiniAccumuloCluster 1.run(MiniAccumuloCluster.java:318) quote  The relevant piece of code (in 1.5.2-SNAP) is the executor.execute below  code   private int stopProcessWithTimeout(final Process proc long timeout TimeUnit unit) throws InterruptedException ExecutionException TimeoutException      FutureTask<Integer> future = new FutureTask<Integer>(new Callable<Integer>()          @Override         public Integer call() throws InterruptedException            proc.destroy();           return proc.waitFor();              );      executor.execute(future);      return future.get(timeout unit);    code  Reading through the code for stop it nulls out executor when its done. So the easy way to get an NPE is calling stop() multiple times on a MAC instance. Since we have a shutdown hook that calls stop that means that a single user invocation of stop should result in a NPE later.  Since start() doesnt allow multiple starts we probably shouldnt allow multiple stops. That would mean adding logic to the shutdown hook to check if were already stopped or making a private unguarded version of stop that allows multiple calls and using that from the hook.  criteria for closing this issue:  * MAC should document wether calling stop() multiple times is allowed * fix MAC.stop to either guard against multiple calls or handle them gracefully * find out why this only gets an NPE in one place. Do we rely on the shutdown hook everywhere?", "A_clean_title": ["call", "miniaccumuloclust", "stop", "mini", "accumulo", "cluster", "multipl", "time", "fail", "npeon", "npe", "mail", "list", "~ctubbsii", "mention", "see", "some", "npe", "np", "es", "stderr", "mvn", "verifi", "see", "one", "here", "when", "run", "mvn", "verifi", "either", "hadoop", "profil", "quot", "except", "thread", "thread", "java", "lang", "nullpointerexcept", "null", "pointer", "except", "at", "org", "apach", "accumulo", "miniclust", "miniaccumuloclust", "stopprocesswithtimeout", "mini", "accumulo", "cluster", "stop", "process", "timeout", "miniaccumuloclust", "java:449", "mini", "accumulo", "cluster", "at", "org", "apach", "accumulo", "miniclust", "miniaccumuloclust", "stop", "mini", "accumulo", "cluster", "miniaccumuloclust", "java:376", "mini", "accumulo", "cluster", "at", "org", "apach", "accumulo", "miniclust", "miniaccumuloclust", "mini", "accumulo", "cluster", "run", "miniaccumuloclust", "java:318", "mini", "accumulo", "cluster", "quot", "relev", "piec", "code", "snap", "executor", "execut", "below", "code", "privat", "int", "stopprocesswithtimeout", "stop", "process", "timeout", "final", "process", "proc", "long", "timeout", "timeunit", "time", "unit", "unit", "throw", "interruptedexcept", "interrupt", "except", "executionexcept", "execut", "except", "timeoutexcept", "timeout", "except", "futuretask", "futur", "task", "integ", "futur", "new", "futuretask", "futur", "task", "integ", "new", "callabl", "integ", "overrid", "public", "integ", "call", "throw", "interruptedexcept", "interrupt", "except", "proc", "destroy", "return", "proc", "waitfor", "wait", "executor", "execut", "futur", "return", "futur", "get", "timeout", "unit", "code", "read", "through", "code", "stop", "it", "null", "out", "executor", "when", "it", "done", "so", "easi", "way", "get", "npe", "call", "stop", "multipl", "time", "mac", "instanc", "sinc", "we", "have", "shutdown", "hook", "that", "call", "stop", "that", "mean", "that", "singl", "user", "invoc", "stop", "result", "npe", "later", "sinc", "start", "doesnt", "allow", "multipl", "start", "we", "probabl", "shouldnt", "allow", "multipl", "stop", "that", "would", "mean", "ad", "logic", "shutdown", "hook", "check", "were", "alreadi", "stop", "or", "make", "privat", "unguard", "version", "stop", "that", "allow", "multipl", "call", "that", "hook", "criteria", "close", "thi", "issu", "mac", "document", "wether", "call", "stop", "multipl", "time", "allow", "fix", "mac", "stop", "either", "guard", "against", "multipl", "call", "or", "handl", "them", "grace", "find", "out", "whi", "thi", "onli", "get", "npe", "one", "place", "we", "reli", "shutdown", "hook", "everywher"], "B_title": "Guard against failure due to multiple invocations of MAC.stop", "B_clean_title": ["guard", "against", "failur", "due", "multipl", "invoc", "mac", "stop"]},
{"A_title": "RegulaFalsiSolver failureThe following unit test:  @Test public void testBug()      final UnivariateRealFunction f = new UnivariateRealFunction()              @Override             public double value(double x)                  return Math.exp(x) - Math.pow(Math.PI 3.0);                      ;      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100 f 1 10);    fails with  illegal state: maximal count (100) exceeded: evaluations   Using PegasusSolver the answer is found after 17 evaluations.", "A_clean_title": ["regulafalsisolv", "regula", "falsi", "solver", "failureth", "failur", "follow", "unit", "test", "test", "public", "void", "testbug", "test", "bug", "final", "univariaterealfunct", "univari", "real", "function", "new", "univariaterealfunct", "univari", "real", "function", "overrid", "public", "doubl", "valu", "doubl", "return", "math", "exp", "math", "pow", "math", "pi", "univariaterealsolv", "univari", "real", "solver", "solver", "new", "regulafalsisolv", "regula", "falsi", "solver", "doubl", "root", "solver", "solv", "100", "10", "fail", "illeg", "state", "maxim", "count", "100", "exceed", "evalu", "pegasussolv", "pegasu", "solver", "answer", "found", "after", "17", "evalu"], "B_title": "Early detection of Regula Falsi algorithm being stuck due to finite precision. Javadoc makes it clear that either the Pegasus or the Illinois solver should be preferred over the Regula Falsi one (due to D. Hendriks).", "B_clean_title": ["earli", "detect", "regula", "falsi", "algorithm", "be", "stuck", "due", "finit", "precis", "javadoc", "make", "it", "clear", "that", "either", "pegasu", "or", "illinoi", "solver", "prefer", "over", "regula", "falsi", "one", "due", "hendrik"]},
{"A_title": "Default sync handler property mapping does not allow constant propertiesit would be useful if the default sync handler user (and group) mapping could also handle constant properties and use given primary type and mixin type information. eg:  noformat profile/nt:primaryType=sling:Folder profile/sling:resourceType=sling/security/profile noformat", "A_clean_title": ["default", "sync", "handler", "properti", "map", "not", "allow", "constant", "propertiesit", "would", "use", "default", "sync", "handler", "user", "group", "map", "could", "also", "handl", "constant", "properti", "use", "given", "primari", "type", "mixin", "type", "inform", "eg", "noformat", "profil", "nt", "primarytype=sl", "primari", "type=sl", "folder", "profil", "sling", "resourcetype=sl", "secur", "profil", "resourc", "type=sl", "noformat"], "B_title": "Default sync handler property mapping does not allow constant properties", "B_clean_title": ["default", "sync", "handler", "properti", "map", "not", "allow", "constant", "properti"]},
{"A_title": "Exception when emitting code containing gettersNone", "A_clean_title": ["except", "when", "emit", "code", "contain", "gettersnon", "getter", "none"], "B_title": "Dont attempt to rewrite object literal get/set definitions. Fixes issue 538.", "B_clean_title": ["dont", "attempt", "rewrit", "object", "liter", "get", "set", "definit", "fix", "issu", "538"]},
{"A_title": "AbstractNumberConverter issue when used with NumberFormat#getCurrencyInstanceSummary of the discussion on users@:  There is an issue when using AbstractNumberConverter when #getNumberFormat returns NumberFormat#getCurrencyInstance() I think the problem is due to AbstractNumberConverter#parse(Object double double Locale):  if (value instanceof String)          // Convert spaces to no-break space (U+00A0) to fix problems with         // browser conversions.         // Space is not valid thousands-separator but no-br space is.         value = ((String)value).replace(  u00A0);   Which replace spaces so a string like 15  is invalid while being parsed.  public class CurrencyConverter extends AbstractNumberConverter<Double>      private static final long serialVersionUID = 1L;      public CurrencyConverter()                @Override     protected Class<Double> getTargetType()              return Double.class;           @Override     public NumberFormat getNumberFormat(Locale locale)              return NumberFormat.getCurrencyInstance(locale);           @Override     public Double convertToObject(String value Locale locale)              locale = Locale.FRANCE;          return this.parse(value Double.MIN_VALUE Double.MAX_VALUE locale);  //        This does work: //        final NumberFormat format = this.getNumberFormat(locale); //        return this.parse(format value locale);        As Sven indicates there is (yet another) issue in Java currency formating (space as thousand separator) http://matthiaswessendorf.wordpress.com/2007/12/03/javas-numberformat-bug/ http://bugs.sun.com/view_bug.do?bug_id=4510618  So will I let you decide whether or not you wish to fix it (the space before the currency symbol).  Thanks & best regards Sebastien.", "A_clean_title": ["abstractnumberconvert", "abstract", "number", "convert", "issu", "when", "use", "numberformat", "number", "format", "getcurrencyinstancesummari", "get", "currenc", "instanc", "summari", "discuss", "user", "there", "issu", "when", "abstractnumberconvert", "abstract", "number", "convert", "when", "getnumberformat", "get", "number", "format", "return", "numberformat", "number", "format", "getcurrencyinst", "get", "currenc", "instanc", "think", "problem", "due", "abstractnumberconvert", "abstract", "number", "convert", "pars", "object", "doubl", "doubl", "local", "valu", "instanceof", "string", "convert", "space", "no", "break", "space", "u+00a0", "fix", "problem", "browser", "convers", "space", "not", "valid", "thousand", "separ", "but", "no", "br", "space", "valu", "string", "valu", "replac", "u00a0", "which", "replac", "space", "so", "string", "like", "15", "invalid", "while", "be", "pars", "public", "class", "currencyconvert", "currenc", "convert", "extend", "abstractnumberconvert", "abstract", "number", "convert", "doubl", "privat", "static", "final", "long", "serialversionuid", "serial", "version", "uid", "1l", "public", "currencyconvert", "currenc", "convert", "overrid", "protect", "class", "doubl", "gettargettyp", "get", "target", "type", "return", "doubl", "class", "overrid", "public", "numberformat", "number", "format", "getnumberformat", "get", "number", "format", "local", "local", "return", "numberformat", "getcurrencyinst", "number", "format", "get", "currenc", "instanc", "local", "overrid", "public", "doubl", "converttoobject", "convert", "object", "string", "valu", "local", "local", "local", "local", "franc", "return", "thi", "pars", "valu", "doubl", "min", "valu", "doubl", "max", "valu", "local", "thi", "work", "final", "numberformat", "number", "format", "format", "thi", "getnumberformat", "get", "number", "format", "local", "return", "thi", "pars", "format", "valu", "local", "as", "sven", "indic", "there", "yet", "anoth", "issu", "java", "currenc", "format", "space", "as", "thousand", "separ", "http", "numberformat", "wordpress", "bug", "matthiaswessendorf", "com", "2007", "12", "03", "java", "http", "sun", "bug", "bug", "com", "view", "bug", "id=4510618", "so", "will", "let", "you", "decid", "whether", "or", "not", "you", "wish", "fix", "it", "space", "befor", "currenc", "symbol", "thank", "best", "regard", "sebastien"], "B_title": "replace space with non-breaking space between digits only", "B_clean_title": ["replac", "space", "non", "break", "space", "between", "digit", "onli"]},
{"A_title": "Dont cache credentials in client-side ConnectorAuthenticationToken objects are Destroyable. However this cannot be exercised properly in the client code because the Connector immediately serializes the credentials and stores them as long as the Connector lives.  It should be possible to destroy a token after creating a Connector and thereby forcing any further RPC calls initiated by that Connector to fail to authenticate. This means that serialization on the client side to a TCredentials object needs to occur just before the RPC call.", "A_clean_title": ["dont", "cach", "credenti", "client", "side", "connectorauthenticationtoken", "connector", "authent", "token", "object", "are", "destroy", "howev", "thi", "not", "exercis", "properli", "client", "code", "becaus", "connector", "immedi", "serial", "credenti", "store", "them", "as", "long", "as", "connector", "live", "it", "possibl", "destroy", "token", "after", "creat", "connector", "therebi", "forc", "ani", "further", "rpc", "call", "initi", "by", "that", "connector", "fail", "authent", "thi", "mean", "that", "serial", "client", "side", "tcredenti", "credenti", "object", "need", "occur", "just", "befor", "rpc", "call"], "B_title": "Test Connectors and serialization with destroyed tokens", "B_clean_title": ["test", "connector", "serial", "destroy", "token"]},
{"A_title": "QueryEngine adding invalid property restriction for fulltext queryQueryEngine inserts a property restriction of is not null for any property used in fulltext constraint. For e.g. for query  noformat select * from nt:unstructured where CONTAINS(jcr:content/metadata/comment december) noformat  A property restriction would be added for jcr:content/metadata/comment. However currently due to bug in FulltextSearchImpl 1 the property name generated is comment/jcr:content/metadata.  code @Override     public void restrict(FilterImpl f)          if (propertyName != null)              if (f.getSelector().equals(selector))                  String p = propertyName;                 if (relativePath != null)                      p = PathUtils.concat(p relativePath);                                                  p = normalizePropertyName(p);                 restrictPropertyOnFilter(p f);                               f.restrictFulltextCondition(fullTextSearchExpression.currentValue().getValue(Type.STRING));      code  This happens because relativePath is passed as second param to PathUtils.concat. It should be first param  1 https://github.com/apache/jackrabbit-oak/blob/1.4/oak-core/src/main/java/org/apache/jackrabbit/oak/query/ast/FullTextSearchImpl.java#L275-L286", "A_clean_title": ["queryengin", "queri", "engin", "ad", "invalid", "properti", "restrict", "fulltext", "queryqueryengin", "queri", "queri", "engin", "insert", "properti", "restrict", "not", "null", "ani", "properti", "use", "fulltext", "constraint", "queri", "noformat", "select", "nt", "unstructur", "where", "contain", "jcr", "content", "metadata", "comment", "decemb", "noformat", "properti", "restrict", "would", "ad", "jcr", "content", "metadata", "comment", "howev", "current", "due", "bug", "fulltextsearchimpl", "fulltext", "search", "impl", "properti", "name", "gener", "comment", "jcr", "content", "metadata", "code", "overrid", "public", "void", "restrict", "filterimpl", "filter", "impl", "propertynam", "properti", "name", "null", "getselector", "get", "selector", "equal", "selector", "string", "propertynam", "properti", "name", "relativepath", "rel", "path", "null", "pathutil", "concat", "path", "util", "relativepath", "rel", "path", "normalizepropertynam", "normal", "properti", "name", "restrictpropertyonfilt", "restrict", "properti", "filter", "restrictfulltextcondit", "restrict", "fulltext", "condit", "fulltextsearchexpress", "currentvalu", "full", "text", "search", "express", "current", "valu", "getvalu", "get", "valu", "type", "string", "code", "thi", "happen", "becaus", "relativepath", "rel", "path", "pass", "as", "second", "param", "pathutil", "concat", "path", "util", "it", "first", "param", "http", "java", "github", "com", "apach", "jackrabbit", "oak", "blob", "oak", "core", "src", "main", "java", "org", "apach", "jackrabbit", "oak", "queri", "ast", "fulltextsearchimpl", "full", "text", "search", "impl", "l275", "l286"], "B_title": "QueryEngine adding invalid property restriction for fulltext query", "B_clean_title": ["queryengin", "queri", "engin", "ad", "invalid", "properti", "restrict", "fulltext", "queri"]},
{"A_title": "Method Strings.join doesnt work correctly if separator is empty.If we use an empty separator () to join strings the first character of any fragment is truncated. Es foo bar baz became ooaraz.", "A_clean_title": ["method", "string", "join", "doesnt", "work", "correctli", "separ", "empti", "we", "use", "empti", "separ", "join", "string", "first", "charact", "ani", "fragment", "truncat", "es", "foo", "bar", "baz", "becam", "ooaraz"], "B_title": "Method Strings.join doesnt work correctly if separator is empty.", "B_clean_title": ["method", "string", "join", "doesnt", "work", "correctli", "separ", "empti"]},
{"A_title": "Bug in MonotoneChain: a collinear point landing on the existing boundary should be dropped (patch)The is a bug on the code in MonotoneChain.java that attempts to handle the case of a point on the line formed by the previous last points and the last point of the chain being constructed. When `includeCollinearPoints` is false the point should be dropped entirely. In common-math 33 the point is added which in some cases can cause a `ConvergenceException` to be thrown.  In the patch below the data points are from a case that showed up in testing before we went to production.  code:java Index: src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java =================================================================== --- src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java(revision 1609491) +++ src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java(working copy) @@ -1608 +1608 @@                   else                       if (distanceToCurrent > distanceToLast)                           hull.remove(size - 1); +                        hull.add(point);                       -                    hull.add(point);                                    return;               else if (offset > 0)  Index: src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java =================================================================== --- src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java(revision 1609491) +++ src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java(working copy) @@ -2046 +20424 @@              @Test +    public void testCollinnearPointOnExistingBoundary()  +        final Collection<Vector2D> points = new ArrayList<Vector2D>(); +        points.add(new Vector2D(7.3152 34.7472)); +        points.add(new Vector2D(6.400799999999997 34.747199999999985)); +        points.add(new Vector2D(5.486399999999997 34.7472)); +        points.add(new Vector2D(4.876799999999999 34.7472)); +        points.add(new Vector2D(4.876799999999999 34.1376)); +        points.add(new Vector2D(4.876799999999999 30.48)); +        points.add(new Vector2D(6.0959999999999965 30.48)); +        points.add(new Vector2D(6.0959999999999965 34.1376)); +        points.add(new Vector2D(7.315199999999996 34.1376)); +        points.add(new Vector2D(7.3152 30.48)); + +        final ConvexHull2D hull = generator.generate(points); +        checkConvexHull(points hull); +     + +    @Test      public void testIssue1123()             List<Vector2D> points = new ArrayList<Vector2D>(); code", "A_clean_title": ["bug", "monotonechain", "monoton", "chain", "collinear", "point", "land", "exist", "boundari", "drop", "patch", "bug", "code", "monotonechain", "java", "monoton", "chain", "that", "attempt", "handl", "case", "point", "line", "form", "by", "previou", "last", "point", "last", "point", "chain", "be", "construct", "when", "includecollinearpoint", "includ", "collinear", "point", "fals", "point", "drop", "entir", "common", "math", "33", "point", "ad", "which", "some", "case", "caus", "convergenceexcept", "converg", "except", "thrown", "patch", "below", "data", "point", "are", "case", "that", "show", "up", "test", "befor", "we", "went", "product", "code", "java", "index", "java", "src", "main", "java", "org", "apach", "common", "math3", "geometri", "euclidean", "twod", "hull", "monotonechain", "monoton", "chain", "java", "src", "main", "java", "org", "apach", "common", "math3", "geometri", "euclidean", "twod", "hull", "monotonechain", "monoton", "chain", "revis", "1609491", "java", "src", "main", "java", "org", "apach", "common", "math3", "geometri", "euclidean", "twod", "hull", "monotonechain", "monoton", "chain", "work", "copi", "1608", "+1608", "distancetocurr", "distanc", "current", "distancetolast", "distanc", "last", "hull", "remov", "size", "hull", "add", "point", "hull", "add", "point", "return", "offset", "index", "java", "src", "test", "java", "org", "apach", "common", "math3", "geometri", "euclidean", "twod", "hull", "convexhullgenerator2dabstracttest", "convex", "hull", "generator2d", "abstract", "test", "java", "src", "test", "java", "org", "apach", "common", "math3", "geometri", "euclidean", "twod", "hull", "convexhullgenerator2dabstracttest", "convex", "hull", "generator2d", "abstract", "test", "revis", "1609491", "java", "src", "test", "java", "org", "apach", "common", "math3", "geometri", "euclidean", "twod", "hull", "convexhullgenerator2dabstracttest", "convex", "hull", "generator2d", "abstract", "test", "work", "copi", "2046", "+20424", "test", "public", "void", "testcollinnearpointonexistingboundari", "test", "collinnear", "point", "exist", "boundari", "final", "collect", "vector2d", "point", "new", "arraylist", "array", "list", "vector2d", "point", "add", "new", "vector2d", "3152", "34", "7472", "point", "add", "new", "vector2d", "400799999999997", "34", "747199999999985", "point", "add", "new", "vector2d", "486399999999997", "34", "7472", "point", "add", "new", "vector2d", "876799999999999", "34", "7472", "point", "add", "new", "vector2d", "876799999999999", "34", "1376", "point", "add", "new", "vector2d", "876799999999999", "30", "48", "point", "add", "new", "vector2d", "0959999999999965", "30", "48", "point", "add", "new", "vector2d", "0959999999999965", "34", "1376", "point", "add", "new", "vector2d", "315199999999996", "34", "1376", "point", "add", "new", "vector2d", "3152", "30", "48", "final", "convexhull2d", "convex", "hull2d", "hull", "gener", "gener", "point", "checkconvexhul", "check", "convex", "hull", "point", "hull", "test", "public", "void", "testissue1123", "test", "issue1123", "list", "vector2d", "point", "new", "arraylist", "array", "list", "vector2d", "code"], "B_title": "Fix MonotoneChain algorithm in case of collinear hull points. Thanks to Guillaume Marceau.", "B_clean_title": ["fix", "monotonechain", "monoton", "chain", "algorithm", "case", "collinear", "hull", "point", "thank", "guillaum", "marceau"]},
{"A_title": "Bug on withLaterOffsetAtOverlap methodOn the last two brackets we can see that withLaterOffsetAtOverlap is not undoing withEarlierOffsetAtOverlap as it should ( and not even working at all ).", "A_clean_title": ["bug", "withlateroffsetatoverlap", "later", "offset", "at", "overlap", "methodon", "method", "last", "two", "bracket", "we", "see", "that", "withlateroffsetatoverlap", "later", "offset", "at", "overlap", "not", "undo", "withearlieroffsetatoverlap", "earlier", "offset", "at", "overlap", "as", "it", "not", "even", "work", "at", "all"], "B_title": "Fix time zone later/earlier offset methods in Western hemisphere 3476684", "B_clean_title": ["fix", "time", "zone", "later", "earlier", "offset", "method", "western", "hemispher", "3476684"]}]